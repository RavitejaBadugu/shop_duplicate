{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport pandas as pd\nfrom tqdm import tqdm\nfrom kaggle_datasets import KaggleDatasets\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras.backend as K","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:15.447422Z","iopub.execute_input":"2022-06-21T03:57:15.447996Z","iopub.status.idle":"2022-06-21T03:57:21.197901Z","shell.execute_reply.started":"2022-06-21T03:57:15.447906Z","shell.execute_reply":"2022-06-21T03:57:21.197161Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2022-06-21 03:57:16.281584: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2022-06-21 03:57:16.281739: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(action='ignore')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:21.199703Z","iopub.execute_input":"2022-06-21T03:57:21.200077Z","iopub.status.idle":"2022-06-21T03:57:21.205372Z","shell.execute_reply.started":"2022-06-21T03:57:21.200033Z","shell.execute_reply":"2022-06-21T03:57:21.204434Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:21.207345Z","iopub.execute_input":"2022-06-21T03:57:21.207778Z","iopub.status.idle":"2022-06-21T03:57:27.185727Z","shell.execute_reply.started":"2022-06-21T03:57:21.207607Z","shell.execute_reply":"2022-06-21T03:57:27.184701Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Running on TPU  grpc://10.0.0.2:8470\n","output_type":"stream"},{"name":"stderr","text":"2022-06-21 03:57:21.219286: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2022-06-21 03:57:21.222205: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2022-06-21 03:57:21.222235: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2022-06-21 03:57:21.222262: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (0b1b96f83c6b): /proc/driver/nvidia/version does not exist\n2022-06-21 03:57:21.224921: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-06-21 03:57:21.226380: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2022-06-21 03:57:21.230890: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2022-06-21 03:57:21.263164: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-06-21 03:57:21.263218: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30042}\n2022-06-21 03:57:21.281244: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-06-21 03:57:21.281291: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30042}\n2022-06-21 03:57:21.283160: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30042\n","output_type":"stream"},{"name":"stdout","text":"REPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"code","source":"BATCH_SIZE=32 * strategy.num_replicas_in_sync\nIMAGE_SIZE=[512,512]","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:27.187666Z","iopub.execute_input":"2022-06-21T03:57:27.187893Z","iopub.status.idle":"2022-06-21T03:57:27.192227Z","shell.execute_reply.started":"2022-06-21T03:57:27.187867Z","shell.execute_reply":"2022-06-21T03:57:27.191242Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def int_feature(int_list):\n    if isinstance(int_list,type(tf.cast(1,dtype=tf.int32))):\n        int_list=int_list.numpy()\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[int_list]))\n\ndef bytes_feature(value):\n    if isinstance(value,type(tf.cast(1,dtype=tf.float32))):\n        value=value.numpy()\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:27.193416Z","iopub.execute_input":"2022-06-21T03:57:27.193651Z","iopub.status.idle":"2022-06-21T03:57:27.203657Z","shell.execute_reply.started":"2022-06-21T03:57:27.193608Z","shell.execute_reply":"2022-06-21T03:57:27.202926Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def Searlize_to_string(image_feature,label_feature):\n    feature={\n        \"image\":image_feature,\n        \"label\":label_feature\n    }\n    example=tf.train.Example(features=tf.train.Features(feature=feature))\n    return example.SerializeToString()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:27.204993Z","iopub.execute_input":"2022-06-21T03:57:27.205205Z","iopub.status.idle":"2022-06-21T03:57:27.216744Z","shell.execute_reply.started":"2022-06-21T03:57:27.205181Z","shell.execute_reply":"2022-06-21T03:57:27.215962Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#!python EDA.py --data_path train.csv --images_base_path train_images\n#!python create_fold.py --data_path \"processed_data/cleaned_data.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:27.217808Z","iopub.execute_input":"2022-06-21T03:57:27.218180Z","iopub.status.idle":"2022-06-21T03:57:27.227504Z","shell.execute_reply.started":"2022-06-21T03:57:27.218140Z","shell.execute_reply":"2022-06-21T03:57:27.226568Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"../input/shop-tpu-folds/fold_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:27.228608Z","iopub.execute_input":"2022-06-21T03:57:27.228928Z","iopub.status.idle":"2022-06-21T03:57:27.488070Z","shell.execute_reply.started":"2022-06-21T03:57:27.228890Z","shell.execute_reply":"2022-06-21T03:57:27.487210Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:27.489233Z","iopub.execute_input":"2022-06-21T03:57:27.489499Z","iopub.status.idle":"2022-06-21T03:57:27.512804Z","shell.execute_reply.started":"2022-06-21T03:57:27.489471Z","shell.execute_reply":"2022-06-21T03:57:27.512033Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n\n                                               title  label_group  \\\n0                          Paper Bag Victoria Secret          666   \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...         7572   \n2        Maling TTS Canned Pork Luncheon Meat 397 gr         6172   \n3  Daster Batik Lengan pendek - Motif Acak / Camp...        10509   \n4                  Nescafe \\xc3\\x89clair Latte 220ml         9425   \n\n                                          image_path  gfold  \n0  train_images\\0000a68812bc7e98c42888dfb1c07da0.jpg      0  \n1  train_images\\00039780dfc94d01db8676fe789ecd05.jpg      2  \n2  train_images\\000a190fdd715a2a36faed16e2c65df7.jpg      0  \n3  train_images\\00117e4fc239b1b641ff08340b429633.jpg      1  \n4  train_images\\00136d1cf4edede0203f32f05f660588.jpg      3  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>image_path</th>\n      <th>gfold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>666</td>\n      <td>train_images\\0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>7572</td>\n      <td>train_images\\00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>6172</td>\n      <td>train_images\\000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>10509</td>\n      <td>train_images\\00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>9425</td>\n      <td>train_images\\00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_steps={}\nvalid_steps={}\nfor fold in range(5):\n    valid_data=df.loc[df['gfold']==fold].reset_index(drop=True)\n    valid_steps[fold]=valid_data.shape[0]\n    train_steps[fold]=df.shape[0]-valid_data.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:27.514866Z","iopub.execute_input":"2022-06-21T03:57:27.515662Z","iopub.status.idle":"2022-06-21T03:57:27.541103Z","shell.execute_reply.started":"2022-06-21T03:57:27.515629Z","shell.execute_reply":"2022-06-21T03:57:27.540421Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_steps,valid_steps","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:27.542548Z","iopub.execute_input":"2022-06-21T03:57:27.543555Z","iopub.status.idle":"2022-06-21T03:57:27.550752Z","shell.execute_reply.started":"2022-06-21T03:57:27.543509Z","shell.execute_reply":"2022-06-21T03:57:27.549837Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"({0: 27400, 1: 27400, 2: 27400, 3: 27400, 4: 27400},\n {0: 6850, 1: 6850, 2: 6850, 3: 6850, 4: 6850})"},"metadata":{}}]},{"cell_type":"code","source":"#import os\n#os.mkdir(\"tfrecords\")","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:27.551905Z","iopub.execute_input":"2022-06-21T03:57:27.552124Z","iopub.status.idle":"2022-06-21T03:57:27.560644Z","shell.execute_reply.started":"2022-06-21T03:57:27.552098Z","shell.execute_reply":"2022-06-21T03:57:27.559671Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#for fold in tqdm(range(5)):\n#    fold_data=df.loc[df['gfold']==fold].reset_index(drop=True)\n#    with tf.io.TFRecordWriter(f\"tfrecords/fold_{fold}_512.tfrec\") as writer:\n#        for i in fold_data.index:\n#            img=tf.keras.preprocessing.image.load_img(fold_data.loc[i,'image_path'],target_size=IMAGE_SIZE)\n#            img=tf.keras.preprocessing.image.img_to_array(img)\n#            img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, 94))[1].tostring()\n#            img_bytes_feature=bytes_feature(img)\n#            label_feature=int_feature(int(fold_data.loc[i,'label_group']))\n#            example_string=Searlize_to_string(img_bytes_feature,label_feature)\n#            writer.write(example_string)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:27.561760Z","iopub.execute_input":"2022-06-21T03:57:27.562351Z","iopub.status.idle":"2022-06-21T03:57:27.571682Z","shell.execute_reply.started":"2022-06-21T03:57:27.562292Z","shell.execute_reply":"2022-06-21T03:57:27.570797Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"GCS_PATH = KaggleDatasets().get_gcs_path('shop-512-size-tfrecords')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:27.572750Z","iopub.execute_input":"2022-06-21T03:57:27.572983Z","iopub.status.idle":"2022-06-21T03:57:28.110370Z","shell.execute_reply.started":"2022-06-21T03:57:27.572958Z","shell.execute_reply":"2022-06-21T03:57:28.109447Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# DATA AUG","metadata":{}},{"cell_type":"markdown","source":"https://www.kaggle.com/code/cdeotte/rotation-augmentation-gpu-tpu-0-96/notebook","metadata":{}},{"cell_type":"code","source":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear = math.pi * shear / 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:28.111874Z","iopub.execute_input":"2022-06-21T03:57:28.112334Z","iopub.status.idle":"2022-06-21T03:57:28.125155Z","shell.execute_reply.started":"2022-06-21T03:57:28.112264Z","shell.execute_reply":"2022-06-21T03:57:28.124283Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def SHIFT_ROTATE_ZOOM(image,label):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = IMAGE_SIZE[0]\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = 15. * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    h_shift = 16. * tf.random.normal([1],dtype='float32') \n    w_shift = 16. * tf.random.normal([1],dtype='float32') \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM,DIM,3]),label","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:28.126355Z","iopub.execute_input":"2022-06-21T03:57:28.126599Z","iopub.status.idle":"2022-06-21T03:57:28.144627Z","shell.execute_reply.started":"2022-06-21T03:57:28.126572Z","shell.execute_reply":"2022-06-21T03:57:28.143602Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def Desearlize(example):\n    ex={\n        \"image\":tf.io.FixedLenFeature([],dtype=tf.string),\n        \"label\":tf.io.FixedLenFeature([],dtype=tf.int64)\n    }\n    features=tf.io.parse_single_example(example,ex)\n    image,label=features['image'],features['label']\n    label=tf.cast(label,dtype=tf.int32)\n    image=tf.image.decode_jpeg(image,channels=3)\n    image=tf.image.resize(image,IMAGE_SIZE)\n    image=tf.cast(image,dtype=tf.float32)/255.0\n    ex={\"image\":image,\"label\":label}\n    return ex","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:57:28.146064Z","iopub.execute_input":"2022-06-21T03:57:28.146596Z","iopub.status.idle":"2022-06-21T03:57:28.160377Z","shell.execute_reply.started":"2022-06-21T03:57:28.146552Z","shell.execute_reply":"2022-06-21T03:57:28.159606Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def APPLY_AUG(example):\n    image,label=example['image'],example['label']\n    #image,label=SHIFT_ROTATE_ZOOM(image,label)\n    image=tf.image.random_flip_left_right(image,)\n    image=tf.image.random_brightness(image,0.2)\n    image=tf.image.random_contrast(image,0.02,1)\n    return image,label","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:58:33.052590Z","iopub.execute_input":"2022-06-21T03:58:33.053491Z","iopub.status.idle":"2022-06-21T03:58:33.059127Z","shell.execute_reply.started":"2022-06-21T03:58:33.053454Z","shell.execute_reply":"2022-06-21T03:58:33.058202Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def Train_input(image,label):\n    return (image,label),label\ndef Valid_input(example):\n    image,label=example['image'],example['label']\n    return (image,label),label","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:58:34.040484Z","iopub.execute_input":"2022-06-21T03:58:34.041257Z","iopub.status.idle":"2022-06-21T03:58:34.046247Z","shell.execute_reply.started":"2022-06-21T03:58:34.041219Z","shell.execute_reply":"2022-06-21T03:58:34.045245Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def Valid_data_pipe(files):\n    AUTO=tf.data.experimental.AUTOTUNE\n    dataset = tf.data.TFRecordDataset(files, num_parallel_reads = AUTO)\n    dataset=dataset.map(Desearlize,num_parallel_calls=AUTO)\n    dataset=dataset.map(Valid_input,num_parallel_calls=AUTO)\n    dataset=dataset.batch(BATCH_SIZE,drop_remainder=True)\n    dataset=dataset.prefetch(AUTO)\n    return dataset\n\ndef Train_data_pipe(files):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n    AUTO=tf.data.experimental.AUTOTUNE\n    dataset = tf.data.TFRecordDataset(files, num_parallel_reads = AUTO )\n    dataset=dataset.with_options(ignore_order)\n    dataset=dataset.map(Desearlize,num_parallel_calls=AUTO)\n    dataset=dataset.map(APPLY_AUG,num_parallel_calls=AUTO)\n    dataset=dataset.map(Train_input,num_parallel_calls=AUTO)\n    dataset = dataset.repeat()\n    dataset=dataset.shuffle(1024)\n    dataset=dataset.batch(BATCH_SIZE)\n    dataset=dataset.prefetch(AUTO)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:58:34.705844Z","iopub.execute_input":"2022-06-21T03:58:34.706653Z","iopub.status.idle":"2022-06-21T03:58:34.719179Z","shell.execute_reply.started":"2022-06-21T03:58:34.706597Z","shell.execute_reply":"2022-06-21T03:58:34.718256Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"files=tf.io.gfile.glob(f\"{GCS_PATH}/tfrecords/fold_*.tfrec\")","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:58:35.300754Z","iopub.execute_input":"2022-06-21T03:58:35.301556Z","iopub.status.idle":"2022-06-21T03:58:35.401960Z","shell.execute_reply.started":"2022-06-21T03:58:35.301503Z","shell.execute_reply":"2022-06-21T03:58:35.401274Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"2022-06-21 03:58:35.310697: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB5","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:58:35.892770Z","iopub.execute_input":"2022-06-21T03:58:35.893522Z","iopub.status.idle":"2022-06-21T03:58:35.896974Z","shell.execute_reply.started":"2022-06-21T03:58:35.893481Z","shell.execute_reply":"2022-06-21T03:58:35.896297Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class ARCFACE_LAYER(tf.keras.layers.Layer):\n    def __init__(self,m=0.5,s=30,n_classes=11014):\n        super(ARCFACE_LAYER,self).__init__()\n        self.m=m\n        self.s=s\n        self.sin_m=tf.sin(m)\n        self.cos_m=tf.cos(m)\n        self.n_classes=n_classes\n        self.threshold = tf.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def build(self,input_shape):\n        prev_layer_units=input_shape[0][1]\n        self.w=self.add_weight(shape=(prev_layer_units,self.n_classes),trainable=True,\n                              initializer='glorot_uniform')\n\n    def get_config(self):\n        config=super().get_config()\n        config.update({\"m\":0.5,\n                       \"s\":30,\n                       \"n_classes\":11014})\n        return config\n\n\n    def call(self,inputs):\n        prev_layer,y=inputs\n        y=tf.cast(y,dtype=tf.int32)\n        y_hot=tf.one_hot(y,self.n_classes)\n        y_hot=tf.cast(y_hot,dtype=tf.float32)\n        w_norm=tf.linalg.l2_normalize(self.w,axis=0)\n        x_norm=tf.linalg.l2_normalize(prev_layer,axis=1)\n        cos_theta=tf.linalg.matmul(x_norm,w_norm)\n        sin_theta=tf.sqrt(1-tf.pow(cos_theta,tf.cast(2,dtype=tf.float32)))\n        cos_theta_m=(cos_theta*self.cos_m)-(sin_theta*self.sin_m)\n        cos_theta_m=tf.where(cos_theta>self.threshold,cos_theta_m,cos_theta-self.mm)\n        final=self.s*((y_hot*cos_theta_m)+((1-y_hot)*cos_theta))\n        return final","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:58:36.766145Z","iopub.execute_input":"2022-06-21T03:58:36.766591Z","iopub.status.idle":"2022-06-21T03:58:36.779118Z","shell.execute_reply.started":"2022-06-21T03:58:36.766560Z","shell.execute_reply":"2022-06-21T03:58:36.778267Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"LR_START = 0.000005\nLR_MAX   = 0.001#0.00005 * strategy.num_replicas_in_sync\nLR_MIN   = 0.000005\nLR_RAMPUP_EPOCHS = 5\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS) + LR_MIN\n    return lr","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:58:40.853650Z","iopub.execute_input":"2022-06-21T03:58:40.853921Z","iopub.status.idle":"2022-06-21T03:58:40.859859Z","shell.execute_reply.started":"2022-06-21T03:58:40.853895Z","shell.execute_reply":"2022-06-21T03:58:40.858976Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"for fold in range(5):\n    train_records=files[:fold]+files[fold+1:]\n    test_records=files[fold]\n    NUM_TRAIN_STEPS=train_steps[fold]//BATCH_SIZE\n    NUM_VALID_STEPS=valid_steps[fold]//BATCH_SIZE\n    tf.keras.backend.clear_session()\n    with strategy.scope():\n        pre_trained=EfficientNetB5(include_top=False,weights=\"imagenet\",input_shape=(IMAGE_SIZE[0],IMAGE_SIZE[1],3))\n        ins=tf.keras.layers.Input(())\n        x=pre_trained.layers[-1].output\n        x=tf.keras.layers.GlobalMaxPooling2D()(x)\n        #x=tf.keras.layers.Dense(720)(x)\n        arc_layer=ARCFACE_LAYER()\n        x=arc_layer([x,ins])\n        outs=tf.keras.layers.Softmax()(x)\n        model=tf.keras.models.Model(inputs=(pre_trained.input,ins),outputs=outs)\n        print(f\"training for fold {fold}\")\n        #print(model.summary())\n        model.compile(optimizer=tf.keras.optimizers.Adam(),\n                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n    lr_callback=tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch),verbose=1)\n    early=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",mode=\"min\",verbose=1,patience=10)\n    saver=tf.keras.callbacks.ModelCheckpoint(filepath=\"effb5\"+f\"{fold}.h5\",\n                                     monitor=\"val_loss\",mode=\"min\",save_best_only=True,\n                                     save_weights_only=True,verbose=1)\n    train_dataloader=Train_data_pipe(train_records)\n    valid_dataloader=Valid_data_pipe(test_records)\n    model.fit(train_dataloader,\n              validation_data=valid_dataloader,epochs=40,\n              callbacks=[early,saver,lr_callback],steps_per_epoch=NUM_TRAIN_STEPS,\n             validation_steps=NUM_VALID_STEPS)\n    print(f\"model training for {fold} is done\")\n    del model\n    import gc\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T03:58:44.957065Z","iopub.execute_input":"2022-06-21T03:58:44.957834Z","iopub.status.idle":"2022-06-21T11:43:07.185852Z","shell.execute_reply.started":"2022-06-21T03:58:44.957795Z","shell.execute_reply":"2022-06-21T11:43:07.183849Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb5_notop.h5\n115269632/115263384 [==============================] - 1s 0us/step\ntraining for fold 0\nEpoch 1/40\n\nEpoch 00001: LearningRateScheduler reducing learning rate to 5e-06.\n107/107 [==============================] - 277s 1s/step - loss: 23.8726 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 23.8225 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00001: val_loss improved from inf to 23.82248, saving model to effb50.h5\nEpoch 2/40\n\nEpoch 00002: LearningRateScheduler reducing learning rate to 0.00020400000000000003.\n107/107 [==============================] - 131s 1s/step - loss: 23.7551 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 23.4719 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00002: val_loss improved from 23.82248 to 23.47189, saving model to effb50.h5\nEpoch 3/40\n\nEpoch 00003: LearningRateScheduler reducing learning rate to 0.00040300000000000004.\n107/107 [==============================] - 132s 1s/step - loss: 23.0785 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 23.2147 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00003: val_loss improved from 23.47189 to 23.21472, saving model to effb50.h5\nEpoch 4/40\n\nEpoch 00004: LearningRateScheduler reducing learning rate to 0.0006020000000000001.\n107/107 [==============================] - 131s 1s/step - loss: 21.2599 - sparse_categorical_accuracy: 6.2645e-04 - val_loss: 20.9529 - val_sparse_categorical_accuracy: 0.0036\n\nEpoch 00004: val_loss improved from 23.21472 to 20.95288, saving model to effb50.h5\nEpoch 5/40\n\nEpoch 00005: LearningRateScheduler reducing learning rate to 0.0008010000000000001.\n107/107 [==============================] - 131s 1s/step - loss: 18.7016 - sparse_categorical_accuracy: 0.0092 - val_loss: 21.5986 - val_sparse_categorical_accuracy: 0.0017\n\nEpoch 00005: val_loss did not improve from 20.95288\nEpoch 6/40\n\nEpoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n107/107 [==============================] - 131s 1s/step - loss: 15.9263 - sparse_categorical_accuracy: 0.0287 - val_loss: 24.4149 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00006: val_loss did not improve from 20.95288\nEpoch 7/40\n\nEpoch 00007: LearningRateScheduler reducing learning rate to 0.0008010000000000001.\n107/107 [==============================] - 131s 1s/step - loss: 13.2237 - sparse_categorical_accuracy: 0.0664 - val_loss: 17.8544 - val_sparse_categorical_accuracy: 0.0150\n\nEpoch 00007: val_loss improved from 20.95288 to 17.85441, saving model to effb50.h5\nEpoch 8/40\n\nEpoch 00008: LearningRateScheduler reducing learning rate to 0.0006418000000000001.\n107/107 [==============================] - 131s 1s/step - loss: 11.2118 - sparse_categorical_accuracy: 0.1230 - val_loss: 13.0279 - val_sparse_categorical_accuracy: 0.1354\n\nEpoch 00008: val_loss improved from 17.85441 to 13.02791, saving model to effb50.h5\nEpoch 9/40\n\nEpoch 00009: LearningRateScheduler reducing learning rate to 0.0005144400000000001.\n107/107 [==============================] - 131s 1s/step - loss: 9.7759 - sparse_categorical_accuracy: 0.1887 - val_loss: 11.9624 - val_sparse_categorical_accuracy: 0.1919\n\nEpoch 00009: val_loss improved from 13.02791 to 11.96239, saving model to effb50.h5\nEpoch 10/40\n\nEpoch 00010: LearningRateScheduler reducing learning rate to 0.0004125520000000001.\n107/107 [==============================] - 131s 1s/step - loss: 8.8180 - sparse_categorical_accuracy: 0.2467 - val_loss: 12.4165 - val_sparse_categorical_accuracy: 0.1663\n\nEpoch 00010: val_loss did not improve from 11.96239\nEpoch 11/40\n\nEpoch 00011: LearningRateScheduler reducing learning rate to 0.0003310416000000001.\n107/107 [==============================] - 131s 1s/step - loss: 8.0756 - sparse_categorical_accuracy: 0.2997 - val_loss: 11.9528 - val_sparse_categorical_accuracy: 0.1852\n\nEpoch 00011: val_loss improved from 11.96239 to 11.95282, saving model to effb50.h5\nEpoch 12/40\n\nEpoch 00012: LearningRateScheduler reducing learning rate to 0.0002658332800000001.\n107/107 [==============================] - 132s 1s/step - loss: 7.6121 - sparse_categorical_accuracy: 0.3356 - val_loss: 13.0349 - val_sparse_categorical_accuracy: 0.1103\n\nEpoch 00012: val_loss did not improve from 11.95282\nEpoch 13/40\n\nEpoch 00013: LearningRateScheduler reducing learning rate to 0.00021366662400000008.\n107/107 [==============================] - 131s 1s/step - loss: 7.2540 - sparse_categorical_accuracy: 0.3631 - val_loss: 13.4161 - val_sparse_categorical_accuracy: 0.1074\n\nEpoch 00013: val_loss did not improve from 11.95282\nEpoch 14/40\n\nEpoch 00014: LearningRateScheduler reducing learning rate to 0.0001719332992000001.\n107/107 [==============================] - 131s 1s/step - loss: 6.9243 - sparse_categorical_accuracy: 0.3957 - val_loss: 11.0080 - val_sparse_categorical_accuracy: 0.2569\n\nEpoch 00014: val_loss improved from 11.95282 to 11.00798, saving model to effb50.h5\nEpoch 15/40\n\nEpoch 00015: LearningRateScheduler reducing learning rate to 0.00013854663936000008.\n107/107 [==============================] - 131s 1s/step - loss: 6.7350 - sparse_categorical_accuracy: 0.4125 - val_loss: 9.9828 - val_sparse_categorical_accuracy: 0.3377\n\nEpoch 00015: val_loss improved from 11.00798 to 9.98277, saving model to effb50.h5\nEpoch 16/40\n\nEpoch 00016: LearningRateScheduler reducing learning rate to 0.00011183731148800006.\n107/107 [==============================] - 131s 1s/step - loss: 6.6530 - sparse_categorical_accuracy: 0.4307 - val_loss: 9.7921 - val_sparse_categorical_accuracy: 0.3585\n\nEpoch 00016: val_loss improved from 9.98277 to 9.79213, saving model to effb50.h5\nEpoch 17/40\n\nEpoch 00017: LearningRateScheduler reducing learning rate to 9.046984919040005e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.3600 - sparse_categorical_accuracy: 0.4484 - val_loss: 9.8810 - val_sparse_categorical_accuracy: 0.3498\n\nEpoch 00017: val_loss did not improve from 9.79213\nEpoch 18/40\n\nEpoch 00018: LearningRateScheduler reducing learning rate to 7.337587935232004e-05.\n107/107 [==============================] - 131s 1s/step - loss: 6.3963 - sparse_categorical_accuracy: 0.4527 - val_loss: 9.6481 - val_sparse_categorical_accuracy: 0.3699\n\nEpoch 00018: val_loss improved from 9.79213 to 9.64806, saving model to effb50.h5\nEpoch 19/40\n\nEpoch 00019: LearningRateScheduler reducing learning rate to 5.9700703481856035e-05.\n107/107 [==============================] - 131s 1s/step - loss: 6.1841 - sparse_categorical_accuracy: 0.4672 - val_loss: 9.6796 - val_sparse_categorical_accuracy: 0.3717\n\nEpoch 00019: val_loss did not improve from 9.64806\nEpoch 20/40\n\nEpoch 00020: LearningRateScheduler reducing learning rate to 4.8760562785484834e-05.\n107/107 [==============================] - 131s 1s/step - loss: 6.0628 - sparse_categorical_accuracy: 0.4704 - val_loss: 9.6253 - val_sparse_categorical_accuracy: 0.3756\n\nEpoch 00020: val_loss improved from 9.64806 to 9.62534, saving model to effb50.h5\nEpoch 21/40\n\nEpoch 00021: LearningRateScheduler reducing learning rate to 4.000845022838787e-05.\n107/107 [==============================] - 131s 1s/step - loss: 6.1467 - sparse_categorical_accuracy: 0.4815 - val_loss: 9.6074 - val_sparse_categorical_accuracy: 0.3764\n\nEpoch 00021: val_loss improved from 9.62534 to 9.60738, saving model to effb50.h5\nEpoch 22/40\n\nEpoch 00022: LearningRateScheduler reducing learning rate to 3.30067601827103e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.9910 - sparse_categorical_accuracy: 0.4876 - val_loss: 9.5974 - val_sparse_categorical_accuracy: 0.3777\n\nEpoch 00022: val_loss improved from 9.60738 to 9.59740, saving model to effb50.h5\nEpoch 23/40\n\nEpoch 00023: LearningRateScheduler reducing learning rate to 2.740540814616824e-05.\n107/107 [==============================] - 131s 1s/step - loss: 5.9409 - sparse_categorical_accuracy: 0.4894 - val_loss: 9.4702 - val_sparse_categorical_accuracy: 0.3894\n\nEpoch 00023: val_loss improved from 9.59740 to 9.47016, saving model to effb50.h5\nEpoch 24/40\n\nEpoch 00024: LearningRateScheduler reducing learning rate to 2.2924326516934593e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.9722 - sparse_categorical_accuracy: 0.4936 - val_loss: 9.4462 - val_sparse_categorical_accuracy: 0.3915\n\nEpoch 00024: val_loss improved from 9.47016 to 9.44623, saving model to effb50.h5\nEpoch 25/40\n\nEpoch 00025: LearningRateScheduler reducing learning rate to 1.9339461213547675e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.9002 - sparse_categorical_accuracy: 0.4940 - val_loss: 9.4366 - val_sparse_categorical_accuracy: 0.3948\n\nEpoch 00025: val_loss improved from 9.44623 to 9.43656, saving model to effb50.h5\nEpoch 26/40\n\nEpoch 00026: LearningRateScheduler reducing learning rate to 1.647156897083814e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.7926 - sparse_categorical_accuracy: 0.4990 - val_loss: 9.3957 - val_sparse_categorical_accuracy: 0.3977\n\nEpoch 00026: val_loss improved from 9.43656 to 9.39574, saving model to effb50.h5\nEpoch 27/40\n\nEpoch 00027: LearningRateScheduler reducing learning rate to 1.4177255176670514e-05.\n107/107 [==============================] - 131s 1s/step - loss: 5.8515 - sparse_categorical_accuracy: 0.5060 - val_loss: 9.3894 - val_sparse_categorical_accuracy: 0.3986\n\nEpoch 00027: val_loss improved from 9.39574 to 9.38937, saving model to effb50.h5\nEpoch 28/40\n\nEpoch 00028: LearningRateScheduler reducing learning rate to 1.234180414133641e-05.\n107/107 [==============================] - 131s 1s/step - loss: 5.8518 - sparse_categorical_accuracy: 0.5011 - val_loss: 9.3786 - val_sparse_categorical_accuracy: 0.3986\n\nEpoch 00028: val_loss improved from 9.38937 to 9.37856, saving model to effb50.h5\nEpoch 29/40\n\nEpoch 00029: LearningRateScheduler reducing learning rate to 1.087344331306913e-05.\n107/107 [==============================] - 131s 1s/step - loss: 5.8315 - sparse_categorical_accuracy: 0.5073 - val_loss: 9.3701 - val_sparse_categorical_accuracy: 0.4001\n\nEpoch 00029: val_loss improved from 9.37856 to 9.37011, saving model to effb50.h5\nEpoch 30/40\n\nEpoch 00030: LearningRateScheduler reducing learning rate to 9.698754650455303e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.7940 - sparse_categorical_accuracy: 0.5013 - val_loss: 9.3692 - val_sparse_categorical_accuracy: 0.4013\n\nEpoch 00030: val_loss improved from 9.37011 to 9.36917, saving model to effb50.h5\nEpoch 31/40\n\nEpoch 00031: LearningRateScheduler reducing learning rate to 8.759003720364244e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.7966 - sparse_categorical_accuracy: 0.5092 - val_loss: 9.3637 - val_sparse_categorical_accuracy: 0.4011\n\nEpoch 00031: val_loss improved from 9.36917 to 9.36375, saving model to effb50.h5\nEpoch 32/40\n\nEpoch 00032: LearningRateScheduler reducing learning rate to 8.007202976291395e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.7338 - sparse_categorical_accuracy: 0.5115 - val_loss: 9.3532 - val_sparse_categorical_accuracy: 0.4023\n\nEpoch 00032: val_loss improved from 9.36375 to 9.35322, saving model to effb50.h5\nEpoch 33/40\n\nEpoch 00033: LearningRateScheduler reducing learning rate to 7.4057623810331166e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.8540 - sparse_categorical_accuracy: 0.5051 - val_loss: 9.3544 - val_sparse_categorical_accuracy: 0.4031\n\nEpoch 00033: val_loss did not improve from 9.35322\nEpoch 34/40\n\nEpoch 00034: LearningRateScheduler reducing learning rate to 6.9246099048264925e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.7432 - sparse_categorical_accuracy: 0.5142 - val_loss: 9.3464 - val_sparse_categorical_accuracy: 0.4034\n\nEpoch 00034: val_loss improved from 9.35322 to 9.34640, saving model to effb50.h5\nEpoch 35/40\n\nEpoch 00035: LearningRateScheduler reducing learning rate to 6.539687923861194e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.7542 - sparse_categorical_accuracy: 0.5113 - val_loss: 9.3419 - val_sparse_categorical_accuracy: 0.4043\n\nEpoch 00035: val_loss improved from 9.34640 to 9.34188, saving model to effb50.h5\nEpoch 36/40\n\nEpoch 00036: LearningRateScheduler reducing learning rate to 6.231750339088956e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.7877 - sparse_categorical_accuracy: 0.5088 - val_loss: 9.3348 - val_sparse_categorical_accuracy: 0.4049\n\nEpoch 00036: val_loss improved from 9.34188 to 9.33480, saving model to effb50.h5\nEpoch 37/40\n\nEpoch 00037: LearningRateScheduler reducing learning rate to 5.985400271271165e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.6859 - sparse_categorical_accuracy: 0.5188 - val_loss: 9.3302 - val_sparse_categorical_accuracy: 0.4049\n\nEpoch 00037: val_loss improved from 9.33480 to 9.33019, saving model to effb50.h5\nEpoch 38/40\n\nEpoch 00038: LearningRateScheduler reducing learning rate to 5.788320217016932e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.6445 - sparse_categorical_accuracy: 0.5186 - val_loss: 9.3309 - val_sparse_categorical_accuracy: 0.4064\n\nEpoch 00038: val_loss did not improve from 9.33019\nEpoch 39/40\n\nEpoch 00039: LearningRateScheduler reducing learning rate to 5.630656173613546e-06.\n107/107 [==============================] - 134s 1s/step - loss: 5.8343 - sparse_categorical_accuracy: 0.5115 - val_loss: 9.3277 - val_sparse_categorical_accuracy: 0.4056\n\nEpoch 00039: val_loss improved from 9.33019 to 9.32774, saving model to effb50.h5\nEpoch 40/40\n\nEpoch 00040: LearningRateScheduler reducing learning rate to 5.5045249388908364e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.6900 - sparse_categorical_accuracy: 0.5178 - val_loss: 9.3241 - val_sparse_categorical_accuracy: 0.4059\n\nEpoch 00040: val_loss improved from 9.32774 to 9.32411, saving model to effb50.h5\nmodel training for 0 is done\ntraining for fold 1\nEpoch 1/40\n\nEpoch 00001: LearningRateScheduler reducing learning rate to 5e-06.\n107/107 [==============================] - 278s 1s/step - loss: 23.8825 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 23.8337 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00001: val_loss improved from inf to 23.83374, saving model to effb51.h5\nEpoch 2/40\n\nEpoch 00002: LearningRateScheduler reducing learning rate to 0.00020400000000000003.\n107/107 [==============================] - 131s 1s/step - loss: 23.7353 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 23.4020 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00002: val_loss improved from 23.83374 to 23.40196, saving model to effb51.h5\nEpoch 3/40\n\nEpoch 00003: LearningRateScheduler reducing learning rate to 0.00040300000000000004.\n107/107 [==============================] - 131s 1s/step - loss: 23.0370 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 22.0761 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00003: val_loss improved from 23.40196 to 22.07607, saving model to effb51.h5\nEpoch 4/40\n\nEpoch 00004: LearningRateScheduler reducing learning rate to 0.0006020000000000001.\n107/107 [==============================] - 131s 1s/step - loss: 21.2776 - sparse_categorical_accuracy: 0.0013 - val_loss: 22.0253 - val_sparse_categorical_accuracy: 0.0021\n\nEpoch 00004: val_loss improved from 22.07607 to 22.02534, saving model to effb51.h5\nEpoch 5/40\n\nEpoch 00005: LearningRateScheduler reducing learning rate to 0.0008010000000000001.\n107/107 [==============================] - 131s 1s/step - loss: 18.7497 - sparse_categorical_accuracy: 0.0111 - val_loss: 20.4077 - val_sparse_categorical_accuracy: 0.0077\n\nEpoch 00005: val_loss improved from 22.02534 to 20.40765, saving model to effb51.h5\nEpoch 6/40\n\nEpoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n107/107 [==============================] - 132s 1s/step - loss: 16.0106 - sparse_categorical_accuracy: 0.0298 - val_loss: 20.9479 - val_sparse_categorical_accuracy: 0.0039\n\nEpoch 00006: val_loss did not improve from 20.40765\nEpoch 7/40\n\nEpoch 00007: LearningRateScheduler reducing learning rate to 0.0008010000000000001.\n107/107 [==============================] - 132s 1s/step - loss: 13.2541 - sparse_categorical_accuracy: 0.0652 - val_loss: 16.3960 - val_sparse_categorical_accuracy: 0.0446\n\nEpoch 00007: val_loss improved from 20.40765 to 16.39602, saving model to effb51.h5\nEpoch 8/40\n\nEpoch 00008: LearningRateScheduler reducing learning rate to 0.0006418000000000001.\n107/107 [==============================] - 132s 1s/step - loss: 11.1967 - sparse_categorical_accuracy: 0.1214 - val_loss: 14.1848 - val_sparse_categorical_accuracy: 0.0938\n\nEpoch 00008: val_loss improved from 16.39602 to 14.18476, saving model to effb51.h5\nEpoch 9/40\n\nEpoch 00009: LearningRateScheduler reducing learning rate to 0.0005144400000000001.\n107/107 [==============================] - 132s 1s/step - loss: 9.7934 - sparse_categorical_accuracy: 0.1845 - val_loss: 15.8580 - val_sparse_categorical_accuracy: 0.0352\n\nEpoch 00009: val_loss did not improve from 14.18476\nEpoch 10/40\n\nEpoch 00010: LearningRateScheduler reducing learning rate to 0.0004125520000000001.\n107/107 [==============================] - 131s 1s/step - loss: 8.9265 - sparse_categorical_accuracy: 0.2392 - val_loss: 14.8642 - val_sparse_categorical_accuracy: 0.0631\n\nEpoch 00010: val_loss did not improve from 14.18476\nEpoch 11/40\n\nEpoch 00011: LearningRateScheduler reducing learning rate to 0.0003310416000000001.\n107/107 [==============================] - 131s 1s/step - loss: 8.2016 - sparse_categorical_accuracy: 0.2887 - val_loss: 14.5234 - val_sparse_categorical_accuracy: 0.0712\n\nEpoch 00011: val_loss did not improve from 14.18476\nEpoch 12/40\n\nEpoch 00012: LearningRateScheduler reducing learning rate to 0.0002658332800000001.\n107/107 [==============================] - 131s 1s/step - loss: 7.5216 - sparse_categorical_accuracy: 0.3349 - val_loss: 17.1800 - val_sparse_categorical_accuracy: 0.0219\n\nEpoch 00012: val_loss did not improve from 14.18476\nEpoch 13/40\n\nEpoch 00013: LearningRateScheduler reducing learning rate to 0.00021366662400000008.\n107/107 [==============================] - 131s 1s/step - loss: 7.2971 - sparse_categorical_accuracy: 0.3615 - val_loss: 10.5335 - val_sparse_categorical_accuracy: 0.2940\n\nEpoch 00013: val_loss improved from 14.18476 to 10.53347, saving model to effb51.h5\nEpoch 14/40\n\nEpoch 00014: LearningRateScheduler reducing learning rate to 0.0001719332992000001.\n107/107 [==============================] - 131s 1s/step - loss: 7.0685 - sparse_categorical_accuracy: 0.3921 - val_loss: 13.5681 - val_sparse_categorical_accuracy: 0.1041\n\nEpoch 00014: val_loss did not improve from 10.53347\nEpoch 15/40\n\nEpoch 00015: LearningRateScheduler reducing learning rate to 0.00013854663936000008.\n107/107 [==============================] - 131s 1s/step - loss: 6.8848 - sparse_categorical_accuracy: 0.4030 - val_loss: 11.3031 - val_sparse_categorical_accuracy: 0.2324\n\nEpoch 00015: val_loss did not improve from 10.53347\nEpoch 16/40\n\nEpoch 00016: LearningRateScheduler reducing learning rate to 0.00011183731148800006.\n107/107 [==============================] - 131s 1s/step - loss: 6.6021 - sparse_categorical_accuracy: 0.4280 - val_loss: 9.7360 - val_sparse_categorical_accuracy: 0.3666\n\nEpoch 00016: val_loss improved from 10.53347 to 9.73597, saving model to effb51.h5\nEpoch 17/40\n\nEpoch 00017: LearningRateScheduler reducing learning rate to 9.046984919040005e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.4309 - sparse_categorical_accuracy: 0.4475 - val_loss: 9.6665 - val_sparse_categorical_accuracy: 0.3726\n\nEpoch 00017: val_loss improved from 9.73597 to 9.66649, saving model to effb51.h5\nEpoch 18/40\n\nEpoch 00018: LearningRateScheduler reducing learning rate to 7.337587935232004e-05.\n107/107 [==============================] - 131s 1s/step - loss: 6.2545 - sparse_categorical_accuracy: 0.4601 - val_loss: 9.7627 - val_sparse_categorical_accuracy: 0.3664\n\nEpoch 00018: val_loss did not improve from 9.66649\nEpoch 19/40\n\nEpoch 00019: LearningRateScheduler reducing learning rate to 5.9700703481856035e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.3460 - sparse_categorical_accuracy: 0.4603 - val_loss: 9.5032 - val_sparse_categorical_accuracy: 0.3834\n\nEpoch 00019: val_loss improved from 9.66649 to 9.50317, saving model to effb51.h5\nEpoch 20/40\n\nEpoch 00020: LearningRateScheduler reducing learning rate to 4.8760562785484834e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.1774 - sparse_categorical_accuracy: 0.4694 - val_loss: 9.4656 - val_sparse_categorical_accuracy: 0.3887\n\nEpoch 00020: val_loss improved from 9.50317 to 9.46560, saving model to effb51.h5\nEpoch 21/40\n\nEpoch 00021: LearningRateScheduler reducing learning rate to 4.000845022838787e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.0385 - sparse_categorical_accuracy: 0.4792 - val_loss: 9.4537 - val_sparse_categorical_accuracy: 0.3891\n\nEpoch 00021: val_loss improved from 9.46560 to 9.45368, saving model to effb51.h5\nEpoch 22/40\n\nEpoch 00022: LearningRateScheduler reducing learning rate to 3.30067601827103e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.1206 - sparse_categorical_accuracy: 0.4775 - val_loss: 9.4092 - val_sparse_categorical_accuracy: 0.3918\n\nEpoch 00022: val_loss improved from 9.45368 to 9.40924, saving model to effb51.h5\nEpoch 23/40\n\nEpoch 00023: LearningRateScheduler reducing learning rate to 2.740540814616824e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.9794 - sparse_categorical_accuracy: 0.4854 - val_loss: 9.3951 - val_sparse_categorical_accuracy: 0.3924\n\nEpoch 00023: val_loss improved from 9.40924 to 9.39514, saving model to effb51.h5\nEpoch 24/40\n\nEpoch 00024: LearningRateScheduler reducing learning rate to 2.2924326516934593e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.0008 - sparse_categorical_accuracy: 0.4913 - val_loss: 9.4162 - val_sparse_categorical_accuracy: 0.3930\n\nEpoch 00024: val_loss did not improve from 9.39514\nEpoch 25/40\n\nEpoch 00025: LearningRateScheduler reducing learning rate to 1.9339461213547675e-05.\n107/107 [==============================] - 131s 1s/step - loss: 5.8829 - sparse_categorical_accuracy: 0.4919 - val_loss: 9.3599 - val_sparse_categorical_accuracy: 0.3989\n\nEpoch 00025: val_loss improved from 9.39514 to 9.35987, saving model to effb51.h5\nEpoch 26/40\n\nEpoch 00026: LearningRateScheduler reducing learning rate to 1.647156897083814e-05.\n107/107 [==============================] - 131s 1s/step - loss: 5.9066 - sparse_categorical_accuracy: 0.4927 - val_loss: 9.3477 - val_sparse_categorical_accuracy: 0.3989\n\nEpoch 00026: val_loss improved from 9.35987 to 9.34770, saving model to effb51.h5\nEpoch 27/40\n\nEpoch 00027: LearningRateScheduler reducing learning rate to 1.4177255176670514e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.8805 - sparse_categorical_accuracy: 0.4988 - val_loss: 9.3511 - val_sparse_categorical_accuracy: 0.3981\n\nEpoch 00027: val_loss did not improve from 9.34770\nEpoch 28/40\n\nEpoch 00028: LearningRateScheduler reducing learning rate to 1.234180414133641e-05.\n107/107 [==============================] - 131s 1s/step - loss: 5.9085 - sparse_categorical_accuracy: 0.4971 - val_loss: 9.3378 - val_sparse_categorical_accuracy: 0.4005\n\nEpoch 00028: val_loss improved from 9.34770 to 9.33780, saving model to effb51.h5\nEpoch 29/40\n\nEpoch 00029: LearningRateScheduler reducing learning rate to 1.087344331306913e-05.\n107/107 [==============================] - 131s 1s/step - loss: 5.8348 - sparse_categorical_accuracy: 0.5052 - val_loss: 9.3352 - val_sparse_categorical_accuracy: 0.4010\n\nEpoch 00029: val_loss improved from 9.33780 to 9.33521, saving model to effb51.h5\nEpoch 30/40\n\nEpoch 00030: LearningRateScheduler reducing learning rate to 9.698754650455303e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.7373 - sparse_categorical_accuracy: 0.5069 - val_loss: 9.3206 - val_sparse_categorical_accuracy: 0.4040\n\nEpoch 00030: val_loss improved from 9.33521 to 9.32059, saving model to effb51.h5\nEpoch 31/40\n\nEpoch 00031: LearningRateScheduler reducing learning rate to 8.759003720364244e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.7986 - sparse_categorical_accuracy: 0.5002 - val_loss: 9.3201 - val_sparse_categorical_accuracy: 0.4025\n\nEpoch 00031: val_loss improved from 9.32059 to 9.32011, saving model to effb51.h5\nEpoch 32/40\n\nEpoch 00032: LearningRateScheduler reducing learning rate to 8.007202976291395e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.8987 - sparse_categorical_accuracy: 0.5029 - val_loss: 9.3102 - val_sparse_categorical_accuracy: 0.4035\n\nEpoch 00032: val_loss improved from 9.32011 to 9.31023, saving model to effb51.h5\nEpoch 33/40\n\nEpoch 00033: LearningRateScheduler reducing learning rate to 7.4057623810331166e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.7183 - sparse_categorical_accuracy: 0.5081 - val_loss: 9.3115 - val_sparse_categorical_accuracy: 0.4040\n\nEpoch 00033: val_loss did not improve from 9.31023\nEpoch 34/40\n\nEpoch 00034: LearningRateScheduler reducing learning rate to 6.9246099048264925e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.7714 - sparse_categorical_accuracy: 0.5104 - val_loss: 9.3112 - val_sparse_categorical_accuracy: 0.4041\n\nEpoch 00034: val_loss did not improve from 9.31023\nEpoch 35/40\n\nEpoch 00035: LearningRateScheduler reducing learning rate to 6.539687923861194e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.7585 - sparse_categorical_accuracy: 0.5043 - val_loss: 9.3077 - val_sparse_categorical_accuracy: 0.4041\n\nEpoch 00035: val_loss improved from 9.31023 to 9.30774, saving model to effb51.h5\nEpoch 36/40\n\nEpoch 00036: LearningRateScheduler reducing learning rate to 6.231750339088956e-06.\n107/107 [==============================] - 133s 1s/step - loss: 5.7915 - sparse_categorical_accuracy: 0.5092 - val_loss: 9.3021 - val_sparse_categorical_accuracy: 0.4052\n\nEpoch 00036: val_loss improved from 9.30774 to 9.30214, saving model to effb51.h5\nEpoch 37/40\n\nEpoch 00037: LearningRateScheduler reducing learning rate to 5.985400271271165e-06.\n107/107 [==============================] - 133s 1s/step - loss: 5.7893 - sparse_categorical_accuracy: 0.5128 - val_loss: 9.3045 - val_sparse_categorical_accuracy: 0.4059\n\nEpoch 00037: val_loss did not improve from 9.30214\nEpoch 38/40\n\nEpoch 00038: LearningRateScheduler reducing learning rate to 5.788320217016932e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.7884 - sparse_categorical_accuracy: 0.5115 - val_loss: 9.3005 - val_sparse_categorical_accuracy: 0.4056\n\nEpoch 00038: val_loss improved from 9.30214 to 9.30053, saving model to effb51.h5\nEpoch 39/40\n\nEpoch 00039: LearningRateScheduler reducing learning rate to 5.630656173613546e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.7421 - sparse_categorical_accuracy: 0.5124 - val_loss: 9.2951 - val_sparse_categorical_accuracy: 0.4055\n\nEpoch 00039: val_loss improved from 9.30053 to 9.29508, saving model to effb51.h5\nEpoch 40/40\n\nEpoch 00040: LearningRateScheduler reducing learning rate to 5.5045249388908364e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.6587 - sparse_categorical_accuracy: 0.5162 - val_loss: 9.2932 - val_sparse_categorical_accuracy: 0.4066\n\nEpoch 00040: val_loss improved from 9.29508 to 9.29319, saving model to effb51.h5\nmodel training for 1 is done\ntraining for fold 2\nEpoch 1/40\n\nEpoch 00001: LearningRateScheduler reducing learning rate to 5e-06.\n107/107 [==============================] - 285s 1s/step - loss: 23.8939 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 23.7767 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00001: val_loss improved from inf to 23.77666, saving model to effb52.h5\nEpoch 2/40\n\nEpoch 00002: LearningRateScheduler reducing learning rate to 0.00020400000000000003.\n107/107 [==============================] - 132s 1s/step - loss: 23.7398 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 23.5273 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00002: val_loss improved from 23.77666 to 23.52729, saving model to effb52.h5\nEpoch 3/40\n\nEpoch 00003: LearningRateScheduler reducing learning rate to 0.00040300000000000004.\n107/107 [==============================] - 132s 1s/step - loss: 23.0276 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 22.2379 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00003: val_loss improved from 23.52729 to 22.23787, saving model to effb52.h5\nEpoch 4/40\n\nEpoch 00004: LearningRateScheduler reducing learning rate to 0.0006020000000000001.\n107/107 [==============================] - 132s 1s/step - loss: 21.2191 - sparse_categorical_accuracy: 5.7899e-04 - val_loss: 21.5734 - val_sparse_categorical_accuracy: 0.0029\n\nEpoch 00004: val_loss improved from 22.23787 to 21.57337, saving model to effb52.h5\nEpoch 5/40\n\nEpoch 00005: LearningRateScheduler reducing learning rate to 0.0008010000000000001.\n107/107 [==============================] - 132s 1s/step - loss: 18.6118 - sparse_categorical_accuracy: 0.0115 - val_loss: 22.7055 - val_sparse_categorical_accuracy: 9.0144e-04\n\nEpoch 00005: val_loss did not improve from 21.57337\nEpoch 6/40\n\nEpoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n107/107 [==============================] - 131s 1s/step - loss: 15.8160 - sparse_categorical_accuracy: 0.0338 - val_loss: 23.8606 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00006: val_loss did not improve from 21.57337\nEpoch 7/40\n\nEpoch 00007: LearningRateScheduler reducing learning rate to 0.0008010000000000001.\n107/107 [==============================] - 131s 1s/step - loss: 13.0936 - sparse_categorical_accuracy: 0.0722 - val_loss: 25.3541 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00007: val_loss did not improve from 21.57337\nEpoch 8/40\n\nEpoch 00008: LearningRateScheduler reducing learning rate to 0.0006418000000000001.\n107/107 [==============================] - 132s 1s/step - loss: 11.2442 - sparse_categorical_accuracy: 0.1268 - val_loss: 16.8608 - val_sparse_categorical_accuracy: 0.0206\n\nEpoch 00008: val_loss improved from 21.57337 to 16.86075, saving model to effb52.h5\nEpoch 9/40\n\nEpoch 00009: LearningRateScheduler reducing learning rate to 0.0005144400000000001.\n107/107 [==============================] - 131s 1s/step - loss: 9.7046 - sparse_categorical_accuracy: 0.1941 - val_loss: 13.8906 - val_sparse_categorical_accuracy: 0.0996\n\nEpoch 00009: val_loss improved from 16.86075 to 13.89057, saving model to effb52.h5\nEpoch 10/40\n\nEpoch 00010: LearningRateScheduler reducing learning rate to 0.0004125520000000001.\n107/107 [==============================] - 132s 1s/step - loss: 8.7572 - sparse_categorical_accuracy: 0.2526 - val_loss: 12.2494 - val_sparse_categorical_accuracy: 0.1767\n\nEpoch 00010: val_loss improved from 13.89057 to 12.24943, saving model to effb52.h5\nEpoch 11/40\n\nEpoch 00011: LearningRateScheduler reducing learning rate to 0.0003310416000000001.\n107/107 [==============================] - 132s 1s/step - loss: 7.9997 - sparse_categorical_accuracy: 0.3036 - val_loss: 10.9566 - val_sparse_categorical_accuracy: 0.2712\n\nEpoch 00011: val_loss improved from 12.24943 to 10.95663, saving model to effb52.h5\nEpoch 12/40\n\nEpoch 00012: LearningRateScheduler reducing learning rate to 0.0002658332800000001.\n107/107 [==============================] - 131s 1s/step - loss: 7.5924 - sparse_categorical_accuracy: 0.3415 - val_loss: 10.5776 - val_sparse_categorical_accuracy: 0.2981\n\nEpoch 00012: val_loss improved from 10.95663 to 10.57763, saving model to effb52.h5\nEpoch 13/40\n\nEpoch 00013: LearningRateScheduler reducing learning rate to 0.00021366662400000008.\n107/107 [==============================] - 131s 1s/step - loss: 7.1699 - sparse_categorical_accuracy: 0.3751 - val_loss: 11.6237 - val_sparse_categorical_accuracy: 0.2117\n\nEpoch 00013: val_loss did not improve from 10.57763\nEpoch 14/40\n\nEpoch 00014: LearningRateScheduler reducing learning rate to 0.0001719332992000001.\n107/107 [==============================] - 132s 1s/step - loss: 6.8666 - sparse_categorical_accuracy: 0.3984 - val_loss: 10.4908 - val_sparse_categorical_accuracy: 0.3047\n\nEpoch 00014: val_loss improved from 10.57763 to 10.49080, saving model to effb52.h5\nEpoch 15/40\n\nEpoch 00015: LearningRateScheduler reducing learning rate to 0.00013854663936000008.\n107/107 [==============================] - 132s 1s/step - loss: 6.6850 - sparse_categorical_accuracy: 0.4165 - val_loss: 9.8015 - val_sparse_categorical_accuracy: 0.3667\n\nEpoch 00015: val_loss improved from 10.49080 to 9.80149, saving model to effb52.h5\nEpoch 16/40\n\nEpoch 00016: LearningRateScheduler reducing learning rate to 0.00011183731148800006.\n107/107 [==============================] - 132s 1s/step - loss: 6.4837 - sparse_categorical_accuracy: 0.4364 - val_loss: 9.7151 - val_sparse_categorical_accuracy: 0.3709\n\nEpoch 00016: val_loss improved from 9.80149 to 9.71510, saving model to effb52.h5\nEpoch 17/40\n\nEpoch 00017: LearningRateScheduler reducing learning rate to 9.046984919040005e-05.\n107/107 [==============================] - 133s 1s/step - loss: 6.4102 - sparse_categorical_accuracy: 0.4492 - val_loss: 9.7873 - val_sparse_categorical_accuracy: 0.3657\n\nEpoch 00017: val_loss did not improve from 9.71510\nEpoch 18/40\n\nEpoch 00018: LearningRateScheduler reducing learning rate to 7.337587935232004e-05.\n107/107 [==============================] - 131s 1s/step - loss: 6.2932 - sparse_categorical_accuracy: 0.4553 - val_loss: 9.6717 - val_sparse_categorical_accuracy: 0.3794\n\nEpoch 00018: val_loss improved from 9.71510 to 9.67166, saving model to effb52.h5\nEpoch 19/40\n\nEpoch 00019: LearningRateScheduler reducing learning rate to 5.9700703481856035e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.0503 - sparse_categorical_accuracy: 0.4729 - val_loss: 9.5396 - val_sparse_categorical_accuracy: 0.3885\n\nEpoch 00019: val_loss improved from 9.67166 to 9.53963, saving model to effb52.h5\nEpoch 20/40\n\nEpoch 00020: LearningRateScheduler reducing learning rate to 4.8760562785484834e-05.\n107/107 [==============================] - 131s 1s/step - loss: 6.0671 - sparse_categorical_accuracy: 0.4836 - val_loss: 9.4982 - val_sparse_categorical_accuracy: 0.3909\n\nEpoch 00020: val_loss improved from 9.53963 to 9.49823, saving model to effb52.h5\nEpoch 21/40\n\nEpoch 00021: LearningRateScheduler reducing learning rate to 4.000845022838787e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.0588 - sparse_categorical_accuracy: 0.4874 - val_loss: 9.4667 - val_sparse_categorical_accuracy: 0.3935\n\nEpoch 00021: val_loss improved from 9.49823 to 9.46674, saving model to effb52.h5\nEpoch 22/40\n\nEpoch 00022: LearningRateScheduler reducing learning rate to 3.30067601827103e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.9449 - sparse_categorical_accuracy: 0.4957 - val_loss: 9.4783 - val_sparse_categorical_accuracy: 0.3947\n\nEpoch 00022: val_loss did not improve from 9.46674\nEpoch 23/40\n\nEpoch 00023: LearningRateScheduler reducing learning rate to 2.740540814616824e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.9397 - sparse_categorical_accuracy: 0.4957 - val_loss: 9.6082 - val_sparse_categorical_accuracy: 0.3812\n\nEpoch 00023: val_loss did not improve from 9.46674\nEpoch 24/40\n\nEpoch 00024: LearningRateScheduler reducing learning rate to 2.2924326516934593e-05.\n107/107 [==============================] - 131s 1s/step - loss: 5.9992 - sparse_categorical_accuracy: 0.4984 - val_loss: 9.3875 - val_sparse_categorical_accuracy: 0.4029\n\nEpoch 00024: val_loss improved from 9.46674 to 9.38753, saving model to effb52.h5\nEpoch 25/40\n\nEpoch 00025: LearningRateScheduler reducing learning rate to 1.9339461213547675e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.8755 - sparse_categorical_accuracy: 0.5013 - val_loss: 9.3677 - val_sparse_categorical_accuracy: 0.4043\n\nEpoch 00025: val_loss improved from 9.38753 to 9.36769, saving model to effb52.h5\nEpoch 26/40\n\nEpoch 00026: LearningRateScheduler reducing learning rate to 1.647156897083814e-05.\n107/107 [==============================] - 131s 1s/step - loss: 5.8849 - sparse_categorical_accuracy: 0.5002 - val_loss: 9.3531 - val_sparse_categorical_accuracy: 0.4067\n\nEpoch 00026: val_loss improved from 9.36769 to 9.35314, saving model to effb52.h5\nEpoch 27/40\n\nEpoch 00027: LearningRateScheduler reducing learning rate to 1.4177255176670514e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.9340 - sparse_categorical_accuracy: 0.5010 - val_loss: 9.3460 - val_sparse_categorical_accuracy: 0.4069\n\nEpoch 00027: val_loss improved from 9.35314 to 9.34604, saving model to effb52.h5\nEpoch 28/40\n\nEpoch 00028: LearningRateScheduler reducing learning rate to 1.234180414133641e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.7927 - sparse_categorical_accuracy: 0.5078 - val_loss: 9.3627 - val_sparse_categorical_accuracy: 0.4072\n\nEpoch 00028: val_loss did not improve from 9.34604\nEpoch 29/40\n\nEpoch 00029: LearningRateScheduler reducing learning rate to 1.087344331306913e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.8437 - sparse_categorical_accuracy: 0.5114 - val_loss: 9.3489 - val_sparse_categorical_accuracy: 0.4076\n\nEpoch 00029: val_loss did not improve from 9.34604\nEpoch 30/40\n\nEpoch 00030: LearningRateScheduler reducing learning rate to 9.698754650455303e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.7396 - sparse_categorical_accuracy: 0.5175 - val_loss: 9.3230 - val_sparse_categorical_accuracy: 0.4096\n\nEpoch 00030: val_loss improved from 9.34604 to 9.32300, saving model to effb52.h5\nEpoch 31/40\n\nEpoch 00031: LearningRateScheduler reducing learning rate to 8.759003720364244e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.7326 - sparse_categorical_accuracy: 0.5129 - val_loss: 9.3184 - val_sparse_categorical_accuracy: 0.4105\n\nEpoch 00031: val_loss improved from 9.32300 to 9.31843, saving model to effb52.h5\nEpoch 32/40\n\nEpoch 00032: LearningRateScheduler reducing learning rate to 8.007202976291395e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.8201 - sparse_categorical_accuracy: 0.5139 - val_loss: 9.3135 - val_sparse_categorical_accuracy: 0.4106\n\nEpoch 00032: val_loss improved from 9.31843 to 9.31350, saving model to effb52.h5\nEpoch 33/40\n\nEpoch 00033: LearningRateScheduler reducing learning rate to 7.4057623810331166e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.8046 - sparse_categorical_accuracy: 0.5115 - val_loss: 9.3130 - val_sparse_categorical_accuracy: 0.4099\n\nEpoch 00033: val_loss improved from 9.31350 to 9.31305, saving model to effb52.h5\nEpoch 34/40\n\nEpoch 00034: LearningRateScheduler reducing learning rate to 6.9246099048264925e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.6952 - sparse_categorical_accuracy: 0.5138 - val_loss: 9.3087 - val_sparse_categorical_accuracy: 0.4114\n\nEpoch 00034: val_loss improved from 9.31305 to 9.30867, saving model to effb52.h5\nEpoch 35/40\n\nEpoch 00035: LearningRateScheduler reducing learning rate to 6.539687923861194e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.6977 - sparse_categorical_accuracy: 0.5213 - val_loss: 9.3006 - val_sparse_categorical_accuracy: 0.4114\n\nEpoch 00035: val_loss improved from 9.30867 to 9.30061, saving model to effb52.h5\nEpoch 36/40\n\nEpoch 00036: LearningRateScheduler reducing learning rate to 6.231750339088956e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.6110 - sparse_categorical_accuracy: 0.5226 - val_loss: 9.2901 - val_sparse_categorical_accuracy: 0.4117\n\nEpoch 00036: val_loss improved from 9.30061 to 9.29009, saving model to effb52.h5\nEpoch 37/40\n\nEpoch 00037: LearningRateScheduler reducing learning rate to 5.985400271271165e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.5918 - sparse_categorical_accuracy: 0.5261 - val_loss: 9.2837 - val_sparse_categorical_accuracy: 0.4120\n\nEpoch 00037: val_loss improved from 9.29009 to 9.28366, saving model to effb52.h5\nEpoch 38/40\n\nEpoch 00038: LearningRateScheduler reducing learning rate to 5.788320217016932e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.7170 - sparse_categorical_accuracy: 0.5202 - val_loss: 9.2897 - val_sparse_categorical_accuracy: 0.4117\n\nEpoch 00038: val_loss did not improve from 9.28366\nEpoch 39/40\n\nEpoch 00039: LearningRateScheduler reducing learning rate to 5.630656173613546e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.6032 - sparse_categorical_accuracy: 0.5197 - val_loss: 9.2828 - val_sparse_categorical_accuracy: 0.4130\n\nEpoch 00039: val_loss improved from 9.28366 to 9.28280, saving model to effb52.h5\nEpoch 40/40\n\nEpoch 00040: LearningRateScheduler reducing learning rate to 5.5045249388908364e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.5867 - sparse_categorical_accuracy: 0.5233 - val_loss: 9.2763 - val_sparse_categorical_accuracy: 0.4139\n\nEpoch 00040: val_loss improved from 9.28280 to 9.27635, saving model to effb52.h5\nmodel training for 2 is done\ntraining for fold 3\nEpoch 1/40\n\nEpoch 00001: LearningRateScheduler reducing learning rate to 5e-06.\n107/107 [==============================] - 285s 1s/step - loss: 23.8790 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 23.7249 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00001: val_loss improved from inf to 23.72488, saving model to effb53.h5\nEpoch 2/40\n\nEpoch 00002: LearningRateScheduler reducing learning rate to 0.00020400000000000003.\n107/107 [==============================] - 131s 1s/step - loss: 23.7417 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 23.5453 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00002: val_loss improved from 23.72488 to 23.54526, saving model to effb53.h5\nEpoch 3/40\n\nEpoch 00003: LearningRateScheduler reducing learning rate to 0.00040300000000000004.\n107/107 [==============================] - 132s 1s/step - loss: 23.0184 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 22.3540 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00003: val_loss improved from 23.54526 to 22.35404, saving model to effb53.h5\nEpoch 4/40\n\nEpoch 00004: LearningRateScheduler reducing learning rate to 0.0006020000000000001.\n107/107 [==============================] - 132s 1s/step - loss: 21.2412 - sparse_categorical_accuracy: 9.1811e-04 - val_loss: 21.3333 - val_sparse_categorical_accuracy: 0.0017\n\nEpoch 00004: val_loss improved from 22.35404 to 21.33333, saving model to effb53.h5\nEpoch 5/40\n\nEpoch 00005: LearningRateScheduler reducing learning rate to 0.0008010000000000001.\n107/107 [==============================] - 131s 1s/step - loss: 18.6336 - sparse_categorical_accuracy: 0.0115 - val_loss: 19.8780 - val_sparse_categorical_accuracy: 0.0090\n\nEpoch 00005: val_loss improved from 21.33333 to 19.87804, saving model to effb53.h5\nEpoch 6/40\n\nEpoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n107/107 [==============================] - 131s 1s/step - loss: 15.8387 - sparse_categorical_accuracy: 0.0344 - val_loss: 19.2697 - val_sparse_categorical_accuracy: 0.0105\n\nEpoch 00006: val_loss improved from 19.87804 to 19.26970, saving model to effb53.h5\nEpoch 7/40\n\nEpoch 00007: LearningRateScheduler reducing learning rate to 0.0008010000000000001.\n107/107 [==============================] - 132s 1s/step - loss: 13.2318 - sparse_categorical_accuracy: 0.0713 - val_loss: 23.4634 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00007: val_loss did not improve from 19.26970\nEpoch 8/40\n\nEpoch 00008: LearningRateScheduler reducing learning rate to 0.0006418000000000001.\n107/107 [==============================] - 131s 1s/step - loss: 11.2449 - sparse_categorical_accuracy: 0.1295 - val_loss: 18.3453 - val_sparse_categorical_accuracy: 0.0122\n\nEpoch 00008: val_loss improved from 19.26970 to 18.34527, saving model to effb53.h5\nEpoch 9/40\n\nEpoch 00009: LearningRateScheduler reducing learning rate to 0.0005144400000000001.\n107/107 [==============================] - 132s 1s/step - loss: 9.8288 - sparse_categorical_accuracy: 0.1908 - val_loss: 17.9443 - val_sparse_categorical_accuracy: 0.0153\n\nEpoch 00009: val_loss improved from 18.34527 to 17.94427, saving model to effb53.h5\nEpoch 10/40\n\nEpoch 00010: LearningRateScheduler reducing learning rate to 0.0004125520000000001.\n107/107 [==============================] - 131s 1s/step - loss: 8.7663 - sparse_categorical_accuracy: 0.2442 - val_loss: 12.4785 - val_sparse_categorical_accuracy: 0.1698\n\nEpoch 00010: val_loss improved from 17.94427 to 12.47845, saving model to effb53.h5\nEpoch 11/40\n\nEpoch 00011: LearningRateScheduler reducing learning rate to 0.0003310416000000001.\n107/107 [==============================] - 132s 1s/step - loss: 8.0443 - sparse_categorical_accuracy: 0.3018 - val_loss: 13.8407 - val_sparse_categorical_accuracy: 0.1011\n\nEpoch 00011: val_loss did not improve from 12.47845\nEpoch 12/40\n\nEpoch 00012: LearningRateScheduler reducing learning rate to 0.0002658332800000001.\n107/107 [==============================] - 131s 1s/step - loss: 7.6212 - sparse_categorical_accuracy: 0.3378 - val_loss: 16.8001 - val_sparse_categorical_accuracy: 0.0222\n\nEpoch 00012: val_loss did not improve from 12.47845\nEpoch 13/40\n\nEpoch 00013: LearningRateScheduler reducing learning rate to 0.00021366662400000008.\n107/107 [==============================] - 132s 1s/step - loss: 7.1793 - sparse_categorical_accuracy: 0.3701 - val_loss: 16.7238 - val_sparse_categorical_accuracy: 0.0294\n\nEpoch 00013: val_loss did not improve from 12.47845\nEpoch 14/40\n\nEpoch 00014: LearningRateScheduler reducing learning rate to 0.0001719332992000001.\n107/107 [==============================] - 132s 1s/step - loss: 6.8019 - sparse_categorical_accuracy: 0.4044 - val_loss: 13.0913 - val_sparse_categorical_accuracy: 0.1264\n\nEpoch 00014: val_loss did not improve from 12.47845\nEpoch 15/40\n\nEpoch 00015: LearningRateScheduler reducing learning rate to 0.00013854663936000008.\n107/107 [==============================] - 132s 1s/step - loss: 6.6684 - sparse_categorical_accuracy: 0.4212 - val_loss: 14.3102 - val_sparse_categorical_accuracy: 0.0751\n\nEpoch 00015: val_loss did not improve from 12.47845\nEpoch 16/40\n\nEpoch 00016: LearningRateScheduler reducing learning rate to 0.00011183731148800006.\n107/107 [==============================] - 132s 1s/step - loss: 6.4810 - sparse_categorical_accuracy: 0.4409 - val_loss: 10.2573 - val_sparse_categorical_accuracy: 0.3176\n\nEpoch 00016: val_loss improved from 12.47845 to 10.25732, saving model to effb53.h5\nEpoch 17/40\n\nEpoch 00017: LearningRateScheduler reducing learning rate to 9.046984919040005e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.3102 - sparse_categorical_accuracy: 0.4556 - val_loss: 10.7451 - val_sparse_categorical_accuracy: 0.2799\n\nEpoch 00017: val_loss did not improve from 10.25732\nEpoch 18/40\n\nEpoch 00018: LearningRateScheduler reducing learning rate to 7.337587935232004e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.2374 - sparse_categorical_accuracy: 0.4642 - val_loss: 9.5792 - val_sparse_categorical_accuracy: 0.3773\n\nEpoch 00018: val_loss improved from 10.25732 to 9.57917, saving model to effb53.h5\nEpoch 19/40\n\nEpoch 00019: LearningRateScheduler reducing learning rate to 5.9700703481856035e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.1403 - sparse_categorical_accuracy: 0.4751 - val_loss: 9.5513 - val_sparse_categorical_accuracy: 0.3803\n\nEpoch 00019: val_loss improved from 9.57917 to 9.55128, saving model to effb53.h5\nEpoch 20/40\n\nEpoch 00020: LearningRateScheduler reducing learning rate to 4.8760562785484834e-05.\n107/107 [==============================] - 133s 1s/step - loss: 6.1100 - sparse_categorical_accuracy: 0.4801 - val_loss: 9.5157 - val_sparse_categorical_accuracy: 0.3867\n\nEpoch 00020: val_loss improved from 9.55128 to 9.51565, saving model to effb53.h5\nEpoch 21/40\n\nEpoch 00021: LearningRateScheduler reducing learning rate to 4.000845022838787e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.9825 - sparse_categorical_accuracy: 0.4904 - val_loss: 9.4748 - val_sparse_categorical_accuracy: 0.3897\n\nEpoch 00021: val_loss improved from 9.51565 to 9.47476, saving model to effb53.h5\nEpoch 22/40\n\nEpoch 00022: LearningRateScheduler reducing learning rate to 3.30067601827103e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.9642 - sparse_categorical_accuracy: 0.4920 - val_loss: 9.4394 - val_sparse_categorical_accuracy: 0.3906\n\nEpoch 00022: val_loss improved from 9.47476 to 9.43942, saving model to effb53.h5\nEpoch 23/40\n\nEpoch 00023: LearningRateScheduler reducing learning rate to 2.740540814616824e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.9064 - sparse_categorical_accuracy: 0.4949 - val_loss: 9.3714 - val_sparse_categorical_accuracy: 0.3974\n\nEpoch 00023: val_loss improved from 9.43942 to 9.37141, saving model to effb53.h5\nEpoch 24/40\n\nEpoch 00024: LearningRateScheduler reducing learning rate to 2.2924326516934593e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.8321 - sparse_categorical_accuracy: 0.5034 - val_loss: 9.3369 - val_sparse_categorical_accuracy: 0.4022\n\nEpoch 00024: val_loss improved from 9.37141 to 9.33692, saving model to effb53.h5\nEpoch 25/40\n\nEpoch 00025: LearningRateScheduler reducing learning rate to 1.9339461213547675e-05.\n107/107 [==============================] - 131s 1s/step - loss: 5.8646 - sparse_categorical_accuracy: 0.5049 - val_loss: 9.3248 - val_sparse_categorical_accuracy: 0.4022\n\nEpoch 00025: val_loss improved from 9.33692 to 9.32477, saving model to effb53.h5\nEpoch 26/40\n\nEpoch 00026: LearningRateScheduler reducing learning rate to 1.647156897083814e-05.\n107/107 [==============================] - 131s 1s/step - loss: 5.8637 - sparse_categorical_accuracy: 0.5029 - val_loss: 9.2927 - val_sparse_categorical_accuracy: 0.4052\n\nEpoch 00026: val_loss improved from 9.32477 to 9.29270, saving model to effb53.h5\nEpoch 27/40\n\nEpoch 00027: LearningRateScheduler reducing learning rate to 1.4177255176670514e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.8068 - sparse_categorical_accuracy: 0.5102 - val_loss: 9.2785 - val_sparse_categorical_accuracy: 0.4069\n\nEpoch 00027: val_loss improved from 9.29270 to 9.27852, saving model to effb53.h5\nEpoch 28/40\n\nEpoch 00028: LearningRateScheduler reducing learning rate to 1.234180414133641e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.8790 - sparse_categorical_accuracy: 0.5043 - val_loss: 9.2751 - val_sparse_categorical_accuracy: 0.4069\n\nEpoch 00028: val_loss improved from 9.27852 to 9.27507, saving model to effb53.h5\nEpoch 29/40\n\nEpoch 00029: LearningRateScheduler reducing learning rate to 1.087344331306913e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.8697 - sparse_categorical_accuracy: 0.5111 - val_loss: 9.2558 - val_sparse_categorical_accuracy: 0.4096\n\nEpoch 00029: val_loss improved from 9.27507 to 9.25575, saving model to effb53.h5\nEpoch 30/40\n\nEpoch 00030: LearningRateScheduler reducing learning rate to 9.698754650455303e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.7998 - sparse_categorical_accuracy: 0.5088 - val_loss: 9.2477 - val_sparse_categorical_accuracy: 0.4090\n\nEpoch 00030: val_loss improved from 9.25575 to 9.24766, saving model to effb53.h5\nEpoch 31/40\n\nEpoch 00031: LearningRateScheduler reducing learning rate to 8.759003720364244e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.6924 - sparse_categorical_accuracy: 0.5166 - val_loss: 9.2398 - val_sparse_categorical_accuracy: 0.4099\n\nEpoch 00031: val_loss improved from 9.24766 to 9.23985, saving model to effb53.h5\nEpoch 32/40\n\nEpoch 00032: LearningRateScheduler reducing learning rate to 8.007202976291395e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.6524 - sparse_categorical_accuracy: 0.5197 - val_loss: 9.2369 - val_sparse_categorical_accuracy: 0.4096\n\nEpoch 00032: val_loss improved from 9.23985 to 9.23690, saving model to effb53.h5\nEpoch 33/40\n\nEpoch 00033: LearningRateScheduler reducing learning rate to 7.4057623810331166e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.7587 - sparse_categorical_accuracy: 0.5137 - val_loss: 9.2235 - val_sparse_categorical_accuracy: 0.4109\n\nEpoch 00033: val_loss improved from 9.23690 to 9.22349, saving model to effb53.h5\nEpoch 34/40\n\nEpoch 00034: LearningRateScheduler reducing learning rate to 6.9246099048264925e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.8563 - sparse_categorical_accuracy: 0.5098 - val_loss: 9.2160 - val_sparse_categorical_accuracy: 0.4120\n\nEpoch 00034: val_loss improved from 9.22349 to 9.21605, saving model to effb53.h5\nEpoch 35/40\n\nEpoch 00035: LearningRateScheduler reducing learning rate to 6.539687923861194e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.7737 - sparse_categorical_accuracy: 0.5160 - val_loss: 9.2091 - val_sparse_categorical_accuracy: 0.4129\n\nEpoch 00035: val_loss improved from 9.21605 to 9.20908, saving model to effb53.h5\nEpoch 36/40\n\nEpoch 00036: LearningRateScheduler reducing learning rate to 6.231750339088956e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.6228 - sparse_categorical_accuracy: 0.5239 - val_loss: 9.2055 - val_sparse_categorical_accuracy: 0.4126\n\nEpoch 00036: val_loss improved from 9.20908 to 9.20545, saving model to effb53.h5\nEpoch 37/40\n\nEpoch 00037: LearningRateScheduler reducing learning rate to 5.985400271271165e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.7561 - sparse_categorical_accuracy: 0.5169 - val_loss: 9.1983 - val_sparse_categorical_accuracy: 0.4138\n\nEpoch 00037: val_loss improved from 9.20545 to 9.19833, saving model to effb53.h5\nEpoch 38/40\n\nEpoch 00038: LearningRateScheduler reducing learning rate to 5.788320217016932e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.7212 - sparse_categorical_accuracy: 0.5226 - val_loss: 9.1945 - val_sparse_categorical_accuracy: 0.4147\n\nEpoch 00038: val_loss improved from 9.19833 to 9.19454, saving model to effb53.h5\nEpoch 39/40\n\nEpoch 00039: LearningRateScheduler reducing learning rate to 5.630656173613546e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.6823 - sparse_categorical_accuracy: 0.5215 - val_loss: 9.1897 - val_sparse_categorical_accuracy: 0.4144\n\nEpoch 00039: val_loss improved from 9.19454 to 9.18965, saving model to effb53.h5\nEpoch 40/40\n\nEpoch 00040: LearningRateScheduler reducing learning rate to 5.5045249388908364e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.7000 - sparse_categorical_accuracy: 0.5196 - val_loss: 9.1874 - val_sparse_categorical_accuracy: 0.4144\n\nEpoch 00040: val_loss improved from 9.18965 to 9.18738, saving model to effb53.h5\nmodel training for 3 is done\ntraining for fold 4\nEpoch 1/40\n\nEpoch 00001: LearningRateScheduler reducing learning rate to 5e-06.\n107/107 [==============================] - 285s 1s/step - loss: 23.8870 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 23.7719 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00001: val_loss improved from inf to 23.77194, saving model to effb54.h5\nEpoch 2/40\n\nEpoch 00002: LearningRateScheduler reducing learning rate to 0.00020400000000000003.\n107/107 [==============================] - 132s 1s/step - loss: 23.7317 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 23.5602 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00002: val_loss improved from 23.77194 to 23.56015, saving model to effb54.h5\nEpoch 3/40\n\nEpoch 00003: LearningRateScheduler reducing learning rate to 0.00040300000000000004.\n107/107 [==============================] - 132s 1s/step - loss: 23.0168 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 22.6660 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00003: val_loss improved from 23.56015 to 22.66603, saving model to effb54.h5\nEpoch 4/40\n\nEpoch 00004: LearningRateScheduler reducing learning rate to 0.0006020000000000001.\n107/107 [==============================] - 132s 1s/step - loss: 21.3180 - sparse_categorical_accuracy: 7.2508e-04 - val_loss: 21.9221 - val_sparse_categorical_accuracy: 7.5120e-04\n\nEpoch 00004: val_loss improved from 22.66603 to 21.92213, saving model to effb54.h5\nEpoch 5/40\n\nEpoch 00005: LearningRateScheduler reducing learning rate to 0.0008010000000000001.\n107/107 [==============================] - 131s 1s/step - loss: 18.9462 - sparse_categorical_accuracy: 0.0105 - val_loss: 19.4279 - val_sparse_categorical_accuracy: 0.0102\n\nEpoch 00005: val_loss improved from 21.92213 to 19.42792, saving model to effb54.h5\nEpoch 6/40\n\nEpoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n107/107 [==============================] - 132s 1s/step - loss: 16.2530 - sparse_categorical_accuracy: 0.0302 - val_loss: 23.8954 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00006: val_loss did not improve from 19.42792\nEpoch 7/40\n\nEpoch 00007: LearningRateScheduler reducing learning rate to 0.0008010000000000001.\n107/107 [==============================] - 131s 1s/step - loss: 13.4890 - sparse_categorical_accuracy: 0.0663 - val_loss: 18.3778 - val_sparse_categorical_accuracy: 0.0146\n\nEpoch 00007: val_loss improved from 19.42792 to 18.37779, saving model to effb54.h5\nEpoch 8/40\n\nEpoch 00008: LearningRateScheduler reducing learning rate to 0.0006418000000000001.\n107/107 [==============================] - 132s 1s/step - loss: 11.3910 - sparse_categorical_accuracy: 0.1213 - val_loss: 25.9972 - val_sparse_categorical_accuracy: 0.0000e+00\n\nEpoch 00008: val_loss did not improve from 18.37779\nEpoch 9/40\n\nEpoch 00009: LearningRateScheduler reducing learning rate to 0.0005144400000000001.\n107/107 [==============================] - 131s 1s/step - loss: 10.0021 - sparse_categorical_accuracy: 0.1813 - val_loss: 16.3143 - val_sparse_categorical_accuracy: 0.0341\n\nEpoch 00009: val_loss improved from 18.37779 to 16.31429, saving model to effb54.h5\nEpoch 10/40\n\nEpoch 00010: LearningRateScheduler reducing learning rate to 0.0004125520000000001.\n107/107 [==============================] - 131s 1s/step - loss: 8.8102 - sparse_categorical_accuracy: 0.2412 - val_loss: 12.7391 - val_sparse_categorical_accuracy: 0.1439\n\nEpoch 00010: val_loss improved from 16.31429 to 12.73913, saving model to effb54.h5\nEpoch 11/40\n\nEpoch 00011: LearningRateScheduler reducing learning rate to 0.0003310416000000001.\n107/107 [==============================] - 132s 1s/step - loss: 8.1871 - sparse_categorical_accuracy: 0.2863 - val_loss: 12.1229 - val_sparse_categorical_accuracy: 0.1743\n\nEpoch 00011: val_loss improved from 12.73913 to 12.12288, saving model to effb54.h5\nEpoch 12/40\n\nEpoch 00012: LearningRateScheduler reducing learning rate to 0.0002658332800000001.\n107/107 [==============================] - 131s 1s/step - loss: 7.6240 - sparse_categorical_accuracy: 0.3300 - val_loss: 18.0694 - val_sparse_categorical_accuracy: 0.0120\n\nEpoch 00012: val_loss did not improve from 12.12288\nEpoch 13/40\n\nEpoch 00013: LearningRateScheduler reducing learning rate to 0.00021366662400000008.\n107/107 [==============================] - 132s 1s/step - loss: 7.2109 - sparse_categorical_accuracy: 0.3622 - val_loss: 11.1256 - val_sparse_categorical_accuracy: 0.2467\n\nEpoch 00013: val_loss improved from 12.12288 to 11.12563, saving model to effb54.h5\nEpoch 14/40\n\nEpoch 00014: LearningRateScheduler reducing learning rate to 0.0001719332992000001.\n107/107 [==============================] - 132s 1s/step - loss: 6.9879 - sparse_categorical_accuracy: 0.3871 - val_loss: 11.1374 - val_sparse_categorical_accuracy: 0.2446\n\nEpoch 00014: val_loss did not improve from 11.12563\nEpoch 15/40\n\nEpoch 00015: LearningRateScheduler reducing learning rate to 0.00013854663936000008.\n107/107 [==============================] - 132s 1s/step - loss: 6.8318 - sparse_categorical_accuracy: 0.4129 - val_loss: 10.3502 - val_sparse_categorical_accuracy: 0.3120\n\nEpoch 00015: val_loss improved from 11.12563 to 10.35019, saving model to effb54.h5\nEpoch 16/40\n\nEpoch 00016: LearningRateScheduler reducing learning rate to 0.00011183731148800006.\n107/107 [==============================] - 131s 1s/step - loss: 6.6696 - sparse_categorical_accuracy: 0.4260 - val_loss: 10.0539 - val_sparse_categorical_accuracy: 0.3383\n\nEpoch 00016: val_loss improved from 10.35019 to 10.05395, saving model to effb54.h5\nEpoch 17/40\n\nEpoch 00017: LearningRateScheduler reducing learning rate to 9.046984919040005e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.4621 - sparse_categorical_accuracy: 0.4431 - val_loss: 10.0356 - val_sparse_categorical_accuracy: 0.3403\n\nEpoch 00017: val_loss improved from 10.05395 to 10.03562, saving model to effb54.h5\nEpoch 18/40\n\nEpoch 00018: LearningRateScheduler reducing learning rate to 7.337587935232004e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.3797 - sparse_categorical_accuracy: 0.4539 - val_loss: 9.8718 - val_sparse_categorical_accuracy: 0.3586\n\nEpoch 00018: val_loss improved from 10.03562 to 9.87183, saving model to effb54.h5\nEpoch 19/40\n\nEpoch 00019: LearningRateScheduler reducing learning rate to 5.9700703481856035e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.1986 - sparse_categorical_accuracy: 0.4600 - val_loss: 9.8378 - val_sparse_categorical_accuracy: 0.3615\n\nEpoch 00019: val_loss improved from 9.87183 to 9.83781, saving model to effb54.h5\nEpoch 20/40\n\nEpoch 00020: LearningRateScheduler reducing learning rate to 4.8760562785484834e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.0753 - sparse_categorical_accuracy: 0.4759 - val_loss: 9.7392 - val_sparse_categorical_accuracy: 0.3699\n\nEpoch 00020: val_loss improved from 9.83781 to 9.73919, saving model to effb54.h5\nEpoch 21/40\n\nEpoch 00021: LearningRateScheduler reducing learning rate to 4.000845022838787e-05.\n107/107 [==============================] - 131s 1s/step - loss: 6.1978 - sparse_categorical_accuracy: 0.4702 - val_loss: 9.7338 - val_sparse_categorical_accuracy: 0.3697\n\nEpoch 00021: val_loss improved from 9.73919 to 9.73382, saving model to effb54.h5\nEpoch 22/40\n\nEpoch 00022: LearningRateScheduler reducing learning rate to 3.30067601827103e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.1199 - sparse_categorical_accuracy: 0.4760 - val_loss: 9.7577 - val_sparse_categorical_accuracy: 0.3691\n\nEpoch 00022: val_loss did not improve from 9.73382\nEpoch 23/40\n\nEpoch 00023: LearningRateScheduler reducing learning rate to 2.740540814616824e-05.\n107/107 [==============================] - 131s 1s/step - loss: 6.0465 - sparse_categorical_accuracy: 0.4788 - val_loss: 9.6810 - val_sparse_categorical_accuracy: 0.3759\n\nEpoch 00023: val_loss improved from 9.73382 to 9.68104, saving model to effb54.h5\nEpoch 24/40\n\nEpoch 00024: LearningRateScheduler reducing learning rate to 2.2924326516934593e-05.\n107/107 [==============================] - 131s 1s/step - loss: 6.0301 - sparse_categorical_accuracy: 0.4849 - val_loss: 9.6204 - val_sparse_categorical_accuracy: 0.3818\n\nEpoch 00024: val_loss improved from 9.68104 to 9.62045, saving model to effb54.h5\nEpoch 25/40\n\nEpoch 00025: LearningRateScheduler reducing learning rate to 1.9339461213547675e-05.\n107/107 [==============================] - 132s 1s/step - loss: 6.0361 - sparse_categorical_accuracy: 0.4826 - val_loss: 9.6705 - val_sparse_categorical_accuracy: 0.3818\n\nEpoch 00025: val_loss did not improve from 9.62045\nEpoch 26/40\n\nEpoch 00026: LearningRateScheduler reducing learning rate to 1.647156897083814e-05.\n107/107 [==============================] - 131s 1s/step - loss: 5.9687 - sparse_categorical_accuracy: 0.4927 - val_loss: 9.5748 - val_sparse_categorical_accuracy: 0.3885\n\nEpoch 00026: val_loss improved from 9.62045 to 9.57479, saving model to effb54.h5\nEpoch 27/40\n\nEpoch 00027: LearningRateScheduler reducing learning rate to 1.4177255176670514e-05.\n107/107 [==============================] - 133s 1s/step - loss: 5.8921 - sparse_categorical_accuracy: 0.4955 - val_loss: 9.5760 - val_sparse_categorical_accuracy: 0.3881\n\nEpoch 00027: val_loss did not improve from 9.57479\nEpoch 28/40\n\nEpoch 00028: LearningRateScheduler reducing learning rate to 1.234180414133641e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.8871 - sparse_categorical_accuracy: 0.4948 - val_loss: 9.5613 - val_sparse_categorical_accuracy: 0.3884\n\nEpoch 00028: val_loss improved from 9.57479 to 9.56133, saving model to effb54.h5\nEpoch 29/40\n\nEpoch 00029: LearningRateScheduler reducing learning rate to 1.087344331306913e-05.\n107/107 [==============================] - 132s 1s/step - loss: 5.9197 - sparse_categorical_accuracy: 0.4989 - val_loss: 9.5468 - val_sparse_categorical_accuracy: 0.3905\n\nEpoch 00029: val_loss improved from 9.56133 to 9.54685, saving model to effb54.h5\nEpoch 30/40\n\nEpoch 00030: LearningRateScheduler reducing learning rate to 9.698754650455303e-06.\n107/107 [==============================] - 131s 1s/step - loss: 5.9342 - sparse_categorical_accuracy: 0.5009 - val_loss: 9.5382 - val_sparse_categorical_accuracy: 0.3918\n\nEpoch 00030: val_loss improved from 9.54685 to 9.53816, saving model to effb54.h5\nEpoch 31/40\n\nEpoch 00031: LearningRateScheduler reducing learning rate to 8.759003720364244e-06.\n107/107 [==============================] - 133s 1s/step - loss: 5.8813 - sparse_categorical_accuracy: 0.5033 - val_loss: 9.5313 - val_sparse_categorical_accuracy: 0.3929\n\nEpoch 00031: val_loss improved from 9.53816 to 9.53126, saving model to effb54.h5\nEpoch 32/40\n\nEpoch 00032: LearningRateScheduler reducing learning rate to 8.007202976291395e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.9705 - sparse_categorical_accuracy: 0.4959 - val_loss: 9.5285 - val_sparse_categorical_accuracy: 0.3923\n\nEpoch 00032: val_loss improved from 9.53126 to 9.52850, saving model to effb54.h5\nEpoch 33/40\n\nEpoch 00033: LearningRateScheduler reducing learning rate to 7.4057623810331166e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.9514 - sparse_categorical_accuracy: 0.4977 - val_loss: 9.5223 - val_sparse_categorical_accuracy: 0.3942\n\nEpoch 00033: val_loss improved from 9.52850 to 9.52232, saving model to effb54.h5\nEpoch 34/40\n\nEpoch 00034: LearningRateScheduler reducing learning rate to 6.9246099048264925e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.8115 - sparse_categorical_accuracy: 0.5033 - val_loss: 9.5110 - val_sparse_categorical_accuracy: 0.3947\n\nEpoch 00034: val_loss improved from 9.52232 to 9.51103, saving model to effb54.h5\nEpoch 35/40\n\nEpoch 00035: LearningRateScheduler reducing learning rate to 6.539687923861194e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.8002 - sparse_categorical_accuracy: 0.5078 - val_loss: 9.5039 - val_sparse_categorical_accuracy: 0.3951\n\nEpoch 00035: val_loss improved from 9.51103 to 9.50388, saving model to effb54.h5\nEpoch 36/40\n\nEpoch 00036: LearningRateScheduler reducing learning rate to 6.231750339088956e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.8804 - sparse_categorical_accuracy: 0.5069 - val_loss: 9.4980 - val_sparse_categorical_accuracy: 0.3956\n\nEpoch 00036: val_loss improved from 9.50388 to 9.49804, saving model to effb54.h5\nEpoch 37/40\n\nEpoch 00037: LearningRateScheduler reducing learning rate to 5.985400271271165e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.8246 - sparse_categorical_accuracy: 0.5104 - val_loss: 9.4954 - val_sparse_categorical_accuracy: 0.3959\n\nEpoch 00037: val_loss improved from 9.49804 to 9.49542, saving model to effb54.h5\nEpoch 38/40\n\nEpoch 00038: LearningRateScheduler reducing learning rate to 5.788320217016932e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.7145 - sparse_categorical_accuracy: 0.5104 - val_loss: 9.4848 - val_sparse_categorical_accuracy: 0.3956\n\nEpoch 00038: val_loss improved from 9.49542 to 9.48483, saving model to effb54.h5\nEpoch 39/40\n\nEpoch 00039: LearningRateScheduler reducing learning rate to 5.630656173613546e-06.\n107/107 [==============================] - 132s 1s/step - loss: 5.8001 - sparse_categorical_accuracy: 0.5067 - val_loss: 9.4755 - val_sparse_categorical_accuracy: 0.3972\n\nEpoch 00039: val_loss improved from 9.48483 to 9.47552, saving model to effb54.h5\nEpoch 40/40\n\nEpoch 00040: LearningRateScheduler reducing learning rate to 5.5045249388908364e-06.\n107/107 [==============================] - 133s 1s/step - loss: 5.7890 - sparse_categorical_accuracy: 0.5125 - val_loss: 9.4785 - val_sparse_categorical_accuracy: 0.3969\n\nEpoch 00040: val_loss did not improve from 9.47552\nmodel training for 4 is done\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Experiments 5 fold error\nwith dense\n1) B0->1e-5 to 4e-4 -> 12.648266\n   B0->1e-6 to 4e-5 -> \"very high error\"\n   B0 ->1e-6 to 4e-4 -> 12.823738\nwithout dense\n1) B0->1e-5 to 4e-4 -> 13.708846\n##################################################\n(only 1 fold)\nno dense\n1) B0->1e-5 to 4e-4->13.8519\n2) B0-> 1e-5 to 1e-3->11.1301\n   B0-> 1e-6 to 1e-3->11.08619\nWith dense\n1) B0->1e-5 to 4e-4-> 12.53761\n2) B0-> 1e-5 to 1e-3->10.94927\n   B0-> 1e-6 to 1e-3->10.72708(30 epochs)\n###############################################\nB3(dense)\n1) B3 -> 1e-6 to 1e-3 -> 10.26380(50 epochs)\n2) B3 -> 5e-6 to 1e-3 -> 10.11745(44 epochs)\nB3(no dense)\n5e-6 to 1e-3->10.06651(40 epochs)\n1e-6 to 1e-3 -> 10.17261\n1e-5 to 4e-4 -> 11.89102\n#may be during training I observed that in b3 case dense layer is causing overfiitting at first.\n##########################################\nB5(no dense)\n5e-6 to 1e-3 -> 9.59691\nB5(no dense removed shift scale rotate)\n5 fold error\n5e-6 to 1e-3 ->9.31131","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"","metadata":{},"execution_count":null,"outputs":[]}]}