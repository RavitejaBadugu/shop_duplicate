{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:32.569826Z","iopub.status.busy":"2022-06-27T12:45:32.569454Z","iopub.status.idle":"2022-06-27T12:45:40.105901Z","shell.execute_reply":"2022-06-27T12:45:40.104540Z","shell.execute_reply.started":"2022-06-27T12:45:32.569731Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-06-27 12:45:33.644881: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n","2022-06-27 12:45:33.645022: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"]}],"source":["import os\n","import math\n","import pandas as pd\n","from tqdm import tqdm\n","from kaggle_datasets import KaggleDatasets\n","import cv2\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow.keras.backend as K"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:40.108340Z","iopub.status.busy":"2022-06-27T12:45:40.108052Z","iopub.status.idle":"2022-06-27T12:45:40.113762Z","shell.execute_reply":"2022-06-27T12:45:40.112691Z","shell.execute_reply.started":"2022-06-27T12:45:40.108309Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(action='ignore')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:40.115901Z","iopub.status.busy":"2022-06-27T12:45:40.115521Z","iopub.status.idle":"2022-06-27T12:45:46.268777Z","shell.execute_reply":"2022-06-27T12:45:46.267721Z","shell.execute_reply.started":"2022-06-27T12:45:40.115864Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on TPU  grpc://10.0.0.2:8470\n"]},{"name":"stderr","output_type":"stream","text":["2022-06-27 12:45:40.129071: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","2022-06-27 12:45:40.132434: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n","2022-06-27 12:45:40.132494: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n","2022-06-27 12:45:40.132535: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (7f0badcf809d): /proc/driver/nvidia/version does not exist\n","2022-06-27 12:45:40.137640: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-06-27 12:45:40.139309: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","2022-06-27 12:45:40.147192: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","2022-06-27 12:45:40.189175: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n","2022-06-27 12:45:40.189254: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30042}\n","2022-06-27 12:45:40.209227: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n","2022-06-27 12:45:40.209291: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30042}\n","2022-06-27 12:45:40.211425: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30042\n"]},{"name":"stdout","output_type":"stream","text":["REPLICAS:  8\n"]}],"source":["try:\n","    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n","    # set: this is always the case on Kaggle.\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print('Running on TPU ', tpu.master())\n","except ValueError:\n","    tpu = None\n","\n","if tpu:\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.TPUStrategy(tpu)\n","else:\n","    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n","    strategy = tf.distribute.get_strategy()\n","\n","print(\"REPLICAS: \", strategy.num_replicas_in_sync)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:46.271833Z","iopub.status.busy":"2022-06-27T12:45:46.271442Z","iopub.status.idle":"2022-06-27T12:45:46.277676Z","shell.execute_reply":"2022-06-27T12:45:46.276792Z","shell.execute_reply.started":"2022-06-27T12:45:46.271783Z"},"trusted":true},"outputs":[],"source":["BATCH_SIZE=32 * strategy.num_replicas_in_sync\n","IMAGE_SIZE=[512,512]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:46.279913Z","iopub.status.busy":"2022-06-27T12:45:46.279184Z","iopub.status.idle":"2022-06-27T12:45:46.291585Z","shell.execute_reply":"2022-06-27T12:45:46.290725Z","shell.execute_reply.started":"2022-06-27T12:45:46.279867Z"},"trusted":true},"outputs":[],"source":["def int_feature(int_list):\n","    if isinstance(int_list,type(tf.cast(1,dtype=tf.int32))):\n","        int_list=int_list.numpy()\n","    return tf.train.Feature(int64_list=tf.train.Int64List(value=[int_list]))\n","\n","def bytes_feature(value):\n","    if isinstance(value,type(tf.cast(1,dtype=tf.float32))):\n","        value=value.numpy()\n","    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:46.293556Z","iopub.status.busy":"2022-06-27T12:45:46.293295Z","iopub.status.idle":"2022-06-27T12:45:46.301867Z","shell.execute_reply":"2022-06-27T12:45:46.301137Z","shell.execute_reply.started":"2022-06-27T12:45:46.293526Z"},"trusted":true},"outputs":[],"source":["def Searlize_to_string(image_feature,label_feature):\n","    feature={\n","        \"image\":image_feature,\n","        \"label\":label_feature\n","    }\n","    example=tf.train.Example(features=tf.train.Features(feature=feature))\n","    return example.SerializeToString()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:46.303436Z","iopub.status.busy":"2022-06-27T12:45:46.303219Z","iopub.status.idle":"2022-06-27T12:45:46.311572Z","shell.execute_reply":"2022-06-27T12:45:46.310815Z","shell.execute_reply.started":"2022-06-27T12:45:46.303411Z"},"trusted":true},"outputs":[],"source":["#!python EDA.py --data_path train.csv --images_base_path train_images\n","#!python create_fold.py --data_path \"processed_data/cleaned_data.csv\""]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:46.316336Z","iopub.status.busy":"2022-06-27T12:45:46.313078Z","iopub.status.idle":"2022-06-27T12:45:46.651471Z","shell.execute_reply":"2022-06-27T12:45:46.650354Z","shell.execute_reply.started":"2022-06-27T12:45:46.316291Z"},"trusted":true},"outputs":[],"source":["df=pd.read_csv(\"../input/shop-tpu-folds/fold_data.csv\")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:46.653091Z","iopub.status.busy":"2022-06-27T12:45:46.652776Z","iopub.status.idle":"2022-06-27T12:45:46.678356Z","shell.execute_reply":"2022-06-27T12:45:46.676580Z","shell.execute_reply.started":"2022-06-27T12:45:46.652996Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>posting_id</th>\n","      <th>image</th>\n","      <th>image_phash</th>\n","      <th>title</th>\n","      <th>label_group</th>\n","      <th>image_path</th>\n","      <th>gfold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>train_129225211</td>\n","      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n","      <td>94974f937d4c2433</td>\n","      <td>Paper Bag Victoria Secret</td>\n","      <td>666</td>\n","      <td>train_images\\0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>train_3386243561</td>\n","      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n","      <td>af3f9460c2838f0f</td>\n","      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n","      <td>7572</td>\n","      <td>train_images\\00039780dfc94d01db8676fe789ecd05.jpg</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>train_2288590299</td>\n","      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n","      <td>b94cb00ed3e50f78</td>\n","      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n","      <td>6172</td>\n","      <td>train_images\\000a190fdd715a2a36faed16e2c65df7.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>train_2406599165</td>\n","      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n","      <td>8514fc58eafea283</td>\n","      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n","      <td>10509</td>\n","      <td>train_images\\00117e4fc239b1b641ff08340b429633.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>train_3369186413</td>\n","      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n","      <td>a6f319f924ad708c</td>\n","      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n","      <td>9425</td>\n","      <td>train_images\\00136d1cf4edede0203f32f05f660588.jpg</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         posting_id                                 image       image_phash  \\\n","0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n","1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n","2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n","3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n","4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n","\n","                                               title  label_group  \\\n","0                          Paper Bag Victoria Secret          666   \n","1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...         7572   \n","2        Maling TTS Canned Pork Luncheon Meat 397 gr         6172   \n","3  Daster Batik Lengan pendek - Motif Acak / Camp...        10509   \n","4                  Nescafe \\xc3\\x89clair Latte 220ml         9425   \n","\n","                                          image_path  gfold  \n","0  train_images\\0000a68812bc7e98c42888dfb1c07da0.jpg      0  \n","1  train_images\\00039780dfc94d01db8676fe789ecd05.jpg      2  \n","2  train_images\\000a190fdd715a2a36faed16e2c65df7.jpg      0  \n","3  train_images\\00117e4fc239b1b641ff08340b429633.jpg      1  \n","4  train_images\\00136d1cf4edede0203f32f05f660588.jpg      3  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:46.681786Z","iopub.status.busy":"2022-06-27T12:45:46.681469Z","iopub.status.idle":"2022-06-27T12:45:46.711108Z","shell.execute_reply":"2022-06-27T12:45:46.709975Z","shell.execute_reply.started":"2022-06-27T12:45:46.681748Z"},"trusted":true},"outputs":[],"source":["train_steps={}\n","valid_steps={}\n","for fold in range(5):\n","    valid_data=df.loc[df['gfold']==fold].reset_index(drop=True)\n","    valid_steps[fold]=valid_data.shape[0]\n","    train_steps[fold]=df.shape[0]-valid_data.shape[0]"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:46.713192Z","iopub.status.busy":"2022-06-27T12:45:46.712672Z","iopub.status.idle":"2022-06-27T12:45:46.719881Z","shell.execute_reply":"2022-06-27T12:45:46.719070Z","shell.execute_reply.started":"2022-06-27T12:45:46.713157Z"},"trusted":true},"outputs":[{"data":{"text/plain":["({0: 27400, 1: 27400, 2: 27400, 3: 27400, 4: 27400},\n"," {0: 6850, 1: 6850, 2: 6850, 3: 6850, 4: 6850})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["train_steps,valid_steps"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:46.721851Z","iopub.status.busy":"2022-06-27T12:45:46.721450Z","iopub.status.idle":"2022-06-27T12:45:46.729767Z","shell.execute_reply":"2022-06-27T12:45:46.729022Z","shell.execute_reply.started":"2022-06-27T12:45:46.721819Z"},"trusted":true},"outputs":[],"source":["#import os\n","#os.mkdir(\"tfrecords\")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:46.731517Z","iopub.status.busy":"2022-06-27T12:45:46.730933Z","iopub.status.idle":"2022-06-27T12:45:46.741410Z","shell.execute_reply":"2022-06-27T12:45:46.740359Z","shell.execute_reply.started":"2022-06-27T12:45:46.731480Z"},"trusted":true},"outputs":[],"source":["#for fold in tqdm(range(5)):\n","#    fold_data=df.loc[df['gfold']==fold].reset_index(drop=True)\n","#    with tf.io.TFRecordWriter(f\"tfrecords/fold_{fold}_512.tfrec\") as writer:\n","#        for i in fold_data.index:\n","#            img=tf.keras.preprocessing.image.load_img(fold_data.loc[i,'image_path'],target_size=IMAGE_SIZE)\n","#            img=tf.keras.preprocessing.image.img_to_array(img)\n","#            img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, 94))[1].tostring()\n","#            img_bytes_feature=bytes_feature(img)\n","#            label_feature=int_feature(int(fold_data.loc[i,'label_group']))\n","#            example_string=Searlize_to_string(img_bytes_feature,label_feature)\n","#            writer.write(example_string)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:46.744271Z","iopub.status.busy":"2022-06-27T12:45:46.742801Z","iopub.status.idle":"2022-06-27T12:45:47.258023Z","shell.execute_reply":"2022-06-27T12:45:47.256737Z","shell.execute_reply.started":"2022-06-27T12:45:46.744217Z"},"trusted":true},"outputs":[],"source":["GCS_PATH = KaggleDatasets().get_gcs_path('shop-512-size-tfrecords')"]},{"cell_type":"markdown","metadata":{},"source":["# DATA AUG"]},{"cell_type":"markdown","metadata":{},"source":["https://www.kaggle.com/code/cdeotte/rotation-augmentation-gpu-tpu-0-96/notebook"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:47.259832Z","iopub.status.busy":"2022-06-27T12:45:47.259540Z","iopub.status.idle":"2022-06-27T12:45:47.273328Z","shell.execute_reply":"2022-06-27T12:45:47.272567Z","shell.execute_reply.started":"2022-06-27T12:45:47.259791Z"},"trusted":true},"outputs":[],"source":["def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n","    # returns 3x3 transformmatrix which transforms indicies\n","        \n","    # CONVERT DEGREES TO RADIANS\n","    rotation = math.pi * rotation / 180.\n","    shear = math.pi * shear / 180.\n","    \n","    # ROTATION MATRIX\n","    c1 = tf.math.cos(rotation)\n","    s1 = tf.math.sin(rotation)\n","    one = tf.constant([1],dtype='float32')\n","    zero = tf.constant([0],dtype='float32')\n","    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n","        \n","    # SHEAR MATRIX\n","    c2 = tf.math.cos(shear)\n","    s2 = tf.math.sin(shear)\n","    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n","    \n","    # ZOOM MATRIX\n","    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n","    \n","    # SHIFT MATRIX\n","    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n","    \n","    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:47.275235Z","iopub.status.busy":"2022-06-27T12:45:47.274454Z","iopub.status.idle":"2022-06-27T12:45:47.291926Z","shell.execute_reply":"2022-06-27T12:45:47.290965Z","shell.execute_reply.started":"2022-06-27T12:45:47.275183Z"},"trusted":true},"outputs":[],"source":["def SHIFT_ROTATE_ZOOM(image,label):\n","    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n","    # output - image randomly rotated, sheared, zoomed, and shifted\n","    DIM = IMAGE_SIZE[0]\n","    XDIM = DIM%2 #fix for size 331\n","    \n","    rot = 15. * tf.random.normal([1],dtype='float32')\n","    shr = 5. * tf.random.normal([1],dtype='float32') \n","    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n","    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n","    h_shift = 16. * tf.random.normal([1],dtype='float32') \n","    w_shift = 16. * tf.random.normal([1],dtype='float32') \n","  \n","    # GET TRANSFORMATION MATRIX\n","    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n","\n","    # LIST DESTINATION PIXEL INDICES\n","    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n","    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n","    z = tf.ones([DIM*DIM],dtype='int32')\n","    idx = tf.stack( [x,y,z] )\n","    \n","    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n","    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n","    idx2 = K.cast(idx2,dtype='int32')\n","    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n","    \n","    # FIND ORIGIN PIXEL VALUES           \n","    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n","    d = tf.gather_nd(image,tf.transpose(idx3))\n","        \n","    return tf.reshape(d,[DIM,DIM,3]),label"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:47.293917Z","iopub.status.busy":"2022-06-27T12:45:47.293564Z","iopub.status.idle":"2022-06-27T12:45:47.308502Z","shell.execute_reply":"2022-06-27T12:45:47.307458Z","shell.execute_reply.started":"2022-06-27T12:45:47.293872Z"},"trusted":true},"outputs":[],"source":["def Desearlize(example):\n","    ex={\n","        \"image\":tf.io.FixedLenFeature([],dtype=tf.string),\n","        \"label\":tf.io.FixedLenFeature([],dtype=tf.int64)\n","    }\n","    features=tf.io.parse_single_example(example,ex)\n","    image,label=features['image'],features['label']\n","    label=tf.cast(label,dtype=tf.int32)\n","    image=tf.image.decode_jpeg(image,channels=3)\n","    image=tf.image.resize(image,IMAGE_SIZE)\n","    image=tf.cast(image,dtype=tf.float32)/255.0\n","    ex={\"image\":image,\"label\":label}\n","    return ex"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:47.310306Z","iopub.status.busy":"2022-06-27T12:45:47.310019Z","iopub.status.idle":"2022-06-27T12:45:47.319997Z","shell.execute_reply":"2022-06-27T12:45:47.319114Z","shell.execute_reply.started":"2022-06-27T12:45:47.310275Z"},"trusted":true},"outputs":[],"source":["def APPLY_AUG(example):\n","    image,label=example['image'],example['label']\n","    image,label=SHIFT_ROTATE_ZOOM(image,label)\n","    image=tf.image.random_flip_left_right(image,)\n","    image=tf.image.random_brightness(image,0.2)\n","    image=tf.image.random_contrast(image,0.02,1)\n","    return image,label"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:47.321759Z","iopub.status.busy":"2022-06-27T12:45:47.321380Z","iopub.status.idle":"2022-06-27T12:45:47.335789Z","shell.execute_reply":"2022-06-27T12:45:47.334963Z","shell.execute_reply.started":"2022-06-27T12:45:47.321723Z"},"trusted":true},"outputs":[],"source":["def Train_input(image,label):\n","    return (image,label),label\n","def Valid_input(example):\n","    image,label=example['image'],example['label']\n","    return (image,label),label"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:47.337705Z","iopub.status.busy":"2022-06-27T12:45:47.337233Z","iopub.status.idle":"2022-06-27T12:45:47.350163Z","shell.execute_reply":"2022-06-27T12:45:47.348978Z","shell.execute_reply.started":"2022-06-27T12:45:47.337649Z"},"trusted":true},"outputs":[],"source":["def Valid_data_pipe(files):\n","    AUTO=tf.data.experimental.AUTOTUNE\n","    dataset = tf.data.TFRecordDataset(files, num_parallel_reads = AUTO)\n","    dataset=dataset.map(Desearlize,num_parallel_calls=AUTO)\n","    dataset=dataset.map(Valid_input,num_parallel_calls=AUTO)\n","    dataset=dataset.batch(BATCH_SIZE,drop_remainder=True)\n","    dataset=dataset.prefetch(AUTO)\n","    return dataset\n","\n","def Train_data_pipe(files):\n","    ignore_order = tf.data.Options()\n","    ignore_order.experimental_deterministic = False\n","    AUTO=tf.data.experimental.AUTOTUNE\n","    dataset = tf.data.TFRecordDataset(files, num_parallel_reads = AUTO )\n","    dataset=dataset.with_options(ignore_order)\n","    dataset=dataset.map(Desearlize,num_parallel_calls=AUTO)\n","    dataset=dataset.map(APPLY_AUG,num_parallel_calls=AUTO)\n","    dataset=dataset.map(Train_input,num_parallel_calls=AUTO)\n","    dataset = dataset.repeat()\n","    dataset=dataset.shuffle(1024)\n","    dataset=dataset.batch(BATCH_SIZE)\n","    dataset=dataset.prefetch(AUTO)\n","    return dataset"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:47.352569Z","iopub.status.busy":"2022-06-27T12:45:47.351472Z","iopub.status.idle":"2022-06-27T12:45:47.465483Z","shell.execute_reply":"2022-06-27T12:45:47.464494Z","shell.execute_reply.started":"2022-06-27T12:45:47.352520Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-06-27 12:45:47.364692: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n"]}],"source":["files=tf.io.gfile.glob(f\"{GCS_PATH}/tfrecords/fold_*.tfrec\")"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:47.467914Z","iopub.status.busy":"2022-06-27T12:45:47.467358Z","iopub.status.idle":"2022-06-27T12:45:47.473348Z","shell.execute_reply":"2022-06-27T12:45:47.472484Z","shell.execute_reply.started":"2022-06-27T12:45:47.467862Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.applications import DenseNet201"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:47.475968Z","iopub.status.busy":"2022-06-27T12:45:47.475065Z","iopub.status.idle":"2022-06-27T12:45:47.492895Z","shell.execute_reply":"2022-06-27T12:45:47.491569Z","shell.execute_reply.started":"2022-06-27T12:45:47.475919Z"},"trusted":true},"outputs":[],"source":["class ARCFACE_LAYER(tf.keras.layers.Layer):\n","    def __init__(self,m=0.5,s=30,n_classes=11014):\n","        super(ARCFACE_LAYER,self).__init__()\n","        self.m=m\n","        self.s=s\n","        self.sin_m=tf.sin(m)\n","        self.cos_m=tf.cos(m)\n","        self.n_classes=n_classes\n","        self.threshold = tf.cos(math.pi - m)\n","        self.mm = tf.math.sin(math.pi - m) * m\n","\n","    def build(self,input_shape):\n","        prev_layer_units=input_shape[0][1]\n","        self.w=self.add_weight(shape=(prev_layer_units,self.n_classes),trainable=True,\n","                              initializer='glorot_uniform')\n","\n","    def get_config(self):\n","        config=super().get_config()\n","        config.update({\"m\":0.5,\n","                       \"s\":30,\n","                       \"n_classes\":11014})\n","        return config\n","\n","\n","    def call(self,inputs):\n","        prev_layer,y=inputs\n","        y=tf.cast(y,dtype=tf.int32)\n","        y_hot=tf.one_hot(y,self.n_classes)\n","        y_hot=tf.cast(y_hot,dtype=tf.float32)\n","        w_norm=tf.linalg.l2_normalize(self.w,axis=0)\n","        x_norm=tf.linalg.l2_normalize(prev_layer,axis=1)\n","        cos_theta=tf.linalg.matmul(x_norm,w_norm)\n","        sin_theta=tf.sqrt(1-tf.pow(cos_theta,tf.cast(2,dtype=tf.float32)))\n","        cos_theta_m=(cos_theta*self.cos_m)-(sin_theta*self.sin_m)\n","        cos_theta_m=tf.where(cos_theta>self.threshold,cos_theta_m,cos_theta-self.mm)\n","        final=self.s*((y_hot*cos_theta_m)+((1-y_hot)*cos_theta))\n","        return final"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:47.495567Z","iopub.status.busy":"2022-06-27T12:45:47.494637Z","iopub.status.idle":"2022-06-27T12:45:47.508933Z","shell.execute_reply":"2022-06-27T12:45:47.507817Z","shell.execute_reply.started":"2022-06-27T12:45:47.495519Z"},"trusted":true},"outputs":[],"source":["LR_START = 0.000005\n","LR_MAX   = 0.001#0.00005 * strategy.num_replicas_in_sync\n","LR_MIN   = 0.000005\n","LR_RAMPUP_EPOCHS = 5\n","LR_EXP_DECAY = .8\n","\n","def lrfn(epoch):\n","    if epoch < LR_RAMPUP_EPOCHS:\n","        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n","    elif epoch < LR_RAMPUP_EPOCHS:\n","        lr = LR_MAX\n","    else:\n","        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS) + LR_MIN\n","    return lr"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-06-27T12:45:47.513822Z","iopub.status.busy":"2022-06-27T12:45:47.512786Z","iopub.status.idle":"2022-06-27T17:00:23.086374Z","shell.execute_reply":"2022-06-27T17:00:23.085320Z","shell.execute_reply.started":"2022-06-27T12:45:47.513763Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n","74842112/74836368 [==============================] - 1s 0us/step\n","training for fold 3\n","Epoch 1/60\n","\n","Epoch 00001: LearningRateScheduler reducing learning rate to 5e-06.\n","107/107 [==============================] - 383s 1s/step - loss: 24.2594 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 24.0777 - val_sparse_categorical_accuracy: 0.0000e+00\n","\n","Epoch 00001: val_loss improved from inf to 24.07773, saving model to densenet201_720_3.h5\n","Epoch 2/60\n","\n","Epoch 00002: LearningRateScheduler reducing learning rate to 0.00020400000000000003.\n","107/107 [==============================] - 119s 1s/step - loss: 23.6262 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 22.3143 - val_sparse_categorical_accuracy: 0.0000e+00\n","\n","Epoch 00002: val_loss improved from 24.07773 to 22.31434, saving model to densenet201_720_3.h5\n","Epoch 3/60\n","\n","Epoch 00003: LearningRateScheduler reducing learning rate to 0.00040300000000000004.\n","107/107 [==============================] - 117s 1s/step - loss: 20.8047 - sparse_categorical_accuracy: 0.0045 - val_loss: 18.9916 - val_sparse_categorical_accuracy: 0.0132\n","\n","Epoch 00003: val_loss improved from 22.31434 to 18.99160, saving model to densenet201_720_3.h5\n","Epoch 4/60\n","\n","Epoch 00004: LearningRateScheduler reducing learning rate to 0.0006020000000000001.\n","107/107 [==============================] - 119s 1s/step - loss: 15.9612 - sparse_categorical_accuracy: 0.0354 - val_loss: 16.2651 - val_sparse_categorical_accuracy: 0.0350\n","\n","Epoch 00004: val_loss improved from 18.99160 to 16.26506, saving model to densenet201_720_3.h5\n","Epoch 5/60\n","\n","Epoch 00005: LearningRateScheduler reducing learning rate to 0.0008010000000000001.\n","107/107 [==============================] - 118s 1s/step - loss: 11.8427 - sparse_categorical_accuracy: 0.0812 - val_loss: 14.4989 - val_sparse_categorical_accuracy: 0.0485\n","\n","Epoch 00005: val_loss improved from 16.26506 to 14.49886, saving model to densenet201_720_3.h5\n","Epoch 6/60\n","\n","Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n","107/107 [==============================] - 117s 1s/step - loss: 8.7758 - sparse_categorical_accuracy: 0.1480 - val_loss: 13.5620 - val_sparse_categorical_accuracy: 0.0572\n","\n","Epoch 00006: val_loss improved from 14.49886 to 13.56197, saving model to densenet201_720_3.h5\n","Epoch 7/60\n","\n","Epoch 00007: LearningRateScheduler reducing learning rate to 0.0008010000000000001.\n","107/107 [==============================] - 116s 1s/step - loss: 6.2534 - sparse_categorical_accuracy: 0.2614 - val_loss: 11.8309 - val_sparse_categorical_accuracy: 0.1250\n","\n","Epoch 00007: val_loss improved from 13.56197 to 11.83085, saving model to densenet201_720_3.h5\n","Epoch 8/60\n","\n","Epoch 00008: LearningRateScheduler reducing learning rate to 0.0006418000000000001.\n","107/107 [==============================] - 118s 1s/step - loss: 4.3449 - sparse_categorical_accuracy: 0.4191 - val_loss: 10.5665 - val_sparse_categorical_accuracy: 0.2188\n","\n","Epoch 00008: val_loss improved from 11.83085 to 10.56648, saving model to densenet201_720_3.h5\n","Epoch 9/60\n","\n","Epoch 00009: LearningRateScheduler reducing learning rate to 0.0005144400000000001.\n","107/107 [==============================] - 116s 1s/step - loss: 3.1411 - sparse_categorical_accuracy: 0.5627 - val_loss: 9.9846 - val_sparse_categorical_accuracy: 0.2686\n","\n","Epoch 00009: val_loss improved from 10.56648 to 9.98457, saving model to densenet201_720_3.h5\n","Epoch 10/60\n","\n","Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004125520000000001.\n","107/107 [==============================] - 117s 1s/step - loss: 2.3939 - sparse_categorical_accuracy: 0.6618 - val_loss: 9.5249 - val_sparse_categorical_accuracy: 0.3116\n","\n","Epoch 00010: val_loss improved from 9.98457 to 9.52494, saving model to densenet201_720_3.h5\n","Epoch 11/60\n","\n","Epoch 00011: LearningRateScheduler reducing learning rate to 0.0003310416000000001.\n","107/107 [==============================] - 117s 1s/step - loss: 1.8829 - sparse_categorical_accuracy: 0.7350 - val_loss: 9.2677 - val_sparse_categorical_accuracy: 0.3415\n","\n","Epoch 00011: val_loss improved from 9.52494 to 9.26768, saving model to densenet201_720_3.h5\n","Epoch 12/60\n","\n","Epoch 00012: LearningRateScheduler reducing learning rate to 0.0002658332800000001.\n","107/107 [==============================] - 118s 1s/step - loss: 1.5255 - sparse_categorical_accuracy: 0.7866 - val_loss: 9.1606 - val_sparse_categorical_accuracy: 0.3550\n","\n","Epoch 00012: val_loss improved from 9.26768 to 9.16055, saving model to densenet201_720_3.h5\n","Epoch 13/60\n","\n","Epoch 00013: LearningRateScheduler reducing learning rate to 0.00021366662400000008.\n","107/107 [==============================] - 116s 1s/step - loss: 1.3019 - sparse_categorical_accuracy: 0.8220 - val_loss: 8.9674 - val_sparse_categorical_accuracy: 0.3768\n","\n","Epoch 00013: val_loss improved from 9.16055 to 8.96738, saving model to densenet201_720_3.h5\n","Epoch 14/60\n","\n","Epoch 00014: LearningRateScheduler reducing learning rate to 0.0001719332992000001.\n","107/107 [==============================] - 122s 1s/step - loss: 1.1202 - sparse_categorical_accuracy: 0.8534 - val_loss: 8.8655 - val_sparse_categorical_accuracy: 0.3885\n","\n","Epoch 00014: val_loss improved from 8.96738 to 8.86551, saving model to densenet201_720_3.h5\n","Epoch 15/60\n","\n","Epoch 00015: LearningRateScheduler reducing learning rate to 0.00013854663936000008.\n","107/107 [==============================] - 122s 1s/step - loss: 0.9887 - sparse_categorical_accuracy: 0.8763 - val_loss: 8.6772 - val_sparse_categorical_accuracy: 0.4111\n","\n","Epoch 00015: val_loss improved from 8.86551 to 8.67717, saving model to densenet201_720_3.h5\n","Epoch 16/60\n","\n","Epoch 00016: LearningRateScheduler reducing learning rate to 0.00011183731148800006.\n","107/107 [==============================] - 121s 1s/step - loss: 0.8965 - sparse_categorical_accuracy: 0.8906 - val_loss: 8.5887 - val_sparse_categorical_accuracy: 0.4201\n","\n","Epoch 00016: val_loss improved from 8.67717 to 8.58874, saving model to densenet201_720_3.h5\n","Epoch 17/60\n","\n","Epoch 00017: LearningRateScheduler reducing learning rate to 9.046984919040005e-05.\n","107/107 [==============================] - 119s 1s/step - loss: 0.8174 - sparse_categorical_accuracy: 0.9039 - val_loss: 8.5242 - val_sparse_categorical_accuracy: 0.4313\n","\n","Epoch 00017: val_loss improved from 8.58874 to 8.52416, saving model to densenet201_720_3.h5\n","Epoch 18/60\n","\n","Epoch 00018: LearningRateScheduler reducing learning rate to 7.337587935232004e-05.\n","107/107 [==============================] - 117s 1s/step - loss: 0.7644 - sparse_categorical_accuracy: 0.9125 - val_loss: 8.4849 - val_sparse_categorical_accuracy: 0.4361\n","\n","Epoch 00018: val_loss improved from 8.52416 to 8.48493, saving model to densenet201_720_3.h5\n","Epoch 19/60\n","\n","Epoch 00019: LearningRateScheduler reducing learning rate to 5.9700703481856035e-05.\n","107/107 [==============================] - 121s 1s/step - loss: 0.7217 - sparse_categorical_accuracy: 0.9183 - val_loss: 8.4301 - val_sparse_categorical_accuracy: 0.4411\n","\n","Epoch 00019: val_loss improved from 8.48493 to 8.43008, saving model to densenet201_720_3.h5\n","Epoch 20/60\n","\n","Epoch 00020: LearningRateScheduler reducing learning rate to 4.8760562785484834e-05.\n","107/107 [==============================] - 117s 1s/step - loss: 0.7059 - sparse_categorical_accuracy: 0.9202 - val_loss: 8.3968 - val_sparse_categorical_accuracy: 0.4462\n","\n","Epoch 00020: val_loss improved from 8.43008 to 8.39676, saving model to densenet201_720_3.h5\n","Epoch 21/60\n","\n","Epoch 00021: LearningRateScheduler reducing learning rate to 4.000845022838787e-05.\n","107/107 [==============================] - 116s 1s/step - loss: 0.6486 - sparse_categorical_accuracy: 0.9305 - val_loss: 8.3201 - val_sparse_categorical_accuracy: 0.4540\n","\n","Epoch 00021: val_loss improved from 8.39676 to 8.32010, saving model to densenet201_720_3.h5\n","Epoch 22/60\n","\n","Epoch 00022: LearningRateScheduler reducing learning rate to 3.30067601827103e-05.\n","107/107 [==============================] - 120s 1s/step - loss: 0.6354 - sparse_categorical_accuracy: 0.9330 - val_loss: 8.2663 - val_sparse_categorical_accuracy: 0.4587\n","\n","Epoch 00022: val_loss improved from 8.32010 to 8.26632, saving model to densenet201_720_3.h5\n","Epoch 23/60\n","\n","Epoch 00023: LearningRateScheduler reducing learning rate to 2.740540814616824e-05.\n","107/107 [==============================] - 116s 1s/step - loss: 0.6144 - sparse_categorical_accuracy: 0.9352 - val_loss: 8.2237 - val_sparse_categorical_accuracy: 0.4626\n","\n","Epoch 00023: val_loss improved from 8.26632 to 8.22365, saving model to densenet201_720_3.h5\n","Epoch 24/60\n","\n","Epoch 00024: LearningRateScheduler reducing learning rate to 2.2924326516934593e-05.\n","107/107 [==============================] - 117s 1s/step - loss: 0.6140 - sparse_categorical_accuracy: 0.9319 - val_loss: 8.1760 - val_sparse_categorical_accuracy: 0.4657\n","\n","Epoch 00024: val_loss improved from 8.22365 to 8.17596, saving model to densenet201_720_3.h5\n","Epoch 25/60\n","\n","Epoch 00025: LearningRateScheduler reducing learning rate to 1.9339461213547675e-05.\n","107/107 [==============================] - 117s 1s/step - loss: 0.5907 - sparse_categorical_accuracy: 0.9374 - val_loss: 8.1561 - val_sparse_categorical_accuracy: 0.4683\n","\n","Epoch 00025: val_loss improved from 8.17596 to 8.15615, saving model to densenet201_720_3.h5\n","Epoch 26/60\n","\n","Epoch 00026: LearningRateScheduler reducing learning rate to 1.647156897083814e-05.\n","107/107 [==============================] - 118s 1s/step - loss: 0.5741 - sparse_categorical_accuracy: 0.9382 - val_loss: 8.1076 - val_sparse_categorical_accuracy: 0.4724\n","\n","Epoch 00026: val_loss improved from 8.15615 to 8.10763, saving model to densenet201_720_3.h5\n","Epoch 27/60\n","\n","Epoch 00027: LearningRateScheduler reducing learning rate to 1.4177255176670514e-05.\n","107/107 [==============================] - 116s 1s/step - loss: 0.5660 - sparse_categorical_accuracy: 0.9403 - val_loss: 8.0780 - val_sparse_categorical_accuracy: 0.4740\n","\n","Epoch 00027: val_loss improved from 8.10763 to 8.07798, saving model to densenet201_720_3.h5\n","Epoch 28/60\n","\n","Epoch 00028: LearningRateScheduler reducing learning rate to 1.234180414133641e-05.\n","107/107 [==============================] - 115s 1s/step - loss: 0.5608 - sparse_categorical_accuracy: 0.9407 - val_loss: 8.0584 - val_sparse_categorical_accuracy: 0.4758\n","\n","Epoch 00028: val_loss improved from 8.07798 to 8.05841, saving model to densenet201_720_3.h5\n","Epoch 29/60\n","\n","Epoch 00029: LearningRateScheduler reducing learning rate to 1.087344331306913e-05.\n","107/107 [==============================] - 116s 1s/step - loss: 0.5405 - sparse_categorical_accuracy: 0.9413 - val_loss: 8.0398 - val_sparse_categorical_accuracy: 0.4781\n","\n","Epoch 00029: val_loss improved from 8.05841 to 8.03983, saving model to densenet201_720_3.h5\n","Epoch 30/60\n","\n","Epoch 00030: LearningRateScheduler reducing learning rate to 9.698754650455303e-06.\n","107/107 [==============================] - 116s 1s/step - loss: 0.5416 - sparse_categorical_accuracy: 0.9438 - val_loss: 8.0283 - val_sparse_categorical_accuracy: 0.4799\n","\n","Epoch 00030: val_loss improved from 8.03983 to 8.02830, saving model to densenet201_720_3.h5\n","Epoch 31/60\n","\n","Epoch 00031: LearningRateScheduler reducing learning rate to 8.759003720364244e-06.\n","107/107 [==============================] - 118s 1s/step - loss: 0.5267 - sparse_categorical_accuracy: 0.9450 - val_loss: 8.0175 - val_sparse_categorical_accuracy: 0.4800\n","\n","Epoch 00031: val_loss improved from 8.02830 to 8.01749, saving model to densenet201_720_3.h5\n","Epoch 32/60\n","\n","Epoch 00032: LearningRateScheduler reducing learning rate to 8.007202976291395e-06.\n","107/107 [==============================] - 120s 1s/step - loss: 0.5226 - sparse_categorical_accuracy: 0.9451 - val_loss: 8.0068 - val_sparse_categorical_accuracy: 0.4818\n","\n","Epoch 00032: val_loss improved from 8.01749 to 8.00683, saving model to densenet201_720_3.h5\n","Epoch 33/60\n","\n","Epoch 00033: LearningRateScheduler reducing learning rate to 7.4057623810331166e-06.\n","107/107 [==============================] - 120s 1s/step - loss: 0.5199 - sparse_categorical_accuracy: 0.9466 - val_loss: 8.0025 - val_sparse_categorical_accuracy: 0.4815\n","\n","Epoch 00033: val_loss improved from 8.00683 to 8.00248, saving model to densenet201_720_3.h5\n","Epoch 34/60\n","\n","Epoch 00034: LearningRateScheduler reducing learning rate to 6.9246099048264925e-06.\n","107/107 [==============================] - 124s 1s/step - loss: 0.5049 - sparse_categorical_accuracy: 0.9496 - val_loss: 8.0007 - val_sparse_categorical_accuracy: 0.4820\n","\n","Epoch 00034: val_loss improved from 8.00248 to 8.00066, saving model to densenet201_720_3.h5\n","Epoch 35/60\n","\n","Epoch 00035: LearningRateScheduler reducing learning rate to 6.539687923861194e-06.\n","107/107 [==============================] - 120s 1s/step - loss: 0.4944 - sparse_categorical_accuracy: 0.9494 - val_loss: 7.9941 - val_sparse_categorical_accuracy: 0.4824\n","\n","Epoch 00035: val_loss improved from 8.00066 to 7.99411, saving model to densenet201_720_3.h5\n","Epoch 36/60\n","\n","Epoch 00036: LearningRateScheduler reducing learning rate to 6.231750339088956e-06.\n","107/107 [==============================] - 118s 1s/step - loss: 0.4713 - sparse_categorical_accuracy: 0.9538 - val_loss: 7.9896 - val_sparse_categorical_accuracy: 0.4833\n","\n","Epoch 00036: val_loss improved from 7.99411 to 7.98961, saving model to densenet201_720_3.h5\n","Epoch 37/60\n","\n","Epoch 00037: LearningRateScheduler reducing learning rate to 5.985400271271165e-06.\n","107/107 [==============================] - 118s 1s/step - loss: 0.4874 - sparse_categorical_accuracy: 0.9521 - val_loss: 7.9874 - val_sparse_categorical_accuracy: 0.4829\n","\n","Epoch 00037: val_loss improved from 7.98961 to 7.98739, saving model to densenet201_720_3.h5\n","Epoch 38/60\n","\n","Epoch 00038: LearningRateScheduler reducing learning rate to 5.788320217016932e-06.\n","107/107 [==============================] - 119s 1s/step - loss: 0.4852 - sparse_categorical_accuracy: 0.9502 - val_loss: 7.9865 - val_sparse_categorical_accuracy: 0.4841\n","\n","Epoch 00038: val_loss improved from 7.98739 to 7.98648, saving model to densenet201_720_3.h5\n","Epoch 39/60\n","\n","Epoch 00039: LearningRateScheduler reducing learning rate to 5.630656173613546e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.4608 - sparse_categorical_accuracy: 0.9544 - val_loss: 7.9857 - val_sparse_categorical_accuracy: 0.4836\n","\n","Epoch 00039: val_loss improved from 7.98648 to 7.98575, saving model to densenet201_720_3.h5\n","Epoch 40/60\n","\n","Epoch 00040: LearningRateScheduler reducing learning rate to 5.5045249388908364e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.4824 - sparse_categorical_accuracy: 0.9549 - val_loss: 7.9830 - val_sparse_categorical_accuracy: 0.4836\n","\n","Epoch 00040: val_loss improved from 7.98575 to 7.98298, saving model to densenet201_720_3.h5\n","Epoch 41/60\n","\n","Epoch 00041: LearningRateScheduler reducing learning rate to 5.403619951112669e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.4704 - sparse_categorical_accuracy: 0.9517 - val_loss: 7.9809 - val_sparse_categorical_accuracy: 0.4850\n","\n","Epoch 00041: val_loss improved from 7.98298 to 7.98090, saving model to densenet201_720_3.h5\n","Epoch 42/60\n","\n","Epoch 00042: LearningRateScheduler reducing learning rate to 5.322895960890136e-06.\n","107/107 [==============================] - 118s 1s/step - loss: 0.4604 - sparse_categorical_accuracy: 0.9538 - val_loss: 7.9795 - val_sparse_categorical_accuracy: 0.4845\n","\n","Epoch 00042: val_loss improved from 7.98090 to 7.97946, saving model to densenet201_720_3.h5\n","Epoch 43/60\n","\n","Epoch 00043: LearningRateScheduler reducing learning rate to 5.258316768712109e-06.\n","107/107 [==============================] - 118s 1s/step - loss: 0.4790 - sparse_categorical_accuracy: 0.9528 - val_loss: 7.9788 - val_sparse_categorical_accuracy: 0.4847\n","\n","Epoch 00043: val_loss improved from 7.97946 to 7.97884, saving model to densenet201_720_3.h5\n","Epoch 44/60\n","\n","Epoch 00044: LearningRateScheduler reducing learning rate to 5.206653414969687e-06.\n","107/107 [==============================] - 116s 1s/step - loss: 0.4549 - sparse_categorical_accuracy: 0.9565 - val_loss: 7.9757 - val_sparse_categorical_accuracy: 0.4845\n","\n","Epoch 00044: val_loss improved from 7.97884 to 7.97573, saving model to densenet201_720_3.h5\n","Epoch 45/60\n","\n","Epoch 00045: LearningRateScheduler reducing learning rate to 5.16532273197575e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.4629 - sparse_categorical_accuracy: 0.9547 - val_loss: 7.9771 - val_sparse_categorical_accuracy: 0.4857\n","\n","Epoch 00045: val_loss did not improve from 7.97573\n","Epoch 46/60\n","\n","Epoch 00046: LearningRateScheduler reducing learning rate to 5.1322581855806e-06.\n","107/107 [==============================] - 115s 1s/step - loss: 0.4643 - sparse_categorical_accuracy: 0.9551 - val_loss: 7.9748 - val_sparse_categorical_accuracy: 0.4850\n","\n","Epoch 00046: val_loss improved from 7.97573 to 7.97478, saving model to densenet201_720_3.h5\n","Epoch 47/60\n","\n","Epoch 00047: LearningRateScheduler reducing learning rate to 5.10580654846448e-06.\n","107/107 [==============================] - 119s 1s/step - loss: 0.4462 - sparse_categorical_accuracy: 0.9563 - val_loss: 7.9754 - val_sparse_categorical_accuracy: 0.4847\n","\n","Epoch 00047: val_loss did not improve from 7.97478\n","Epoch 48/60\n","\n","Epoch 00048: LearningRateScheduler reducing learning rate to 5.084645238771584e-06.\n","107/107 [==============================] - 115s 1s/step - loss: 0.4495 - sparse_categorical_accuracy: 0.9558 - val_loss: 7.9730 - val_sparse_categorical_accuracy: 0.4853\n","\n","Epoch 00048: val_loss improved from 7.97478 to 7.97302, saving model to densenet201_720_3.h5\n","Epoch 49/60\n","\n","Epoch 00049: LearningRateScheduler reducing learning rate to 5.067716191017267e-06.\n","107/107 [==============================] - 115s 1s/step - loss: 0.4401 - sparse_categorical_accuracy: 0.9589 - val_loss: 7.9717 - val_sparse_categorical_accuracy: 0.4865\n","\n","Epoch 00049: val_loss improved from 7.97302 to 7.97174, saving model to densenet201_720_3.h5\n","Epoch 50/60\n","\n","Epoch 00050: LearningRateScheduler reducing learning rate to 5.054172952813814e-06.\n","107/107 [==============================] - 116s 1s/step - loss: 0.4399 - sparse_categorical_accuracy: 0.9569 - val_loss: 7.9720 - val_sparse_categorical_accuracy: 0.4866\n","\n","Epoch 00050: val_loss did not improve from 7.97174\n","Epoch 51/60\n","\n","Epoch 00051: LearningRateScheduler reducing learning rate to 5.043338362251051e-06.\n","107/107 [==============================] - 116s 1s/step - loss: 0.4355 - sparse_categorical_accuracy: 0.9581 - val_loss: 7.9716 - val_sparse_categorical_accuracy: 0.4857\n","\n","Epoch 00051: val_loss improved from 7.97174 to 7.97155, saving model to densenet201_720_3.h5\n","Epoch 52/60\n","\n","Epoch 00052: LearningRateScheduler reducing learning rate to 5.034670689800841e-06.\n","107/107 [==============================] - 116s 1s/step - loss: 0.4353 - sparse_categorical_accuracy: 0.9591 - val_loss: 7.9681 - val_sparse_categorical_accuracy: 0.4866\n","\n","Epoch 00052: val_loss improved from 7.97155 to 7.96813, saving model to densenet201_720_3.h5\n","Epoch 53/60\n","\n","Epoch 00053: LearningRateScheduler reducing learning rate to 5.027736551840673e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.4255 - sparse_categorical_accuracy: 0.9602 - val_loss: 7.9680 - val_sparse_categorical_accuracy: 0.4859\n","\n","Epoch 00053: val_loss improved from 7.96813 to 7.96802, saving model to densenet201_720_3.h5\n","Epoch 54/60\n","\n","Epoch 00054: LearningRateScheduler reducing learning rate to 5.022189241472539e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.4261 - sparse_categorical_accuracy: 0.9600 - val_loss: 7.9679 - val_sparse_categorical_accuracy: 0.4871\n","\n","Epoch 00054: val_loss improved from 7.96802 to 7.96795, saving model to densenet201_720_3.h5\n","Epoch 55/60\n","\n","Epoch 00055: LearningRateScheduler reducing learning rate to 5.017751393178031e-06.\n","107/107 [==============================] - 115s 1s/step - loss: 0.4209 - sparse_categorical_accuracy: 0.9614 - val_loss: 7.9673 - val_sparse_categorical_accuracy: 0.4862\n","\n","Epoch 00055: val_loss improved from 7.96795 to 7.96735, saving model to densenet201_720_3.h5\n","Epoch 56/60\n","\n","Epoch 00056: LearningRateScheduler reducing learning rate to 5.0142011145424245e-06.\n","107/107 [==============================] - 115s 1s/step - loss: 0.4129 - sparse_categorical_accuracy: 0.9625 - val_loss: 7.9669 - val_sparse_categorical_accuracy: 0.4868\n","\n","Epoch 00056: val_loss improved from 7.96735 to 7.96694, saving model to densenet201_720_3.h5\n","Epoch 57/60\n","\n","Epoch 00057: LearningRateScheduler reducing learning rate to 5.0113608916339395e-06.\n","107/107 [==============================] - 114s 1s/step - loss: 0.4121 - sparse_categorical_accuracy: 0.9620 - val_loss: 7.9668 - val_sparse_categorical_accuracy: 0.4869\n","\n","Epoch 00057: val_loss improved from 7.96694 to 7.96675, saving model to densenet201_720_3.h5\n","Epoch 58/60\n","\n","Epoch 00058: LearningRateScheduler reducing learning rate to 5.009088713307152e-06.\n","107/107 [==============================] - 115s 1s/step - loss: 0.4259 - sparse_categorical_accuracy: 0.9596 - val_loss: 7.9665 - val_sparse_categorical_accuracy: 0.4866\n","\n","Epoch 00058: val_loss improved from 7.96675 to 7.96651, saving model to densenet201_720_3.h5\n","Epoch 59/60\n","\n","Epoch 00059: LearningRateScheduler reducing learning rate to 5.007270970645722e-06.\n","107/107 [==============================] - 115s 1s/step - loss: 0.4191 - sparse_categorical_accuracy: 0.9622 - val_loss: 7.9643 - val_sparse_categorical_accuracy: 0.4871\n","\n","Epoch 00059: val_loss improved from 7.96651 to 7.96431, saving model to densenet201_720_3.h5\n","Epoch 60/60\n","\n","Epoch 00060: LearningRateScheduler reducing learning rate to 5.005816776516578e-06.\n","107/107 [==============================] - 114s 1s/step - loss: 0.4131 - sparse_categorical_accuracy: 0.9611 - val_loss: 7.9653 - val_sparse_categorical_accuracy: 0.4868\n","\n","Epoch 00060: val_loss did not improve from 7.96431\n","model training for 3 is done\n","training for fold 4\n","Epoch 1/60\n","\n","Epoch 00001: LearningRateScheduler reducing learning rate to 5e-06.\n","107/107 [==============================] - 382s 1s/step - loss: 24.2635 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 24.0834 - val_sparse_categorical_accuracy: 0.0000e+00\n","\n","Epoch 00001: val_loss improved from inf to 24.08340, saving model to densenet201_720_4.h5\n","Epoch 2/60\n","\n","Epoch 00002: LearningRateScheduler reducing learning rate to 0.00020400000000000003.\n","107/107 [==============================] - 118s 1s/step - loss: 23.6031 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 22.5679 - val_sparse_categorical_accuracy: 0.0000e+00\n","\n","Epoch 00002: val_loss improved from 24.08340 to 22.56794, saving model to densenet201_720_4.h5\n","Epoch 3/60\n","\n","Epoch 00003: LearningRateScheduler reducing learning rate to 0.00040300000000000004.\n","107/107 [==============================] - 117s 1s/step - loss: 20.8030 - sparse_categorical_accuracy: 0.0044 - val_loss: 19.3054 - val_sparse_categorical_accuracy: 0.0072\n","\n","Epoch 00003: val_loss improved from 22.56794 to 19.30542, saving model to densenet201_720_4.h5\n","Epoch 4/60\n","\n","Epoch 00004: LearningRateScheduler reducing learning rate to 0.0006020000000000001.\n","107/107 [==============================] - 118s 1s/step - loss: 15.9721 - sparse_categorical_accuracy: 0.0324 - val_loss: 17.0361 - val_sparse_categorical_accuracy: 0.0141\n","\n","Epoch 00004: val_loss improved from 19.30542 to 17.03608, saving model to densenet201_720_4.h5\n","Epoch 5/60\n","\n","Epoch 00005: LearningRateScheduler reducing learning rate to 0.0008010000000000001.\n","107/107 [==============================] - 117s 1s/step - loss: 11.7232 - sparse_categorical_accuracy: 0.0802 - val_loss: 15.1356 - val_sparse_categorical_accuracy: 0.0255\n","\n","Epoch 00005: val_loss improved from 17.03608 to 15.13563, saving model to densenet201_720_4.h5\n","Epoch 6/60\n","\n","Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n","107/107 [==============================] - 119s 1s/step - loss: 8.7241 - sparse_categorical_accuracy: 0.1443 - val_loss: 14.1811 - val_sparse_categorical_accuracy: 0.0335\n","\n","Epoch 00006: val_loss improved from 15.13563 to 14.18113, saving model to densenet201_720_4.h5\n","Epoch 7/60\n","\n","Epoch 00007: LearningRateScheduler reducing learning rate to 0.0008010000000000001.\n","107/107 [==============================] - 118s 1s/step - loss: 6.2003 - sparse_categorical_accuracy: 0.2587 - val_loss: 12.3226 - val_sparse_categorical_accuracy: 0.0951\n","\n","Epoch 00007: val_loss improved from 14.18113 to 12.32259, saving model to densenet201_720_4.h5\n","Epoch 8/60\n","\n","Epoch 00008: LearningRateScheduler reducing learning rate to 0.0006418000000000001.\n","107/107 [==============================] - 118s 1s/step - loss: 4.3307 - sparse_categorical_accuracy: 0.4179 - val_loss: 11.3224 - val_sparse_categorical_accuracy: 0.1549\n","\n","Epoch 00008: val_loss improved from 12.32259 to 11.32241, saving model to densenet201_720_4.h5\n","Epoch 9/60\n","\n","Epoch 00009: LearningRateScheduler reducing learning rate to 0.0005144400000000001.\n","107/107 [==============================] - 118s 1s/step - loss: 3.1563 - sparse_categorical_accuracy: 0.5567 - val_loss: 10.3141 - val_sparse_categorical_accuracy: 0.2411\n","\n","Epoch 00009: val_loss improved from 11.32241 to 10.31414, saving model to densenet201_720_4.h5\n","Epoch 10/60\n","\n","Epoch 00010: LearningRateScheduler reducing learning rate to 0.0004125520000000001.\n","107/107 [==============================] - 118s 1s/step - loss: 2.3796 - sparse_categorical_accuracy: 0.6620 - val_loss: 9.9968 - val_sparse_categorical_accuracy: 0.2736\n","\n","Epoch 00010: val_loss improved from 10.31414 to 9.99683, saving model to densenet201_720_4.h5\n","Epoch 11/60\n","\n","Epoch 00011: LearningRateScheduler reducing learning rate to 0.0003310416000000001.\n","107/107 [==============================] - 116s 1s/step - loss: 1.8945 - sparse_categorical_accuracy: 0.7340 - val_loss: 9.7965 - val_sparse_categorical_accuracy: 0.2912\n","\n","Epoch 00011: val_loss improved from 9.99683 to 9.79650, saving model to densenet201_720_4.h5\n","Epoch 12/60\n","\n","Epoch 00012: LearningRateScheduler reducing learning rate to 0.0002658332800000001.\n","107/107 [==============================] - 121s 1s/step - loss: 1.5790 - sparse_categorical_accuracy: 0.7785 - val_loss: 9.7516 - val_sparse_categorical_accuracy: 0.3012\n","\n","Epoch 00012: val_loss improved from 9.79650 to 9.75159, saving model to densenet201_720_4.h5\n","Epoch 13/60\n","\n","Epoch 00013: LearningRateScheduler reducing learning rate to 0.00021366662400000008.\n","107/107 [==============================] - 120s 1s/step - loss: 1.2980 - sparse_categorical_accuracy: 0.8181 - val_loss: 9.4333 - val_sparse_categorical_accuracy: 0.3346\n","\n","Epoch 00013: val_loss improved from 9.75159 to 9.43329, saving model to densenet201_720_4.h5\n","Epoch 14/60\n","\n","Epoch 00014: LearningRateScheduler reducing learning rate to 0.0001719332992000001.\n","107/107 [==============================] - 119s 1s/step - loss: 1.1908 - sparse_categorical_accuracy: 0.8397 - val_loss: 9.2525 - val_sparse_categorical_accuracy: 0.3544\n","\n","Epoch 00014: val_loss improved from 9.43329 to 9.25252, saving model to densenet201_720_4.h5\n","Epoch 15/60\n","\n","Epoch 00015: LearningRateScheduler reducing learning rate to 0.00013854663936000008.\n","107/107 [==============================] - 119s 1s/step - loss: 1.0058 - sparse_categorical_accuracy: 0.8698 - val_loss: 9.0843 - val_sparse_categorical_accuracy: 0.3747\n","\n","Epoch 00015: val_loss improved from 9.25252 to 9.08427, saving model to densenet201_720_4.h5\n","Epoch 16/60\n","\n","Epoch 00016: LearningRateScheduler reducing learning rate to 0.00011183731148800006.\n","107/107 [==============================] - 122s 1s/step - loss: 0.9133 - sparse_categorical_accuracy: 0.8825 - val_loss: 9.0165 - val_sparse_categorical_accuracy: 0.3834\n","\n","Epoch 00016: val_loss improved from 9.08427 to 9.01652, saving model to densenet201_720_4.h5\n","Epoch 17/60\n","\n","Epoch 00017: LearningRateScheduler reducing learning rate to 9.046984919040005e-05.\n","107/107 [==============================] - 117s 1s/step - loss: 0.8484 - sparse_categorical_accuracy: 0.8958 - val_loss: 8.9357 - val_sparse_categorical_accuracy: 0.3932\n","\n","Epoch 00017: val_loss improved from 9.01652 to 8.93567, saving model to densenet201_720_4.h5\n","Epoch 18/60\n","\n","Epoch 00018: LearningRateScheduler reducing learning rate to 7.337587935232004e-05.\n","107/107 [==============================] - 117s 1s/step - loss: 0.7888 - sparse_categorical_accuracy: 0.9021 - val_loss: 8.8934 - val_sparse_categorical_accuracy: 0.4013\n","\n","Epoch 00018: val_loss improved from 8.93567 to 8.89338, saving model to densenet201_720_4.h5\n","Epoch 19/60\n","\n","Epoch 00019: LearningRateScheduler reducing learning rate to 5.9700703481856035e-05.\n","107/107 [==============================] - 120s 1s/step - loss: 0.7479 - sparse_categorical_accuracy: 0.9129 - val_loss: 8.8446 - val_sparse_categorical_accuracy: 0.4072\n","\n","Epoch 00019: val_loss improved from 8.89338 to 8.84462, saving model to densenet201_720_4.h5\n","Epoch 20/60\n","\n","Epoch 00020: LearningRateScheduler reducing learning rate to 4.8760562785484834e-05.\n","107/107 [==============================] - 119s 1s/step - loss: 0.6996 - sparse_categorical_accuracy: 0.9172 - val_loss: 8.8063 - val_sparse_categorical_accuracy: 0.4099\n","\n","Epoch 00020: val_loss improved from 8.84462 to 8.80626, saving model to densenet201_720_4.h5\n","Epoch 21/60\n","\n","Epoch 00021: LearningRateScheduler reducing learning rate to 4.000845022838787e-05.\n","107/107 [==============================] - 117s 1s/step - loss: 0.6925 - sparse_categorical_accuracy: 0.9209 - val_loss: 8.7442 - val_sparse_categorical_accuracy: 0.4192\n","\n","Epoch 00021: val_loss improved from 8.80626 to 8.74419, saving model to densenet201_720_4.h5\n","Epoch 22/60\n","\n","Epoch 00022: LearningRateScheduler reducing learning rate to 3.30067601827103e-05.\n","107/107 [==============================] - 117s 1s/step - loss: 0.6556 - sparse_categorical_accuracy: 0.9241 - val_loss: 8.7050 - val_sparse_categorical_accuracy: 0.4228\n","\n","Epoch 00022: val_loss improved from 8.74419 to 8.70504, saving model to densenet201_720_4.h5\n","Epoch 23/60\n","\n","Epoch 00023: LearningRateScheduler reducing learning rate to 2.740540814616824e-05.\n","107/107 [==============================] - 117s 1s/step - loss: 0.6258 - sparse_categorical_accuracy: 0.9288 - val_loss: 8.6078 - val_sparse_categorical_accuracy: 0.4310\n","\n","Epoch 00023: val_loss improved from 8.70504 to 8.60785, saving model to densenet201_720_4.h5\n","Epoch 24/60\n","\n","Epoch 00024: LearningRateScheduler reducing learning rate to 2.2924326516934593e-05.\n","107/107 [==============================] - 117s 1s/step - loss: 0.6286 - sparse_categorical_accuracy: 0.9292 - val_loss: 8.5874 - val_sparse_categorical_accuracy: 0.4349\n","\n","Epoch 00024: val_loss improved from 8.60785 to 8.58736, saving model to densenet201_720_4.h5\n","Epoch 25/60\n","\n","Epoch 00025: LearningRateScheduler reducing learning rate to 1.9339461213547675e-05.\n","107/107 [==============================] - 116s 1s/step - loss: 0.6074 - sparse_categorical_accuracy: 0.9321 - val_loss: 8.5134 - val_sparse_categorical_accuracy: 0.4410\n","\n","Epoch 00025: val_loss improved from 8.58736 to 8.51338, saving model to densenet201_720_4.h5\n","Epoch 26/60\n","\n","Epoch 00026: LearningRateScheduler reducing learning rate to 1.647156897083814e-05.\n","107/107 [==============================] - 115s 1s/step - loss: 0.5846 - sparse_categorical_accuracy: 0.9350 - val_loss: 8.4656 - val_sparse_categorical_accuracy: 0.4446\n","\n","Epoch 00026: val_loss improved from 8.51338 to 8.46560, saving model to densenet201_720_4.h5\n","Epoch 27/60\n","\n","Epoch 00027: LearningRateScheduler reducing learning rate to 1.4177255176670514e-05.\n","107/107 [==============================] - 116s 1s/step - loss: 0.5630 - sparse_categorical_accuracy: 0.9376 - val_loss: 8.4396 - val_sparse_categorical_accuracy: 0.4476\n","\n","Epoch 00027: val_loss improved from 8.46560 to 8.43955, saving model to densenet201_720_4.h5\n","Epoch 28/60\n","\n","Epoch 00028: LearningRateScheduler reducing learning rate to 1.234180414133641e-05.\n","107/107 [==============================] - 115s 1s/step - loss: 0.5533 - sparse_categorical_accuracy: 0.9380 - val_loss: 8.4349 - val_sparse_categorical_accuracy: 0.4488\n","\n","Epoch 00028: val_loss improved from 8.43955 to 8.43490, saving model to densenet201_720_4.h5\n","Epoch 29/60\n","\n","Epoch 00029: LearningRateScheduler reducing learning rate to 1.087344331306913e-05.\n","107/107 [==============================] - 119s 1s/step - loss: 0.5462 - sparse_categorical_accuracy: 0.9410 - val_loss: 8.3947 - val_sparse_categorical_accuracy: 0.4515\n","\n","Epoch 00029: val_loss improved from 8.43490 to 8.39468, saving model to densenet201_720_4.h5\n","Epoch 30/60\n","\n","Epoch 00030: LearningRateScheduler reducing learning rate to 9.698754650455303e-06.\n","107/107 [==============================] - 121s 1s/step - loss: 0.5434 - sparse_categorical_accuracy: 0.9413 - val_loss: 8.3789 - val_sparse_categorical_accuracy: 0.4537\n","\n","Epoch 00030: val_loss improved from 8.39468 to 8.37893, saving model to densenet201_720_4.h5\n","Epoch 31/60\n","\n","Epoch 00031: LearningRateScheduler reducing learning rate to 8.759003720364244e-06.\n","107/107 [==============================] - 120s 1s/step - loss: 0.5479 - sparse_categorical_accuracy: 0.9402 - val_loss: 8.3826 - val_sparse_categorical_accuracy: 0.4534\n","\n","Epoch 00031: val_loss did not improve from 8.37893\n","Epoch 32/60\n","\n","Epoch 00032: LearningRateScheduler reducing learning rate to 8.007202976291395e-06.\n","107/107 [==============================] - 119s 1s/step - loss: 0.5349 - sparse_categorical_accuracy: 0.9429 - val_loss: 8.3875 - val_sparse_categorical_accuracy: 0.4518\n","\n","Epoch 00032: val_loss did not improve from 8.37893\n","Epoch 33/60\n","\n","Epoch 00033: LearningRateScheduler reducing learning rate to 7.4057623810331166e-06.\n","107/107 [==============================] - 120s 1s/step - loss: 0.5450 - sparse_categorical_accuracy: 0.9415 - val_loss: 8.3725 - val_sparse_categorical_accuracy: 0.4536\n","\n","Epoch 00033: val_loss improved from 8.37893 to 8.37247, saving model to densenet201_720_4.h5\n","Epoch 34/60\n","\n","Epoch 00034: LearningRateScheduler reducing learning rate to 6.9246099048264925e-06.\n","107/107 [==============================] - 120s 1s/step - loss: 0.5327 - sparse_categorical_accuracy: 0.9408 - val_loss: 8.3457 - val_sparse_categorical_accuracy: 0.4564\n","\n","Epoch 00034: val_loss improved from 8.37247 to 8.34566, saving model to densenet201_720_4.h5\n","Epoch 35/60\n","\n","Epoch 00035: LearningRateScheduler reducing learning rate to 6.539687923861194e-06.\n","107/107 [==============================] - 118s 1s/step - loss: 0.5342 - sparse_categorical_accuracy: 0.9408 - val_loss: 8.3257 - val_sparse_categorical_accuracy: 0.4587\n","\n","Epoch 00035: val_loss improved from 8.34566 to 8.32570, saving model to densenet201_720_4.h5\n","Epoch 36/60\n","\n","Epoch 00036: LearningRateScheduler reducing learning rate to 6.231750339088956e-06.\n","107/107 [==============================] - 120s 1s/step - loss: 0.5245 - sparse_categorical_accuracy: 0.9416 - val_loss: 8.3164 - val_sparse_categorical_accuracy: 0.4585\n","\n","Epoch 00036: val_loss improved from 8.32570 to 8.31636, saving model to densenet201_720_4.h5\n","Epoch 37/60\n","\n","Epoch 00037: LearningRateScheduler reducing learning rate to 5.985400271271165e-06.\n","107/107 [==============================] - 120s 1s/step - loss: 0.5190 - sparse_categorical_accuracy: 0.9451 - val_loss: 8.3112 - val_sparse_categorical_accuracy: 0.4600\n","\n","Epoch 00037: val_loss improved from 8.31636 to 8.31116, saving model to densenet201_720_4.h5\n","Epoch 38/60\n","\n","Epoch 00038: LearningRateScheduler reducing learning rate to 5.788320217016932e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.5236 - sparse_categorical_accuracy: 0.9442 - val_loss: 8.3134 - val_sparse_categorical_accuracy: 0.4600\n","\n","Epoch 00038: val_loss did not improve from 8.31116\n","Epoch 39/60\n","\n","Epoch 00039: LearningRateScheduler reducing learning rate to 5.630656173613546e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.5287 - sparse_categorical_accuracy: 0.9446 - val_loss: 8.3166 - val_sparse_categorical_accuracy: 0.4599\n","\n","Epoch 00039: val_loss did not improve from 8.31116\n","Epoch 40/60\n","\n","Epoch 00040: LearningRateScheduler reducing learning rate to 5.5045249388908364e-06.\n","107/107 [==============================] - 118s 1s/step - loss: 0.5494 - sparse_categorical_accuracy: 0.9409 - val_loss: 8.3174 - val_sparse_categorical_accuracy: 0.4599\n","\n","Epoch 00040: val_loss did not improve from 8.31116\n","Epoch 41/60\n","\n","Epoch 00041: LearningRateScheduler reducing learning rate to 5.403619951112669e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.5312 - sparse_categorical_accuracy: 0.9439 - val_loss: 8.3131 - val_sparse_categorical_accuracy: 0.4611\n","\n","Epoch 00041: val_loss did not improve from 8.31116\n","Epoch 42/60\n","\n","Epoch 00042: LearningRateScheduler reducing learning rate to 5.322895960890136e-06.\n","107/107 [==============================] - 119s 1s/step - loss: 0.5297 - sparse_categorical_accuracy: 0.9444 - val_loss: 8.3095 - val_sparse_categorical_accuracy: 0.4621\n","\n","Epoch 00042: val_loss improved from 8.31116 to 8.30949, saving model to densenet201_720_4.h5\n","Epoch 43/60\n","\n","Epoch 00043: LearningRateScheduler reducing learning rate to 5.258316768712109e-06.\n","107/107 [==============================] - 118s 1s/step - loss: 0.5179 - sparse_categorical_accuracy: 0.9456 - val_loss: 8.3039 - val_sparse_categorical_accuracy: 0.4612\n","\n","Epoch 00043: val_loss improved from 8.30949 to 8.30387, saving model to densenet201_720_4.h5\n","Epoch 44/60\n","\n","Epoch 00044: LearningRateScheduler reducing learning rate to 5.206653414969687e-06.\n","107/107 [==============================] - 118s 1s/step - loss: 0.5167 - sparse_categorical_accuracy: 0.9475 - val_loss: 8.3091 - val_sparse_categorical_accuracy: 0.4612\n","\n","Epoch 00044: val_loss did not improve from 8.30387\n","Epoch 45/60\n","\n","Epoch 00045: LearningRateScheduler reducing learning rate to 5.16532273197575e-06.\n","107/107 [==============================] - 119s 1s/step - loss: 0.5051 - sparse_categorical_accuracy: 0.9490 - val_loss: 8.2972 - val_sparse_categorical_accuracy: 0.4621\n","\n","Epoch 00045: val_loss improved from 8.30387 to 8.29724, saving model to densenet201_720_4.h5\n","Epoch 46/60\n","\n","Epoch 00046: LearningRateScheduler reducing learning rate to 5.1322581855806e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.5041 - sparse_categorical_accuracy: 0.9484 - val_loss: 8.2949 - val_sparse_categorical_accuracy: 0.4623\n","\n","Epoch 00046: val_loss improved from 8.29724 to 8.29489, saving model to densenet201_720_4.h5\n","Epoch 47/60\n","\n","Epoch 00047: LearningRateScheduler reducing learning rate to 5.10580654846448e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.5004 - sparse_categorical_accuracy: 0.9483 - val_loss: 8.2928 - val_sparse_categorical_accuracy: 0.4629\n","\n","Epoch 00047: val_loss improved from 8.29489 to 8.29282, saving model to densenet201_720_4.h5\n","Epoch 48/60\n","\n","Epoch 00048: LearningRateScheduler reducing learning rate to 5.084645238771584e-06.\n","107/107 [==============================] - 118s 1s/step - loss: 0.4980 - sparse_categorical_accuracy: 0.9497 - val_loss: 8.2927 - val_sparse_categorical_accuracy: 0.4618\n","\n","Epoch 00048: val_loss improved from 8.29282 to 8.29272, saving model to densenet201_720_4.h5\n","Epoch 49/60\n","\n","Epoch 00049: LearningRateScheduler reducing learning rate to 5.067716191017267e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.4867 - sparse_categorical_accuracy: 0.9501 - val_loss: 8.2920 - val_sparse_categorical_accuracy: 0.4623\n","\n","Epoch 00049: val_loss improved from 8.29272 to 8.29203, saving model to densenet201_720_4.h5\n","Epoch 50/60\n","\n","Epoch 00050: LearningRateScheduler reducing learning rate to 5.054172952813814e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.4857 - sparse_categorical_accuracy: 0.9495 - val_loss: 8.2883 - val_sparse_categorical_accuracy: 0.4633\n","\n","Epoch 00050: val_loss improved from 8.29203 to 8.28835, saving model to densenet201_720_4.h5\n","Epoch 51/60\n","\n","Epoch 00051: LearningRateScheduler reducing learning rate to 5.043338362251051e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.4873 - sparse_categorical_accuracy: 0.9520 - val_loss: 8.2894 - val_sparse_categorical_accuracy: 0.4627\n","\n","Epoch 00051: val_loss did not improve from 8.28835\n","Epoch 52/60\n","\n","Epoch 00052: LearningRateScheduler reducing learning rate to 5.034670689800841e-06.\n","107/107 [==============================] - 116s 1s/step - loss: 0.4847 - sparse_categorical_accuracy: 0.9505 - val_loss: 8.2862 - val_sparse_categorical_accuracy: 0.4630\n","\n","Epoch 00052: val_loss improved from 8.28835 to 8.28618, saving model to densenet201_720_4.h5\n","Epoch 53/60\n","\n","Epoch 00053: LearningRateScheduler reducing learning rate to 5.027736551840673e-06.\n","107/107 [==============================] - 116s 1s/step - loss: 0.4851 - sparse_categorical_accuracy: 0.9502 - val_loss: 8.2878 - val_sparse_categorical_accuracy: 0.4630\n","\n","Epoch 00053: val_loss did not improve from 8.28618\n","Epoch 54/60\n","\n","Epoch 00054: LearningRateScheduler reducing learning rate to 5.022189241472539e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.4714 - sparse_categorical_accuracy: 0.9538 - val_loss: 8.2864 - val_sparse_categorical_accuracy: 0.4638\n","\n","Epoch 00054: val_loss did not improve from 8.28618\n","Epoch 55/60\n","\n","Epoch 00055: LearningRateScheduler reducing learning rate to 5.017751393178031e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.4723 - sparse_categorical_accuracy: 0.9529 - val_loss: 8.2865 - val_sparse_categorical_accuracy: 0.4632\n","\n","Epoch 00055: val_loss did not improve from 8.28618\n","Epoch 56/60\n","\n","Epoch 00056: LearningRateScheduler reducing learning rate to 5.0142011145424245e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.4535 - sparse_categorical_accuracy: 0.9544 - val_loss: 8.2828 - val_sparse_categorical_accuracy: 0.4645\n","\n","Epoch 00056: val_loss improved from 8.28618 to 8.28278, saving model to densenet201_720_4.h5\n","Epoch 57/60\n","\n","Epoch 00057: LearningRateScheduler reducing learning rate to 5.0113608916339395e-06.\n","107/107 [==============================] - 116s 1s/step - loss: 0.4737 - sparse_categorical_accuracy: 0.9536 - val_loss: 8.2806 - val_sparse_categorical_accuracy: 0.4647\n","\n","Epoch 00057: val_loss improved from 8.28278 to 8.28063, saving model to densenet201_720_4.h5\n","Epoch 58/60\n","\n","Epoch 00058: LearningRateScheduler reducing learning rate to 5.009088713307152e-06.\n","107/107 [==============================] - 116s 1s/step - loss: 0.4540 - sparse_categorical_accuracy: 0.9551 - val_loss: 8.2818 - val_sparse_categorical_accuracy: 0.4642\n","\n","Epoch 00058: val_loss did not improve from 8.28063\n","Epoch 59/60\n","\n","Epoch 00059: LearningRateScheduler reducing learning rate to 5.007270970645722e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.4626 - sparse_categorical_accuracy: 0.9545 - val_loss: 8.2777 - val_sparse_categorical_accuracy: 0.4641\n","\n","Epoch 00059: val_loss improved from 8.28063 to 8.27765, saving model to densenet201_720_4.h5\n","Epoch 60/60\n","\n","Epoch 00060: LearningRateScheduler reducing learning rate to 5.005816776516578e-06.\n","107/107 [==============================] - 117s 1s/step - loss: 0.4524 - sparse_categorical_accuracy: 0.9560 - val_loss: 8.2782 - val_sparse_categorical_accuracy: 0.4644\n","\n","Epoch 00060: val_loss did not improve from 8.27765\n","model training for 4 is done\n"]}],"source":["for fold in range(3,5):\n","    train_records=files[:fold]+files[fold+1:]\n","    test_records=files[fold]\n","    NUM_TRAIN_STEPS=train_steps[fold]//BATCH_SIZE\n","    NUM_VALID_STEPS=valid_steps[fold]//BATCH_SIZE\n","    tf.keras.backend.clear_session()\n","    with strategy.scope():\n","        pre_trained=DenseNet201(include_top=False,weights=\"imagenet\",input_shape=(IMAGE_SIZE[0],IMAGE_SIZE[1],3))\n","        ins=tf.keras.layers.Input(())\n","        x=pre_trained.layers[-1].output\n","        x=tf.keras.layers.GlobalMaxPooling2D()(x)\n","        x=tf.keras.layers.Dense(720)(x)\n","        arc_layer=ARCFACE_LAYER()\n","        x=arc_layer([x,ins])\n","        outs=tf.keras.layers.Softmax()(x)\n","        model=tf.keras.models.Model(inputs=(pre_trained.input,ins),outputs=outs)\n","        print(f\"training for fold {fold}\")\n","        #print(model.summary())\n","        model.compile(optimizer=tf.keras.optimizers.Adam(),\n","                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n","    lr_callback=tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch),verbose=1)\n","    early=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",mode=\"min\",verbose=1,patience=10)\n","    saver=tf.keras.callbacks.ModelCheckpoint(filepath=\"densenet201_720_\"+f\"{fold}.h5\",\n","                                     monitor=\"val_loss\",mode=\"min\",save_best_only=True,\n","                                     save_weights_only=True,verbose=1)\n","    train_dataloader=Train_data_pipe(train_records)\n","    valid_dataloader=Valid_data_pipe(test_records)\n","    model.fit(train_dataloader,\n","              validation_data=valid_dataloader,epochs=60,\n","              callbacks=[early,saver,lr_callback],steps_per_epoch=NUM_TRAIN_STEPS,\n","             validation_steps=NUM_VALID_STEPS)\n","    print(f\"model training for {fold} is done\")\n","    del model\n","    import gc\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:44:54.7655Z","iopub.status.idle":"2022-06-26T13:44:54.765863Z","shell.execute_reply":"2022-06-26T13:44:54.765699Z","shell.execute_reply.started":"2022-06-26T13:44:54.765682Z"},"trusted":true},"outputs":[],"source":["#Experiments 5 fold error\n","with dense\n","1) B0->1e-5 to 4e-4 -> 12.648266\n","   B0->1e-6 to 4e-5 -> \"very high error\"\n","   B0 ->1e-6 to 4e-4 -> 12.823738\n","without dense\n","1) B0->1e-5 to 4e-4 -> 13.708846\n","##################################################\n","(only 1 fold)\n","no dense\n","1) B0->1e-5 to 4e-4->13.8519\n","2) B0-> 1e-5 to 1e-3->11.1301\n","   B0-> 1e-6 to 1e-3->11.08619\n","With dense\n","1) B0->1e-5 to 4e-4-> 12.53761\n","2) B0-> 1e-5 to 1e-3->10.94927\n","   B0-> 1e-6 to 1e-3->10.72708(30 epochs)\n","###############################################\n","B3(dense)\n","1) B3 -> 1e-6 to 1e-3 -> 10.26380(50 epochs)\n","2) B3 -> 5e-6 to 1e-3 -> 10.11745(44 epochs)\n","B3(no dense)\n","5e-6 to 1e-3->10.06651(40 epochs)\n","1e-6 to 1e-3 -> 10.17261\n","1e-5 to 4e-4 -> 11.89102\n","#may be during training I observed that in b3 case dense layer is causing overfiitting at first.\n","##########################################\n","B5(no dense)\n","5e-6 to 1e-3 -> 9.59691\n","B5(no dense removed shift scale rotate)\n","5 fold error\n","5e-6 to 1e-3 ->9.31131\n","#################################################DenseNet121############################################\n","5fold\n","(no dense)\n","-> no shift,scale,rotate\n","9.55024\n","-> shift,scale,rotate\n","9.42903\n","(dense)\n","-> shift,scale,rotate\n","8.38903\n","#############################densenet169####################################\n","1fold\n","dense\n","-> shift,scale,rotate\n","8.18931\n","-> no shift,scale,rotate\n","nodense\n","-> shift,scale,rotate\n","8.87463\n","-> no shift,scale,rotate\n","#############################densenet201##########################\n","1fold\n","dense(720)\n","-> shift,scale,rotate\n","8.12410------------------------\n","dense(512)\n","-> shift,scale,rotate\n","8.20654\n","nodense\n","-> shift,scale,rotate\n","8.75749\n","##############effb7##########\n","1fold\n","dense(720)\n","-> shift,scale,rotate\n","9.26953(57 epochs)\n","##############Resnet50v2############\n","1fold\n","dense(720)\n","-> shift,scale,rotate\n","8.55440\n","#################ResNet152V2###############\n","1fold\n","dense(720)\n","-> shift,scale,rotate\n","8.38491"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["8.11574,8.02224,8.04300,7.96431,8.27765 (5 fold densenet201 errors)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":4}
