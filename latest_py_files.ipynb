{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ySSYzn3xfhdt"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MFZ3M50B8Vpp"
      },
      "outputs": [],
      "source": [
        "os.mkdir(\"utils\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta56TM6iNNwr",
        "outputId": "5edde188-42de-4ca3-be3b-969c69ef04b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing utils/randomness.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils/randomness.py\n",
        "import os\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "def set_randomness():\n",
        "  '''\n",
        "  Sets the randomness in the code. But still during training we may have\n",
        "  randomness because we use GPU.\n",
        "\n",
        "  '''\n",
        "  os.environ['PYTHONHASHSEED']=\"0\"\n",
        "  rn.seed(42)\n",
        "  np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC91KUcH6iBe",
        "outputId": "fae3ec2f-2005-4b5f-b73e-565838885e32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing EDA.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile EDA.py\n",
        "\"\"\"\n",
        "The EDA and Cleaning the data.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as implt\n",
        "\n",
        "\n",
        "def Visualize(df):\n",
        "  paths=np.random.choice(df['image_path'].values,size=32,replace=False)\n",
        "  plt.subplots(8,4,figsize=(30,30))\n",
        "  for i,path in enumerate(paths):\n",
        "    plt.subplot(8,4,i+1)\n",
        "    img=implt.imread(path)\n",
        "    plt.imshow(img)\n",
        "  plt.savefig\n",
        "  if not os.path.isdir(\"plots\"):\n",
        "    os.mkdir(\"plots\")\n",
        "  plt.savefig(\"plots/random_images.jpg\")\n",
        "  if args.check_sizes:\n",
        "    print(\"checking height distributions\")\n",
        "    df['height']=df['image_path'].apply(lambda x: implt.imread(path).shape[0])\n",
        "    df['width']=df['image_path'].apply(lambda x: implt.imread(path).shape[1])\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.hist(df['height'])\n",
        "    plt.title(\"height distributions\")\n",
        "    plt.savefig(\"plots/height_distributions.jpg\")\n",
        "    print(\"checking weigth distributions\")\n",
        "    plt.figure(figsize=(15,10))\n",
        "    plt.hist(df['width'])\n",
        "    plt.title(\"weidth distributions\")\n",
        "    plt.savefig(\"plots/weigth_distributions.jpg\")\n",
        "  \n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  parser=argparse.ArgumentParser(description=\"EDA and cleaning\")\n",
        "  parser.add_argument(\"--data_path\",help=\"path to the data file\", type=pathlib.Path,required = True)\n",
        "  parser.add_argument(\"--check_sizes\",help=\"whether to check the distributions of height and width of images\", \n",
        "                      type=bool)\n",
        "  args=parser.parse_args()\n",
        "  df=pd.read_csv(args.data_path)\n",
        "  df['image_path']=df['image_path']=df['image'].apply(lambda x: \"/content/train_images/\"+x)\n",
        "  if not os.path.isdir(\"processed_data\"):\n",
        "    os.mkdir(\"processed_data\")\n",
        "  df.to_csv(\"processed_data/cleaned_data.csv\",index=False)\n",
        "  Visualize(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xtu741JOB-O",
        "outputId": "829b94ee-1c7a-4660-8998-bcfef85a6165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing create_fold.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile create_fold.py\n",
        "\"\"\"\n",
        "This file creates the group kfold split to the data. \n",
        "And applies label Encoder to the target Feature.\n",
        "\"\"\"\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from utils.randomness import *\n",
        "import argparse\n",
        "import pickle\n",
        "import pathlib\n",
        "\n",
        "def CREATE_FOLD(args):\n",
        "  df=pd.read_csv(args.data_path)\n",
        "  encoder=LabelEncoder()\n",
        "  df['label_group']=encoder.fit_transform(df['label_group'])\n",
        "  print(\"label encoding is done\")\n",
        "  if not os.path.isdir(\"encoders\"):\n",
        "    os.mkdir(\"encoders\")\n",
        "  with open(\"encoders/label_group_encoder.pkl\",\"wb\") as f:\n",
        "    pickle.dump(encoder,f)\n",
        "  df['gfold']=-1\n",
        "  gfold=GroupKFold(n_splits=5)\n",
        "  for i,(train,test) in enumerate(gfold.split(df,groups=df['label_group'])):\n",
        "    df.loc[test,'gfold']=i\n",
        "  print(\"created the group kfold\")\n",
        "  if not os.path.isdir(\"processed_data\"):\n",
        "    os.mkdir(\"processed_data\")\n",
        "    print(\"created processed_data directory\")\n",
        "  df.to_csv(\"processed_data/fold_data.csv\",index=False)\n",
        "  print(\"group kfold data is stored at processed_data/fold_data.csv\")\n",
        "if __name__==\"__main__\":\n",
        "  set_randomness()\n",
        "  parser=argparse.ArgumentParser(description=\"create folds\")\n",
        "  parser.add_argument(\"--data_path\",help=\"path to the data file\", type=pathlib.Path,required = True)\n",
        "  args=parser.parse_args()\n",
        "  CREATE_FOLD(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GxO0rQg93UiF"
      },
      "outputs": [],
      "source": [
        "#from tensorflow.keras.applications import EfficientNetB4\n",
        "#pre_trained_model=EfficientNetB4(include_top=False,input_shape=(512,512,3))\n",
        "#for i,layer in enumerate(pre_trained_model.layers):\n",
        "#  print(i,layer.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVbxb8Oz0ZkY",
        "outputId": "128ffea2-153c-458e-f46c-240971a46912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing params.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile params.yaml\n",
        "data:\n",
        " initial_file: \"train.csv\"\n",
        "model_type: \"image\"\n",
        "image:\n",
        " image_size: (512,512)\n",
        " unfreeze: 324\n",
        " pre_trained_name: \"EfficientNetB4\"\n",
        "text:\n",
        "  max_length: 512\n",
        "  pre_trained_name: \"bert-base-uncased\"\n",
        "scheduler: \"one_cycle\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4LzLxSFPRg1",
        "outputId": "e40d4f06-da43-4e52-9dda-356ca204b3ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting dvc.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile dvc.yaml\n",
        "vars:\n",
        "- params.yaml\n",
        "stages:\n",
        " eda:\n",
        "  cmd: python EDA.py --data_path ${data.initial_file} --check_sizes True\n",
        "  params:\n",
        "  - ${data.initial_file}\n",
        "  deps:\n",
        "  - EDA.py\n",
        "  outs:\n",
        "  - processed_data/cleaned_data.csv\n",
        "  plots:\n",
        "  - plots/weigth_distributions.jpg:\n",
        "      cache: false\n",
        "  - plots/height_distributions.jpg:\n",
        "      cache: false\n",
        "  - plots/random_images.jpg:\n",
        "      cache: false\n",
        " Folds:\n",
        "  cmd: python create_fold.py --data_path \"processed_data/cleaned_data.csv\"\n",
        "  deps:\n",
        "  - create_fold.py\n",
        "  - processed_data/cleaned_data.csv\n",
        "  outs:\n",
        "  - encoders/label_group_encoder.pkl\n",
        "  - processed_data/fold_data.csv\n",
        " training:\n",
        "  cmd: >\n",
        "  python training.py --data_path processed_data/fold_data.csv --model_type ${model_type} \\\n",
        "  --batch_size 32 --save_model_path \"models_dir/image_test\" --epochs 30 --lr_callback ${scheduler}\n",
        "  deps:\n",
        "  - training.py\n",
        "  - processed_data/fold_data.csv\n",
        "  - utils/models/py\n",
        "  - utils/dataloaders/py\n",
        "  outs:\n",
        "  - \"models_dir/image_test-0.h5\"\n",
        "  - \"models_dir/image_test-1.h5\"\n",
        "  - \"models_dir/image_test-2.h5\"\n",
        "  - \"models_dir/image_test-3.h5\"\n",
        "  - \"models_dir/image_test-4.h5\"\n",
        "  params:\n",
        "  - ${scheduler}\n",
        "  - ${model_type}\n",
        "  - ${image.unfreeze}\n",
        "  - ${image.pretrained_name}\n",
        "  - ${text.pretrained_name}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65kOUYDM1Hza",
        "outputId": "6dabe5c8-cd53-4e43-b121-bc322b4ded23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing params.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile params.py\n",
        "\n",
        "import yaml\n",
        "with open(\"params.yaml\") as f:\n",
        "  HYPERPARAMETERS=yaml.safe_load(f)\n",
        "\n",
        "HYPERPARAMETERS['image']['image_size']=tuple(int(x) for x in HYPERPARAMETERS['image']['image_size'][1:-1].split(\",\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8a7DNQTgBmu",
        "outputId": "709f3a16-43aa-4fef-8ba5-6ad4a94d6f28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing utils/models.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils/models.py\n",
        "\"\"\"\n",
        "Below we have models for image, text and combined models and arcface layer\n",
        "\n",
        "\"\"\"\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,Input,Layer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import EfficientNetB4\n",
        "from transformers import TFBertModel,TFRobertaModel,TFAlbertModel,TFXLNetModel\n",
        "\n",
        "\n",
        "class ARCFACE_LAYER(Layer):\n",
        "  def __init__(self,m=0.5,s=60,n_classes=11014):\n",
        "    super(ARCFACE_LAYER,self).__init__()\n",
        "    self.m=m\n",
        "    self.s=s\n",
        "    self.sin_m=tf.sin(m)\n",
        "    self.cos_m=tf.cos(m)\n",
        "    self.n_classes=n_classes\n",
        "    self.threshold = tf.cos(math.pi - m)\n",
        "    self.mm = tf.math.sin(math.pi - m) * m\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    prev_layer_units=input_shape[0][1]\n",
        "    self.w=self.add_weight(shape=(prev_layer_units,self.n_classes),trainable=True)\n",
        "\n",
        "  def get_config(self):\n",
        "    config=super().get_config()\n",
        "    config.update({\"m\":0.5,\n",
        "                   \"s\":60,\n",
        "                   \"n_classes\":11014})\n",
        "    return config\n",
        "\n",
        "\n",
        "  def call(self,inputs):\n",
        "    prev_layer,y=inputs\n",
        "    y=tf.cast(y,dtype=tf.int32)\n",
        "    y_hot=tf.one_hot(y,self.n_classes)\n",
        "    y_hot=tf.cast(y_hot,dtype=tf.float32)\n",
        "    w_norm=tf.linalg.l2_normalize(self.w,axis=0)\n",
        "    x_norm=tf.linalg.l2_normalize(prev_layer,axis=1)\n",
        "    cos_theta=tf.linalg.matmul(x_norm,w_norm)\n",
        "    cos_theta=tf.keras.backend.clip(cos_theta,-1+1e-5,1-1e-5)\n",
        "    sin_theta=tf.sqrt(1-tf.pow(cos_theta,tf.cast(2,dtype=tf.float32)))\n",
        "    cos_theta_m=(cos_theta*self.cos_m)-(sin_theta*self.sin_m)\n",
        "    cos_theta_m=tf.where(cos_theta_m>self.cos_m,cos_theta_m,cos_theta-self.mm)\n",
        "    final=self.s*((y_hot*cos_theta_m)+((1-y_hot)*cos_theta))\n",
        "    return final\n",
        "  \n",
        "\n",
        "\n",
        "def IMAGE_MODEL(image_size,unfreeze_layers_number):\n",
        "  tf.keras.backend.clear_session()\n",
        "  pre_trained=EfficientNetB4(include_top=False,weights=\"imagenet\",input_shape=(image_size[0],image_size[1],3))\n",
        "  ins=Input((),name=\"label_input\")\n",
        "  for i,layer in enumerate(pre_trained.layers):\n",
        "    if i>=unfreeze_layers_number:\n",
        "      if not layer.name.endswith(\"bn\"):\n",
        "        pre_trained.layers[i].trainable=True\n",
        "      else:\n",
        "        pre_trained.layers[i].trainable=False\n",
        "    else:\n",
        "      pre_trained.layers[i].trainable=False\n",
        "  x=pre_trained.layers[-1].output\n",
        "  x=tf.keras.layers.GlobalMaxPooling2D()(x)\n",
        "  x=Dense(512)(x)\n",
        "  arc_layer=ARCFACE_LAYER()\n",
        "  x=arc_layer([x,ins])\n",
        "  outs=tf.keras.layers.Softmax()(x)\n",
        "  model=Model(inputs=(pre_trained.input,ins),outputs=outs)\n",
        "  return model\n",
        "\n",
        "def TEXT_MODEL(pre_trained_name,max_length):\n",
        "  tf.keras.backend.clear_session()\n",
        "  input_ids=Input((max_length,),dtype=tf.int32)\n",
        "  attention_mask=Input((max_length,),dtype=tf.int32)\n",
        "  token_type_ids=Input((max_length,),dtype=tf.int32)\n",
        "  ins=Input((),name=\"label_input\")\n",
        "  pre_trained=TFBertModel.from_pretrained(pre_trained_name,output_hidden_states=True)\n",
        "  pre_outputs=pre_trained({\"input_ids\":input_ids,\"attention_mask\":attention_mask,\n",
        "                  \"token_type_ids\":token_type_ids})\n",
        "  hidden_layers=[]\n",
        "  for i in range(4):\n",
        "    hidden_layers.append(pre_outputs['hidden_states'][-i])\n",
        "  x=tf.keras.layers.Concatenate()(hidden_layers)[:,0,:]\n",
        "  x=Dense(512)(x)\n",
        "  arc_layer=ARCFACE_LAYER()\n",
        "  x=arc_layer([x,ins])\n",
        "  outs=tf.keras.layers.Softmax()(x)\n",
        "  model=Model(inputs=({\"input_ids\":input_ids,\"attention_mask\":attention_mask,\n",
        "                 \"token_type_ids\":token_type_ids},ins),outputs=outs)\n",
        "  return model\n",
        "\n",
        "def COMBINE_MODEL(max_length,image_size,unfreeze_layers_number):\n",
        "  tf.keras.backend.clear_session()\n",
        "  input_ids=Input((max_length,),dtype=tf.int32)\n",
        "  attention_mask=Input((max_length,),dtype=tf.int32)\n",
        "  token_type_ids=Input((max_length,),dtype=tf.int32)\n",
        "  ins=Input((),name=\"label_input\")\n",
        "  text_trained=TFBertModel.from_pretrained(\"bert-base-uncased\",output_hidden_states=True)\n",
        "  text_outputs=text_trained({\"input_ids\":input_ids,\"attention_mask\":attention_mask,\n",
        "                  \"token_type_ids\":token_type_ids})\n",
        "  hidden_layers=[]\n",
        "  for i in range(4):\n",
        "    hidden_layers.append(text_outputs['hidden_states'][-i])\n",
        "  x1=tf.keras.layers.Concatenate()(hidden_layers)[:,0,:]\n",
        "  ################\n",
        "  img_trained=DenseNet201(include_top=False,weights=\"imagenet\",input_shape=(image_size[0],image_size[1],3))\n",
        "  for i,layer in enumerate(img_trained.layers):\n",
        "    if i>=unfreeze_layers_number:\n",
        "      if not layer.name.endswith(\"bn\"):\n",
        "        img_trained.layers[i].trainable=True\n",
        "      else:\n",
        "        img_trained.layers[i].trainable=False\n",
        "    else:\n",
        "      img_trained.layers[i].trainable=False\n",
        "  x2=img_trained.layers[-1].output\n",
        "  x2=tf.keras.layers.GlobalMaxPooling2D()(x2)\n",
        "  ################\n",
        "  x=tf.keras.layers.Concatenate()([x1,x2])\n",
        "  x=Dense(512)(x)\n",
        "  arc_layer=ARCFACE_LAYER()\n",
        "  x=arc_layer([x,ins])\n",
        "  outs=tf.keras.layers.Softmax()(x)\n",
        "  model=Model(inputs=({\"input_ids\":input_ids,\"attention_mask\":attention_mask,\n",
        "                 \"token_type_ids\":token_type_ids},\n",
        "                 img_trained.input,ins),outputs=outs)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QACOI4yDDYZ0",
        "outputId": "6968c1e4-894e-4d91-d1d3-8cdc967bf6cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing utils/dataloaders.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils/dataloaders.py\n",
        "\"\"\"\n",
        "Data loaders for the models\n",
        "\n",
        "\"\"\"\n",
        "import albumentations as A\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "class IMG_DATA_LOADER(tf.keras.utils.Sequence):\n",
        "  def __init__(self,dataframe,image_size,batch_size,aug,shuffle,inference=False):\n",
        "    self.data=dataframe\n",
        "    self.batch_size=batch_size\n",
        "    self.shuffle=shuffle\n",
        "    self.image_size=image_size\n",
        "    self.aug=aug\n",
        "    self.inference=inference\n",
        "    self.n=0\n",
        "    self.max_=self.__len__()\n",
        "    self.indexes=np.arange(self.data.shape[0])\n",
        "    self.temp_indexes=np.arange(self.data.shape[0])\n",
        "    if not self.inference:\n",
        "      self.on_epoch_end()\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.ceil(self.data.shape[0]/self.batch_size))\n",
        "  \n",
        "  def on_epoch_end(self):\n",
        "    if self.shuffle:\n",
        "      np.random.shuffle(self.temp_indexes)\n",
        "  \n",
        "  def next(self):\n",
        "    if self.n>self.max_:\n",
        "      self.n=0\n",
        "      result=self.__getitem__(self.n)\n",
        "      self.n+=1\n",
        "    else:\n",
        "      result=self.__getitem__(self.n)\n",
        "      self.n+=1\n",
        "    return result\n",
        "  \n",
        "  def Augment_images(self,image):\n",
        "    transformer=A.Compose([A.Rotate(limit=30,p=0.8),\n",
        "          A.HorizontalFlip(),\n",
        "          #A.CoarseDropout(max_height=0.25,max_width=0.25,),\n",
        "          A.ShiftScaleRotate(shift_limit=0.09,scale_limit=0.2,rotate_limit=0),\n",
        "          A.RandomBrightnessContrast()\n",
        "          ])\n",
        "    image=transformer(image=image)['image']\n",
        "    return image\n",
        "\n",
        "  def __getitem__(self,batch):\n",
        "    curr_temp_indexes=self.temp_indexes[batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "    curr_batch=list(self.indexes[i] for i in curr_temp_indexes)\n",
        "    IMAGES=np.zeros((len(curr_batch),self.image_size[0],self.image_size[1],3))\n",
        "    if not self.inference:\n",
        "      Y=np.zeros((len(curr_batch),))      \n",
        "    for i,idx in enumerate(curr_batch):\n",
        "      img_name=self.data.iloc[idx]['image_path']\n",
        "      labels=self.data.iloc[idx]['label_group']\n",
        "      img=tf.keras.preprocessing.image.load_img(img_name,target_size=self.image_size)\n",
        "      img=tf.keras.preprocessing.image.img_to_array(img)/255.0\n",
        "      if self.aug:\n",
        "        img=self.Augment_images(img)\n",
        "      IMAGES[i,]=img\n",
        "      if not self.inference:\n",
        "        Y[i,]=labels\n",
        "    if not self.inference:\n",
        "      return (IMAGES,Y),Y\n",
        "    else:\n",
        "      return IMAGES\n",
        "\n",
        "\n",
        "class TEXT_DATA_LOADER(tf.keras.utils.Sequence):\n",
        "  def __init__(self,dataframe,max_length,pre_trained_name,batch_size,shuffle,inference=False):\n",
        "    self.data=dataframe\n",
        "    self.batch_size=batch_size\n",
        "    self.shuffle=shuffle\n",
        "    self.max_length=max_length\n",
        "    self.pre_trained_name=pre_trained_name\n",
        "    self.inference=inference\n",
        "    self.tokenizer=BertTokenizer.from_pretrained(self.pre_trained_name)\n",
        "    self.n=0\n",
        "    self.max_=self.__len__()\n",
        "    self.indexes=np.arange(self.data.shape[0])\n",
        "    self.temp_indexes=np.arange(self.data.shape[0])\n",
        "    if not self.inference:\n",
        "      self.on_epoch_end()\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.ceil(self.data.shape[0]/self.batch_size))\n",
        "  \n",
        "  def on_epoch_end(self):\n",
        "    if self.shuffle:\n",
        "      np.random.shuffle(self.temp_indexes)\n",
        "  \n",
        "  def next(self):\n",
        "    if self.n>self.max_:\n",
        "      self.n=0\n",
        "      result=self.__getitem__(self.n)\n",
        "      self.n+=1\n",
        "    else:\n",
        "      result=self.__getitem__(self.n)\n",
        "      self.n+=1\n",
        "    return result\n",
        "  \n",
        "  def __getitem__(self,batch):\n",
        "    curr_temp_indexes=self.temp_indexes[batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "    curr_batch=list(self.indexes[i] for i in curr_temp_indexes)\n",
        "    INPUT_IDS=np.zeros((len(curr_batch),self.max_length),dtype=np.int32)\n",
        "    ATTENTION_MASK=np.zeros((len(curr_batch),self.max_length),dtype=np.int32)\n",
        "    TOKEN_TYPE_IDS=np.zeros((len(curr_batch),self.max_length),dtype=np.int32)\n",
        "    if not self.inference:\n",
        "      Y=np.zeros((len(curr_batch),))\n",
        "    for i,idx in enumerate(curr_batch):\n",
        "      title=self.data.iloc[idx]['title']\n",
        "      labels=self.data.iloc[idx]['label_group']\n",
        "      tokenized_title=self.tokenizer.encode_plus(title,padding=\"max_length\",\n",
        "                                                truncation=\"longest_first\",max_length=self.max_length)\n",
        "      \n",
        "      INPUT_IDS[i,]=tokenized_title['input_ids']\n",
        "      ATTENTION_MASK[i,]=tokenized_title['attention_mask']\n",
        "      TOKEN_TYPE_IDS[i,]=tokenized_title['token_type_ids']\n",
        "      if not self.inference:\n",
        "        Y[i,]=labels\n",
        "    if not self.inference:\n",
        "      return ({\"input_ids\":INPUT_IDS,\"attention_mask\":ATTENTION_MASK,\n",
        "              \"token_type_ids\": TOKEN_TYPE_IDS},Y),Y\n",
        "    else:\n",
        "      return {\"input_ids\":INPUT_IDS,\"attention_mask\":ATTENTION_MASK,\n",
        "              \"token_type_ids\": TOKEN_TYPE_IDS}\n",
        "\n",
        "class BOTH_DATA_LOADER(tf.keras.utils.Sequence):\n",
        "  def __init__(self,dataframe,image_size,batch_size,max_length,text_pre_trained_name,aug,shuffle,inference=False):\n",
        "    self.data=dataframe\n",
        "    self.batch_size=batch_size\n",
        "    self.shuffle=shuffle\n",
        "    self.inference=inference\n",
        "    self.image_size=image_size\n",
        "    self.aug=aug\n",
        "    self.max_length=max_length\n",
        "    self.pre_trained_name=text_pre_trained_name\n",
        "    self.tokenizer=BertTokenizer.from_pretrained(self.pre_trained_name)\n",
        "    self.n=0\n",
        "    self.max_=self.__len__()\n",
        "    self.indexes=np.arange(self.data.shape[0])\n",
        "    self.temp_indexes=np.arange(self.data.shape[0])\n",
        "    if not self.inference:\n",
        "      self.on_epoch_end()\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.ceil(self.data.shape[0]/self.batch_size))\n",
        "  \n",
        "  def on_epoch_end(self):\n",
        "    if self.shuffle:\n",
        "      np.random.shuffle(self.temp_indexes)\n",
        "  \n",
        "  def next(self):\n",
        "    if self.n>self.max_:\n",
        "      self.n=0\n",
        "      result=self.__getitem__(self.n)\n",
        "      self.n+=1\n",
        "    else:\n",
        "      result=self.__getitem__(self.n)\n",
        "      self.n+=1\n",
        "    return result\n",
        "  \n",
        "  def Augment_images(self,image):\n",
        "    transformer=A.Compose([A.Rotate(limit=30,p=0.8),\n",
        "          A.HorizontalFlip(),\n",
        "          #A.CoarseDropout(max_height=0.25,max_width=0.25,),\n",
        "          A.ShiftScaleRotate(shift_limit=0.09,scale_limit=0.2,rotate_limit=0),\n",
        "          A.RandomBrightnessContrast()\n",
        "          ])\n",
        "    image=transformer(image=image)['image']\n",
        "    return image\n",
        "\n",
        "  def __getitem__(self,batch):\n",
        "    curr_temp_indexes=self.temp_indexes[batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "    curr_batch=list(self.indexes[i] for i in curr_temp_indexes)\n",
        "    IMAGES=np.zeros((len(curr_batch),self.image_size[0],self.image_size[1],3))\n",
        "    INPUT_IDS=np.zeros((len(curr_batch),self.max_length),dtype=np.int32)\n",
        "    ATTENTION_MASK=np.zeros((len(curr_batch),self.max_length),dtype=np.int32)\n",
        "    TOKEN_TYPE_IDS=np.zeros((len(curr_batch),self.max_length),dtype=np.int32)\n",
        "    if not self.inference:\n",
        "      Y=np.zeros((len(curr_batch),))\n",
        "    for i,idx in enumerate(curr_batch):\n",
        "      img_name=self.data.iloc[idx]['image_path']\n",
        "      labels=self.data.iloc[idx]['label_group']\n",
        "      img=tf.keras.preprocessing.image.load_img(img_name,target_size=self.image_size)\n",
        "      img=tf.keras.preprocessing.image.img_to_array(img)/255.0\n",
        "      if self.aug:\n",
        "        img=self.Augment_images(img)\n",
        "      IMAGES[i,]=img\n",
        "      #############################\n",
        "      title=self.data.iloc[idx]['title']\n",
        "      tokenized_title=self.tokenizer.encode_plus(title,padding=\"max_length\",\n",
        "                                                truncation=\"longest_first\",max_length=self.max_length)\n",
        "      \n",
        "      INPUT_IDS[i,]=tokenized_title['input_ids']\n",
        "      ATTENTION_MASK[i,]=tokenized_title['attention_mask']\n",
        "      TOKEN_TYPE_IDS[i,]=tokenized_title['token_type_ids']\n",
        "      if not self.inference:\n",
        "        Y[i,]=labels\n",
        "    if not self.inference:\n",
        "      return ({\"input_ids\":INPUT_IDS,\"attention_mask\":ATTENTION_MASK,\n",
        "            \"token_type_ids\": TOKEN_TYPE_IDS},IMAGES,Y),Y\n",
        "    else:\n",
        "      return ({\"input_ids\":INPUT_IDS,\"attention_mask\":ATTENTION_MASK,\n",
        "            \"token_type_ids\": TOKEN_TYPE_IDS},IMAGES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwwCAuA3T-C9",
        "outputId": "fc571f87-5c70-464f-fc00-7929195a812d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing training.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile training.py\n",
        "\"\"\"\n",
        "It is the training function for different types of models\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from utils.models import *\n",
        "from utils.randomness import *\n",
        "from utils.dataloaders import *\n",
        "from params import HYPERPARAMETERS\n",
        "\n",
        "\n",
        "def CE(y_true,y_pred):\n",
        "  '''\n",
        "  loss function = y*log(y_hat)\n",
        "  '''\n",
        "  y_true=tf.cast(y_true,dtype=tf.int32)\n",
        "  y_true=tf.one_hot(y_true,depth=11014)\n",
        "  y_true=tf.cast(y_true,dtype=y_pred.dtype)\n",
        "  ce_loss=y_true*tf.keras.backend.log(y_pred+1e-5)\n",
        "  batch_loss=tf.reduce_sum(ce_loss,axis=-1)\n",
        "  return -1*tf.reduce_mean(batch_loss)\n",
        "\n",
        "def one_cycle(epoch,lr_min=1e-5,lr_max=2e-4):\n",
        "  if epoch<5:\n",
        "    lr=(lr_max-lr_min)/5 *(epoch) + lr_min\n",
        "  elif epoch==5:\n",
        "    lr=lr_max\n",
        "  else:\n",
        "    lr= (lr_max-lr_min) * 0.8**(epoch-5) +lr_min\n",
        "  return lr\n",
        "\n",
        "def TRAINING(args):\n",
        "  df=pd.read_csv(args.data_path)\n",
        "  for fold in range(5):\n",
        "    train_data=df.loc[df['gfold']!=fold].drop(\"gfold\",axis=1).reset_index(drop=True)\n",
        "    test_data=df.loc[df['gfold']==fold].drop(\"gfold\",axis=1).reset_index(drop=True)\n",
        "\n",
        "    if args.model_type==\"image\":\n",
        "      model=IMAGE_MODEL(image_size=HYPERPARAMETERS[\"image\"][\"image_size\"],unfreeze_layers_number=HYPERPARAMETERS[\"image\"]['unfreeze'])\n",
        "      train_dataloader=IMG_DATA_LOADER(dataframe=train_data,image_size=HYPERPARAMETERS[\"image\"]['image_size'],\n",
        "                                       batch_size=args.batch_size,aug=True,shuffle=True)\n",
        "      test_dataloader=IMG_DATA_LOADER(dataframe=test_data,image_size=HYPERPARAMETERS[\"image\"]['image_size'],\n",
        "                                      batch_size=args.batch_size,aug=False,shuffle=False)\n",
        "\n",
        "    elif args.model_type==\"text\":\n",
        "      model=TEXT_MODEL(pre_trained_name=HYPERPARAMETERS[\"text\"][\"pre_trained_name\"],max_length=HYPERPARAMETERS[\"text\"][\"max_length\"])\n",
        "      train_dataloader=TEXT_DATA_LOADER(dataframe=train_data,max_length=HYPERPARAMETERS[\"text\"][\"max_length\"],\n",
        "                                        pre_trained_name=HYPERPARAMETERS[\"text\"][\"pre_trained_name\"],batch_size=args.batch_size,shuffle=True)\n",
        "      test_dataloader=TEXT_DATA_LOADER(dataframe=test_data,max_length=HYPERPARAMETERS[\"text\"][\"max_length\"],\n",
        "                                       pre_trained_name=HYPERPARAMETERS[\"text\"][\"pre_trained_name\"],batch_size=args.batch_size,shuffle=False)\n",
        "\n",
        "    else:\n",
        "      model=COMBINE_MODEL(max_length=HYPERPARAMETERS[\"text\"][\"max_length\"],image_size=HYPERPARAMETERS[\"image\"][\"image_size\"],\n",
        "                          unfreeze_layers_number=HYPERPARAMETERS[\"image\"]['unfreeze'])\n",
        "      train_dataloader=BOTH_DATA_LOADER(dataframe=train_data,batch_size=args.batch_size,\n",
        "                                        image_size=HYPERPARAMETERS[\"image\"]['image_size'],\n",
        "                                        max_length=HYPERPARAMETERS[\"text\"][\"max_length\"],\n",
        "                                        text_pre_trained_name=HYPERPARAMETERS[\"text\"][\"pre_trained_name\"],aug=True,shuffle=True)\n",
        "      test_dataloader=BOTH_DATA_LOADER(dataframe=test_data,batch_size=args.batch_size,\n",
        "                                       image_size=HYPERPARAMETERS[\"image\"]['image_size'],\n",
        "                                       max_length=HYPERPARAMETERS[\"text\"][\"max_length\"],\n",
        "                                       text_pre_trained_name=HYPERPARAMETERS[\"text\"][\"pre_trained_name\"],aug=False,shuffle=False)\n",
        "      \n",
        "    model.compile(\"Adam\",loss=CE)\n",
        "    up_down=tf.keras.callbacks.LearningRateScheduler(lambda epoch: one_cycle(epoch),verbose=1)\n",
        "    reduce_plat=tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",mode=\"min\",\n",
        "                                                    patience=5,verbose=1,cooldown=2,\n",
        "                                                    min_lr=1e-6)\n",
        "    early=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",mode=\"min\",verbose=1)\n",
        "    saver=tf.keras.callbacks.ModelCheckpoint(filepath=args.save_model_path+f\"{fold}.h5\",\n",
        "                                             monitor=\"val_loss\",mode=\"min\",save_best_only=True,\n",
        "                                             save_weights_only=True)\n",
        "    if fold==0:\n",
        "      print(model.summary())\n",
        "    model.fit(train_dataloader,validation_data=test_dataloader,epochs=args.epochs,\n",
        "              callbacks=[early,saver,up_down] if args.lr_callback==\"one_cycle\" \\\n",
        "                         else [early,saver,reduce_plat] if args.lr_callback==\"reduce_lr_plateau\" \\\n",
        "              else [early,saver]\n",
        "              )\n",
        "    \n",
        "    print(f\"model training for {fold} is done\")\n",
        "    del model\n",
        "    import gc\n",
        "    gc.collect()\n",
        "  \n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  parser=argparse.ArgumentParser(description=\"training the models\")\n",
        "  parser.add_argument(\"--data_path\",help=\"path to the data file\", type=pathlib.Path,required = True)\n",
        "  parser.add_argument(\"--model_type\",help=\"type of model\", choices=[\"image\",\"text\",\"both\"],type=str,required = True)\n",
        "  parser.add_argument(\"--batch_size\",help=\"batch size for the model\", type=int, default=32,required = True)\n",
        "  parser.add_argument(\"--save_model_path\",help=\"model path along with name where to save it\", type=str,required = True)\n",
        "  parser.add_argument(\"--epochs\",help=\"number of epochs\", type=int,default=30,required = True)\n",
        "  parser.add_argument(\"--lr_callback\",help=\"type of lr scheduler\", choices=[\"one_cycle\",\"reduce_lr_plateau\",\"None\"],\n",
        "                      required=True,type=str)\n",
        "  args=parser.parse_args()\n",
        "  set_randomness()\n",
        "  TRAINING(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AS-K5MB78fO",
        "outputId": "a82e06a2-bc97-4427-beea-427f6113ba6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reinitialized existing Git repository in /content/.git/\n"
          ]
        }
      ],
      "source": [
        "!git init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0_3z3tPgANh",
        "outputId": "09d0f73c-19f4-4dab-a883-3cfa6370beb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing .gitignore\n"
          ]
        }
      ],
      "source": [
        "%%writefile .gitignore\n",
        "train_images/\n",
        "test_images/\n",
        "sample_data/\n",
        "sample_submission.csv\n",
        "shopee-product-matching.zip\n",
        "*.csv\n",
        ".config/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6ovwnU1Kfd3U"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "warning: LF will be replaced by CRLF in latest_py_files.ipynb.\n",
            "The file will have its original line endings in your working directory\n"
          ]
        }
      ],
      "source": [
        "!git add ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6IZQYGIgasy",
        "outputId": "0c54833c-a15d-41c0-f2e0-74ef2dc7bf85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch main\n",
            "\n",
            "No commits yet\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git rm --cached <file>...\" to unstage)\n",
            "\tnew file:   .gitignore\n",
            "\tnew file:   EDA.py\n",
            "\tnew file:   create_fold.py\n",
            "\tnew file:   dvc.yaml\n",
            "\tnew file:   latest_py_files.ipynb\n",
            "\tnew file:   params.py\n",
            "\tnew file:   params.yaml\n",
            "\tnew file:   training.py\n",
            "\tnew file:   utils/dataloaders.py\n",
            "\tnew file:   utils/models.py\n",
            "\tnew file:   utils/randomness.py\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "dk1ipkc8fd3U"
      },
      "outputs": [],
      "source": [
        "!git config --global user.name \"RavitejaBadugu\"\n",
        "!git config --global user.email \"ravi14ashwin@gmail.com\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGXF5QFXfd3U",
        "outputId": "5dfc09d4-c1f7-4dae-b674-47376b053027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[main (root-commit) de0b264] initial files\n",
            " 11 files changed, 1732 insertions(+)\n",
            " create mode 100644 .gitignore\n",
            " create mode 100644 EDA.py\n",
            " create mode 100644 create_fold.py\n",
            " create mode 100644 dvc.yaml\n",
            " create mode 100644 latest_py_files.ipynb\n",
            " create mode 100644 params.py\n",
            " create mode 100644 params.yaml\n",
            " create mode 100644 training.py\n",
            " create mode 100644 utils/dataloaders.py\n",
            " create mode 100644 utils/models.py\n",
            " create mode 100644 utils/randomness.py\n"
          ]
        }
      ],
      "source": [
        "!git commit -m \"initial files\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kbzMFVtnfd3V"
      },
      "outputs": [],
      "source": [
        "!git branch -M main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD1seU5efd3V",
        "outputId": "25d243f5-fcab-4e87-e15b-bed527bbb1cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "error: remote origin already exists.\n"
          ]
        }
      ],
      "source": [
        "!git remote add origin git@github.com:RavitejaBadugu/shop_duplicate.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J1AqqxJfd3V",
        "outputId": "9f9f7b46-0ae7-428b-e597-d7673076ce34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Branch 'main' set up to track remote branch 'main' from 'origin'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "To https://github.com/RavitejaBadugu/shop_duplicate.git\n",
            " * [new branch]      main -> main\n"
          ]
        }
      ],
      "source": [
        "!git push -u origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRVC7q5tfd3V"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "latest-py-files.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f1270e6ce26af4109576f957535bad1f64509504c6d98ff147f498f2d893c152"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('qa-labelling')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
